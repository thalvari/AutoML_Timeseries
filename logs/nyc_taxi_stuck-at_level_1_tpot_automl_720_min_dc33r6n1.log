30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   6%|▌         | 6/100 [00:36<09:32,  6.10s/pipeline]Optimization Progress:  86%|████████▌ | 86/100 [00:39<00:59,  4.28s/pipeline]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 100/100 [00:40<00:00,  4.28s/pipeline]Optimization Progress: 100%|██████████| 100/100 [00:40<00:00,  3.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:41<00:00,  3.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:41<00:00,  3.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:42<00:00,  3.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 100/100 [00:43<00:00,  3.01s/pipeline]Optimization Progress:  50%|█████     | 101/200 [01:00<04:57,  3.01s/pipeline]Optimization Progress:  51%|█████     | 102/200 [04:05<53:37, 32.83s/pipeline]Optimization Progress:  91%|█████████ | 182/200 [04:41<06:56, 23.12s/pipeline]
Generation 1 - Current Pareto front scores:
-1	-382265487.52735007	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 200/200 [04:42<00:00, 23.12s/pipeline]Optimization Progress: 100%|██████████| 200/200 [04:42<00:00, 16.20s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 200/200 [04:47<00:00, 16.20s/pipeline]Optimization Progress:  67%|██████▋   | 201/300 [04:55<25:15, 15.31s/pipeline]Optimization Progress:  94%|█████████▎| 281/300 [04:59<03:23, 10.73s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 300/300 [04:59<00:00, 10.73s/pipeline]Optimization Progress: 100%|██████████| 300/300 [04:59<00:00,  7.51s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [05:00<00:00,  7.51s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [05:00<00:00,  7.51s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [05:04<00:00,  7.51s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▌  | 303/400 [05:08<12:08,  7.51s/pipeline]                                                                              Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  76%|███████▌  | 304/400 [05:08<12:01,  7.51s/pipeline]Optimization Progress:  76%|███████▋  | 305/400 [05:08<09:10,  5.79s/pipeline]Optimization Progress:  76%|███████▋  | 305/400 [05:20<09:10,  5.79s/pipeline]Optimization Progress:  76%|███████▋  | 306/400 [05:27<15:14,  9.72s/pipeline]Optimization Progress:  96%|█████████▋| 386/400 [05:43<01:36,  6.87s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [05:45<00:00,  6.87s/pipeline]Optimization Progress: 100%|██████████| 400/400 [05:45<00:00,  4.86s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 400/400 [05:49<00:00,  4.86s/pipeline]Optimization Progress:  82%|████████▏ | 408/500 [05:50<05:28,  3.57s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  82%|████████▏ | 408/500 [05:50<05:28,  3.57s/pipeline]Optimization Progress:  82%|████████▏ | 410/500 [05:54<04:48,  3.20s/pipeline]Optimization Progress:  98%|█████████▊| 490/500 [05:56<00:22,  2.25s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-356170656.8077505	XGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [05:56<00:00,  2.25s/pipeline]Optimization Progress: 100%|██████████| 500/500 [05:56<00:00,  1.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [05:57<00:00,  1.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 500/500 [05:57<00:00,  1.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [05:58<00:00,  1.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [06:01<00:00,  1.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [06:02<00:00,  1.58s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  83%|████████▎ | 500/600 [06:02<02:37,  1.58s/pipeline]Optimization Progress:  84%|████████▍ | 504/600 [06:06<02:57,  1.85s/pipeline]Optimization Progress:  97%|█████████▋| 582/600 [06:09<00:23,  1.31s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-355213572.37452537	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [07:58:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 600/600 [06:10<00:00,  1.31s/pipeline]Optimization Progress: 100%|██████████| 600/600 [06:10<00:00,  1.09pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [06:11<00:00,  1.09pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [06:12<00:00,  1.09pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 600/600 [06:13<00:00,  1.09pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 600/600 [06:13<00:00,  1.09pipeline/s]Optimization Progress:  86%|████████▋ | 604/700 [06:17<01:58,  1.23s/pipeline]Optimization Progress:  97%|█████████▋| 681/700 [06:21<00:16,  1.14pipeline/s]
Generation 6 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-355213572.37452537	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 700/700 [06:21<00:00,  1.14pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [07:58:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 700/700 [06:21<00:00,  1.14pipeline/s]Optimization Progress: 100%|██████████| 700/700 [06:21<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [06:22<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [06:23<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 700/700 [06:23<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..
Optimization Progress: 100%|██████████| 700/700 [06:23<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 700/700 [06:23<00:00,  1.61pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [06:24<00:00,  1.61pipeline/s]Optimization Progress:  88%|████████▊ | 704/800 [06:28<01:32,  1.04pipeline/s]Optimization Progress:  98%|█████████▊| 782/800 [06:31<00:12,  1.46pipeline/s]
Generation 7 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-355213572.37452537	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 800/800 [06:31<00:00,  1.46pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 100%|██████████| 800/800 [06:33<00:00,  1.46pipeline/s]Optimization Progress: 100%|██████████| 800/800 [06:33<00:00,  2.01pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [07:58:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 800/800 [06:34<00:00,  2.01pipeline/s]Optimization Progress:  89%|████████▉ | 804/900 [06:34<00:47,  2.03pipeline/s]Optimization Progress:  90%|████████▉ | 806/900 [06:40<01:44,  1.11s/pipeline]Optimization Progress:  98%|█████████▊| 885/900 [06:44<00:11,  1.26pipeline/s]
Generation 8 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-355213572.37452537	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-3	-353773367.07350284	XGBRegressor(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-4	-344469107.97326607	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-6	-341455635.828589	XGBRegressor(RobustScaler(ZeroCount(PCA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PCA__iterated_power=2, PCA__svd_solver=randomized))), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 900/900 [06:44<00:00,  1.26pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [06:44<00:00,  1.26pipeline/s]Optimization Progress: 100%|██████████| 900/900 [06:44<00:00,  1.75pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 900/900 [06:45<00:00,  1.75pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [06:45<00:00,  1.75pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 [07:59:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:45<00:00,  1.75pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  90%|█████████ | 900/1000 [06:47<00:57,  1.75pipeline/s]Optimization Progress:  90%|█████████ | 902/1000 [06:51<02:16,  1.40s/pipeline]Optimization Progress:  98%|█████████▊| 982/1000 [06:57<00:18,  1.00s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-353580544.5820704	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-344469107.97326607	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-6	-341455635.828589	XGBRegressor(RobustScaler(ZeroCount(PCA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PCA__iterated_power=2, PCA__svd_solver=randomized))), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [06:57<00:00,  1.00s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 1000/1000 [06:58<00:00,  1.00s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [06:58<00:00,  1.42pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:59:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [06:59<00:00,  1.42pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [06:59<00:00,  1.42pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [06:59<00:00,  1.42pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 1000/1000 [06:59<00:00,  1.42pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1000/1000 [07:00<00:00,  1.42pipeline/s]Optimization Progress:  91%|█████████ | 1002/1100 [07:10<01:08,  1.42pipeline/s]Optimization Progress:  91%|█████████ | 1003/1100 [07:13<03:17,  2.03s/pipeline]Optimization Progress:  98%|█████████▊| 1083/1100 [07:25<00:24,  1.47s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-353580544.5820704	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-352731686.96246403	XGBRegressor(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-344469107.97326607	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-6	-341455635.828589	XGBRegressor(RobustScaler(ZeroCount(PCA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PCA__iterated_power=2, PCA__svd_solver=randomized))), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [07:25<00:00,  1.47s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [07:25<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 1100/1100 [07:26<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 1100/1100 [07:26<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:59:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [07:26<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [07:27<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [07:27<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 1100/1100 [07:28<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [07:28<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:59:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [07:28<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1100/1100 [07:28<00:00,  1.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 1100/1100 [07:29<00:00,  1.03s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  92%|█████████▏| 1101/1200 [07:29<01:42,  1.03s/pipeline]Optimization Progress:  92%|█████████▏| 1102/1200 [07:29<02:10,  1.33s/pipeline]Optimization Progress:  92%|█████████▏| 1104/1200 [07:36<03:06,  1.94s/pipeline]Optimization Progress:  99%|█████████▊| 1183/1200 [07:39<00:23,  1.37s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-355004138.65041745	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-352183568.02250457	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-348212060.5035275	XGBRegressor(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-332666545.5400155	XGBRegressor(RobustScaler(MinMaxScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2))), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [07:40<00:00,  1.37s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [07:40<00:00,  1.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [07:41<00:00,  1.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 1200/1200 [07:43<00:00,  1.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:00:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1200/1200 [07:44<00:00,  1.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [07:44<00:00,  1.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1200/1200 [07:44<00:00,  1.03pipeline/s]Optimization Progress:  93%|█████████▎| 1203/1300 [07:45<02:00,  1.25s/pipeline]Optimization Progress:  93%|█████████▎| 1205/1300 [07:57<04:09,  2.63s/pipeline]Optimization Progress:  99%|█████████▉| 1284/1300 [08:01<00:29,  1.86s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-354399555.8080705	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-351526219.5296336	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-348212060.5035275	XGBRegressor(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-332666545.5400155	XGBRegressor(RobustScaler(MinMaxScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2))), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [08:05<00:00,  1.86s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [08:05<00:00,  1.38s/pipeline]Optimization Progress:  93%|█████████▎| 1301/1400 [08:08<03:06,  1.89s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [09:54<54:23, 33.30s/pipeline]Optimization Progress:  99%|█████████▊| 1382/1400 [10:07<07:00, 23.36s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-354399555.8080705	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-351526219.5296336	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [10:10<00:00, 23.36s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [10:10<00:00, 16.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1400/1400 [10:11<00:00, 16.40s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1400/1400 [10:13<00:00, 16.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 1400/1400 [10:14<00:00, 16.40s/pipeline]Optimization Progress:  93%|█████████▎| 1402/1500 [10:14<19:46, 12.11s/pipeline]Optimization Progress:  94%|█████████▎| 1403/1500 [10:19<16:03,  9.93s/pipeline]Optimization Progress:  99%|█████████▉| 1483/1500 [10:23<01:58,  6.96s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-354399030.52166903	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-351526219.5296336	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [10:25<00:00,  6.96s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [10:25<00:00,  4.92s/pipeline]Optimization Progress:  94%|█████████▍| 1501/1600 [10:29<07:43,  4.69s/pipeline]Optimization Progress:  94%|█████████▍| 1502/1600 [10:35<08:00,  4.91s/pipeline]Optimization Progress:  99%|█████████▉| 1582/1600 [10:38<01:02,  3.45s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-354238023.1216068	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-351526219.5296336	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [10:40<00:00,  3.45s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [10:40<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 1600/1600 [10:40<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 1600/1600 [10:40<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:03:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:41<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [10:42<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [10:44<00:00,  2.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:03:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:44<00:00,  2.44s/pipeline]Optimization Progress:  94%|█████████▍| 1602/1700 [10:46<04:16,  2.62s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [10:52<05:57,  3.69s/pipeline]Optimization Progress:  99%|█████████▉| 1683/1700 [10:58<00:44,  2.60s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-354238023.1216068	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-351526219.5296336	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1700/1700 [11:02<00:00,  2.60s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [11:02<00:00,  1.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:03:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:03<00:00,  1.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [11:05<00:00,  1.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 1700/1700 [11:05<00:00,  1.89s/pipeline]Optimization Progress:  95%|█████████▍| 1702/1800 [11:06<03:10,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1702/1800 [11:06<03:10,  1.94s/pipeline]Optimization Progress:  95%|█████████▍| 1704/1800 [11:11<03:16,  2.04s/pipeline]Optimization Progress:  99%|█████████▉| 1784/1800 [11:14<00:23,  1.44s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [11:19<00:00,  1.44s/pipeline]Optimization Progress: 100%|██████████| 1800/1800 [11:19<00:00,  1.11s/pipeline]Optimization Progress:  95%|█████████▍| 1803/1900 [11:20<01:24,  1.15pipeline/s]Optimization Progress:  95%|█████████▍| 1804/1900 [11:33<06:56,  4.34s/pipeline]Optimization Progress:  99%|█████████▉| 1884/1900 [11:36<00:48,  3.05s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1900/1900 [11:38<00:00,  3.05s/pipeline]Optimization Progress: 100%|██████████| 1900/1900 [11:38<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1900/1900 [11:39<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1900/1900 [11:39<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1900/1900 [11:40<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1900/1900 [11:43<00:00,  2.17s/pipeline]Optimization Progress:  95%|█████████▌| 1903/2000 [11:44<03:24,  2.11s/pipeline]Optimization Progress:  95%|█████████▌| 1904/2000 [11:50<05:09,  3.23s/pipeline]Optimization Progress:  99%|█████████▉| 1984/2000 [11:54<00:36,  2.27s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334308670.5857986	XGBRegressor(RobustScaler(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2000/2000 [11:55<00:00,  2.27s/pipeline]Optimization Progress: 100%|██████████| 2000/2000 [11:55<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=3 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=4 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=5 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=6 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2000/2000 [11:57<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 2000/2000 [11:59<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:04:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2000/2000 [11:59<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2000/2000 [11:59<00:00,  1.61s/pipeline]Optimization Progress:  95%|█████████▌| 2003/2100 [12:03<02:58,  1.84s/pipeline]Optimization Progress:  95%|█████████▌| 2004/2100 [12:37<18:32, 11.59s/pipeline]Optimization Progress:  99%|█████████▉| 2084/2100 [12:41<02:10,  8.13s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335621171.6702399	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-334177406.08176714	XGBRegressor(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2100/2100 [12:41<00:00,  8.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2100/2100 [12:42<00:00,  8.13s/pipeline]Optimization Progress: 100%|██████████| 2100/2100 [12:42<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [12:42<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 2100/2100 [12:43<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 2100/2100 [12:45<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2100/2100 [12:45<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2100/2100 [12:49<00:00,  5.69s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2101/2200 [12:50<09:23,  5.69s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2102/2200 [12:50<09:17,  5.69s/pipeline]Optimization Progress:  96%|█████████▌| 2103/2200 [12:50<07:44,  4.78s/pipeline]Optimization Progress:  96%|█████████▌| 2105/2200 [12:55<06:30,  4.12s/pipeline]Optimization Progress:  99%|█████████▉| 2184/2200 [12:58<00:46,  2.89s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-332274193.4892396	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2200/2200 [13:00<00:00,  2.89s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [13:00<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2200/2200 [13:03<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:05:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2200/2200 [13:04<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [13:04<00:00,  2.06s/pipeline]Optimization Progress:  96%|█████████▌| 2202/2300 [13:06<03:51,  2.36s/pipeline]Optimization Progress:  96%|█████████▌| 2203/2300 [13:12<05:20,  3.30s/pipeline]Optimization Progress:  99%|█████████▉| 2283/2300 [13:18<00:39,  2.33s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346519226.29089224	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=4, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-332274193.4892396	XGBRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-4	-330764315.6087915	XGBRegressor(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)
-5	-330317315.0238346	XGBRegressor(RobustScaler(ExtraTreesRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=13, ExtraTreesRegressor__n_estimators=100)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 2300/2300 [13:18<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2300/2300 [13:19<00:00,  2.33s/pipeline]Optimization Progress: 100%|██████████| 2300/2300 [13:19<00:00,  1.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [13:20<00:00,  1.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2300/2300 [13:24<00:00,  1.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:05:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2300/2300 [13:24<00:00,  1.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [13:25<00:00,  1.66s/pipeline]Optimization Progress:  96%|█████████▌| 2303/2400 [13:26<02:55,  1.81s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2303/2400 [13:26<02:55,  1.81s/pipeline]Optimization Progress:  96%|█████████▌| 2305/2400 [13:30<03:07,  1.98s/pipeline]Optimization Progress:  99%|█████████▉| 2385/2400 [13:35<00:21,  1.40s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-344868513.42338955	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [13:35<00:00,  1.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [13:39<00:00,  1.40s/pipeline]Optimization Progress: 100%|██████████| 2400/2400 [13:39<00:00,  1.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2400/2400 [13:42<00:00,  1.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [13:43<00:00,  1.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 2400/2400 [13:44<00:00,  1.06s/pipeline]Optimization Progress:  96%|█████████▌| 2401/2500 [13:45<04:21,  2.64s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2401/2500 [13:45<04:21,  2.64s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2402/2500 [13:45<04:19,  2.64s/pipeline]Optimization Progress:  96%|█████████▌| 2404/2500 [13:57<04:49,  3.02s/pipeline]Optimization Progress:  99%|█████████▉| 2484/2500 [14:02<00:34,  2.13s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-344868513.42338955	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 2500/2500 [14:08<00:00,  2.13s/pipeline]Optimization Progress: 100%|██████████| 2500/2500 [14:08<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:06:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2500/2500 [14:09<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:06:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2500/2500 [14:09<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:06:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2500/2500 [14:11<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 2500/2500 [14:11<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2500/2500 [14:13<00:00,  1.60s/pipeline]Optimization Progress:  96%|█████████▌| 2502/2600 [14:13<03:10,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2502/2600 [14:13<03:10,  1.94s/pipeline]Optimization Progress:  96%|█████████▋| 2504/2600 [14:20<03:40,  2.30s/pipeline]Optimization Progress:  99%|█████████▉| 2584/2600 [14:24<00:26,  1.63s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-344868513.42338955	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2600/2600 [14:25<00:00,  1.63s/pipeline]Optimization Progress: 100%|██████████| 2600/2600 [14:25<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2600/2600 [14:32<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2600/2600 [14:32<00:00,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2600/2700 [14:34<01:54,  1.15s/pipeline]Optimization Progress:  96%|█████████▋| 2601/2700 [14:40<01:53,  1.15s/pipeline]Optimization Progress:  96%|█████████▋| 2602/2700 [14:41<05:12,  3.19s/pipeline]Optimization Progress:  99%|█████████▉| 2682/2700 [14:46<00:40,  2.25s/pipeline]
Generation 26 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-344868513.42338955	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.15000000000000002, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [14:47<00:00,  2.25s/pipeline]Optimization Progress: 100%|██████████| 2700/2700 [14:47<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2700/2700 [14:48<00:00,  1.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2700/2700 [14:55<00:00,  1.60s/pipeline]Optimization Progress:  97%|█████████▋| 2704/2800 [14:59<03:16,  2.05s/pipeline]Optimization Progress:  97%|█████████▋| 2705/2800 [15:07<06:04,  3.84s/pipeline]Optimization Progress:  99%|█████████▉| 2785/2800 [15:14<00:40,  2.71s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-344684457.530609	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:07:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [15:15<00:00,  2.71s/pipeline]Optimization Progress: 100%|██████████| 2800/2800 [15:15<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:07:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [15:16<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [15:17<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2800/2800 [15:22<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [15:22<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2800/2800 [15:24<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 2800/2800 [15:25<00:00,  1.92s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 2800/2800 [15:26<00:00,  1.92s/pipeline]Optimization Progress:  97%|█████████▋| 2802/2900 [15:27<05:06,  3.13s/pipeline]Optimization Progress:  97%|█████████▋| 2803/2900 [15:32<06:19,  3.91s/pipeline]Optimization Progress:  99%|█████████▉| 2883/2900 [16:06<00:48,  2.86s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341784803.9635166	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-217640072.40324122	LinearSVR(FastICA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-217251389.95603794	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [16:07<00:00,  2.86s/pipeline]Optimization Progress: 100%|██████████| 2900/2900 [16:07<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 2900/2900 [16:10<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2900/2900 [16:19<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [16:20<00:00,  2.01s/pipeline]Optimization Progress: 100%|██████████| 2900/2900 [16:20<00:00,  2.01s/pipeline]Optimization Progress:  97%|█████████▋| 2901/3000 [16:21<09:09,  5.55s/pipeline]Optimization Progress:  97%|█████████▋| 2902/3000 [18:36<1:12:32, 44.41s/pipeline]Optimization Progress:  99%|█████████▉| 2982/3000 [18:40<09:19, 31.11s/pipeline]  
Generation 29 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341784803.9635166	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217642878.02620393	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-217640072.40324122	LinearSVR(FastICA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-217251389.95603794	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 3000/3000 [18:45<00:00, 31.11s/pipeline]Optimization Progress: 100%|██████████| 3000/3000 [18:45<00:00, 21.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 3000/3000 [18:45<00:00, 21.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [18:49<00:00, 21.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [18:51<00:00, 21.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 3000/3000 [18:51<00:00, 21.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 3000/3000 [18:53<00:00, 21.85s/pipeline]Optimization Progress:  97%|█████████▋| 3002/3100 [18:54<27:06, 16.60s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3002/3100 [18:54<27:06, 16.60s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3003/3100 [18:54<26:50, 16.60s/pipeline]Optimization Progress:  97%|█████████▋| 3005/3100 [23:55<1:06:11, 41.81s/pipeline]                                                                                  Skipped pipeline #3021 due to time out. Continuing to the next pipeline.
Optimization Progress:  97%|█████████▋| 3021/3100 [23:55<55:02, 41.81s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress: 100%|█████████▉| 3086/3100 [24:01<06:49, 29.29s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341784803.9635166	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=12, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-217586222.33570242	LinearSVR(FastICA(FastICA(input_matrix, FastICA__tol=0.2), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-217417642.24738628	LinearSVR(FastICA(RandomForestRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=6, RandomForestRegressor__min_samples_split=13, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-217251389.95603794	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 3101pipeline [24:03, 29.29s/pipeline]Optimization Progress: 3101pipeline [24:03, 20.54s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 3101pipeline [24:08, 20.54s/pipeline]Optimization Progress:  97%|█████████▋| 3102/3200 [25:33<1:07:38, 41.42s/pipeline]Optimization Progress:  99%|█████████▉| 3182/3200 [25:37<08:42, 29.01s/pipeline]  
Generation 31 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217573767.97290105	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-217485596.73587704	LinearSVR(FastICA(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-217417642.24738628	LinearSVR(FastICA(RandomForestRegressor(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=6, RandomForestRegressor__min_samples_split=13, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-217251389.95603794	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 3201pipeline [25:41, 29.01s/pipeline]Optimization Progress: 3201pipeline [25:41, 20.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 3201pipeline [25:41, 20.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:18:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 3201pipeline [25:41, 20.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 3201pipeline [25:42, 20.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 3201pipeline [25:46, 20.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 3201pipeline [25:47, 20.37s/pipeline]Optimization Progress:  97%|█████████▋| 3202/3300 [25:49<27:09, 16.62s/pipeline]Optimization Progress:  97%|█████████▋| 3203/3300 [25:58<23:20, 14.44s/pipeline]Optimization Progress:  99%|█████████▉| 3283/3300 [26:04<02:52, 10.13s/pipeline]
Generation 32 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217573767.97290105	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-206018416.01329458	LinearSVR(Binarizer(FastICA(FastICA(input_matrix, FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3301pipeline [26:05, 10.13s/pipeline]Optimization Progress: 3301pipeline [26:05,  7.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 3301pipeline [26:08,  7.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:18:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 3301pipeline [26:10,  7.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3301pipeline [26:12,  7.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3301pipeline [26:13,  7.10s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3302/3400 [26:13<11:36,  7.10s/pipeline]Optimization Progress:  97%|█████████▋| 3303/3400 [26:13<10:04,  6.24s/pipeline]Optimization Progress:  97%|█████████▋| 3304/3400 [26:19<09:45,  6.10s/pipeline]Optimization Progress: 100%|█████████▉| 3384/3400 [26:26<01:08,  4.29s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217573767.97290105	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-206018416.01329458	LinearSVR(Binarizer(FastICA(FastICA(input_matrix, FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:18:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 3401pipeline [26:28,  4.29s/pipeline]Optimization Progress: 3401pipeline [26:28,  3.04s/pipeline]Optimization Progress:  97%|█████████▋| 3405/3500 [26:32<03:57,  2.50s/pipeline]Optimization Progress:  97%|█████████▋| 3406/3500 [26:37<05:00,  3.20s/pipeline]Optimization Progress: 100%|█████████▉| 3486/3500 [27:05<00:32,  2.35s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217573767.97290105	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-3	-216476279.46567637	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-206018416.01329458	LinearSVR(Binarizer(FastICA(FastICA(input_matrix, FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-7	-205531838.1320099	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(Nystroem(input_matrix, Nystroem__gamma=0.8500000000000001, Nystroem__kernel=chi2, Nystroem__n_components=10), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 3501pipeline [27:06,  2.35s/pipeline]Optimization Progress: 3501pipeline [27:06,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3501pipeline [27:09,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3501pipeline [27:11,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3501pipeline [27:13,  1.66s/pipeline]Optimization Progress:  97%|█████████▋| 3502/3600 [27:13<05:13,  3.20s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3502/3600 [27:13<05:13,  3.20s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3503/3600 [27:13<05:10,  3.20s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3504/3600 [27:13<05:06,  3.20s/pipeline]Optimization Progress:  97%|█████████▋| 3506/3600 [30:03<23:31, 15.01s/pipeline]Optimization Progress: 100%|█████████▉| 3586/3600 [30:07<02:27, 10.52s/pipeline]
Generation 35 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217573767.97290105	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-7	-205531838.1320099	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(Nystroem(input_matrix, Nystroem__gamma=0.8500000000000001, Nystroem__kernel=chi2, Nystroem__n_components=10), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3601pipeline [30:09, 10.52s/pipeline]Optimization Progress: 3601pipeline [30:09,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3601pipeline [30:09,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3601pipeline [30:11,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3601pipeline [30:11,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3601pipeline [30:11,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3601pipeline [30:12,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 3601pipeline [30:12,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3601pipeline [30:12,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 3601pipeline [30:12,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3601pipeline [30:13,  7.40s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 3601pipeline [30:13,  7.40s/pipeline]Optimization Progress:  97%|█████████▋| 3602/3700 [30:14<10:43,  6.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3602/3700 [30:14<10:43,  6.57s/pipeline]Optimization Progress:  97%|█████████▋| 3604/3700 [30:56<17:35, 11.00s/pipeline]Optimization Progress: 100%|█████████▉| 3684/3700 [31:02<02:03,  7.72s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217477277.39729387	LinearSVR(FastICA(input_matrix, FastICA__tol=0.8), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-7	-205531838.1320099	LinearSVR(FastICA(RandomForestRegressor(PolynomialFeatures(FastICA(RandomForestRegressor(Nystroem(input_matrix, Nystroem__gamma=0.8500000000000001, Nystroem__kernel=chi2, Nystroem__n_components=10), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3701pipeline [31:04,  7.72s/pipeline]Optimization Progress: 3701pipeline [31:04,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3701pipeline [31:04,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 3701pipeline [31:04,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 3701pipeline [31:05,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3701pipeline [31:05,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3701pipeline [31:05,  5.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3701pipeline [31:06,  5.45s/pipeline]Optimization Progress:  97%|█████████▋| 3703/3800 [31:06<06:35,  4.07s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3703/3800 [31:06<06:35,  4.07s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3704/3800 [31:06<06:31,  4.07s/pipeline]Optimization Progress:  98%|█████████▊| 3706/3800 [31:10<05:12,  3.32s/pipeline]Optimization Progress: 100%|█████████▉| 3786/3800 [31:12<00:32,  2.33s/pipeline]
Generation 37 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208647332.98316926	LinearSVR(FastICA(input_matrix, FastICA__tol=0.55), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3801pipeline [31:14,  2.33s/pipeline]Optimization Progress: 3801pipeline [31:14,  1.67s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3801pipeline [31:14,  1.67s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 3801pipeline [31:16,  1.67s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3801pipeline [31:16,  1.67s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3801pipeline [31:16,  1.67s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3802/3900 [31:17<02:43,  1.67s/pipeline]Optimization Progress:  98%|█████████▊| 3803/3900 [31:17<02:32,  1.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3803/3900 [31:17<02:32,  1.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3804/3900 [31:17<02:31,  1.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3805/3900 [31:17<02:29,  1.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3806/3900 [31:17<02:27,  1.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3807/3900 [31:17<02:26,  1.57s/pipeline]Optimization Progress:  98%|█████████▊| 3808/3900 [31:30<02:24,  1.57s/pipeline]Optimization Progress:  98%|█████████▊| 3809/3900 [31:55<04:34,  3.02s/pipeline]Optimization Progress: 100%|█████████▉| 3889/3900 [31:56<00:23,  2.11s/pipeline]
Generation 38 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208647332.98316926	LinearSVR(FastICA(input_matrix, FastICA__tol=0.55), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3901pipeline [31:56,  2.11s/pipeline]Optimization Progress: 3901pipeline [31:56,  1.49s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3901pipeline [31:56,  1.49s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:24:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 3901pipeline [31:57,  1.49s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3901pipeline [31:59,  1.49s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3904/4000 [32:00<02:23,  1.49s/pipeline]Optimization Progress:  98%|█████████▊| 3905/4000 [32:00<02:05,  1.32s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3905/4000 [32:00<02:05,  1.32s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 3906/4000 [32:00<02:03,  1.32s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3907/4000 [32:00<02:02,  1.32s/pipeline]Optimization Progress:  98%|█████████▊| 3908/4000 [32:20<02:01,  1.32s/pipeline]Optimization Progress:  98%|█████████▊| 3909/4000 [32:41<06:08,  4.05s/pipeline]Optimization Progress: 100%|█████████▉| 3989/4000 [32:44<00:31,  2.84s/pipeline]
Generation 39 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208647332.98316926	LinearSVR(FastICA(input_matrix, FastICA__tol=0.55), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4001pipeline [32:45,  2.84s/pipeline]Optimization Progress: 4001pipeline [32:45,  2.01s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4001pipeline [32:47,  2.01s/pipeline]Optimization Progress:  98%|█████████▊| 4003/4100 [32:47<02:52,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4003/4100 [32:47<02:52,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4004/4100 [32:47<02:50,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4005/4100 [32:47<02:49,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4006/4100 [32:47<02:47,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4007/4100 [32:47<02:45,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4008/4100 [32:47<02:43,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4009/4100 [32:47<02:41,  1.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4010/4100 [32:47<02:40,  1.78s/pipeline]Optimization Progress:  98%|█████████▊| 4012/4100 [32:50<01:58,  1.34s/pipeline]Optimization Progress: 100%|█████████▉| 4092/4100 [32:55<00:07,  1.05pipeline/s]
Generation 40 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208647332.98316926	LinearSVR(FastICA(input_matrix, FastICA__tol=0.55), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4101pipeline [32:55,  1.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4101pipeline [32:55,  1.05pipeline/s]Optimization Progress: 4101pipeline [32:55,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4101pipeline [32:55,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4101pipeline [32:56,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4101pipeline [32:56,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4101pipeline [32:56,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:25:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4101pipeline [32:57,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4101pipeline [32:57,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4101pipeline [32:57,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4101pipeline [32:58,  1.46pipeline/s]Optimization Progress:  98%|█████████▊| 4104/4200 [32:58<01:14,  1.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4104/4200 [32:58<01:14,  1.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4105/4200 [32:58<01:13,  1.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4106/4200 [32:58<01:12,  1.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4107/4200 [32:58<01:12,  1.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4108/4200 [32:58<01:11,  1.29pipeline/s]Optimization Progress:  98%|█████████▊| 4110/4200 [33:16<02:09,  1.43s/pipeline]Optimization Progress: 100%|█████████▉| 4190/4200 [33:17<00:10,  1.01s/pipeline]
Generation 41 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208647332.98316926	LinearSVR(FastICA(input_matrix, FastICA__tol=0.55), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4201pipeline [33:17,  1.01s/pipeline]Optimization Progress: 4201pipeline [33:17,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 4201pipeline [33:18,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4201pipeline [33:18,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:25:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4201pipeline [33:18,  1.41pipeline/s]Optimization Progress:  98%|█████████▊| 4205/4300 [33:19<01:03,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4205/4300 [33:19<01:03,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4206/4300 [33:19<01:03,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4207/4300 [33:19<01:02,  1.49pipeline/s]Optimization Progress:  98%|█████████▊| 4209/4300 [33:24<01:12,  1.26pipeline/s]Optimization Progress: 100%|█████████▉| 4289/4300 [33:27<00:06,  1.75pipeline/s]
Generation 42 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208595236.18029132	LinearSVR(FastICA(input_matrix, FastICA__tol=0.6000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205661970.19086424	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-205475995.595118	LinearSVR(Binarizer(FastICA(FastICA(StandardScaler(input_matrix), FastICA__tol=0.2), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4301pipeline [33:27,  1.75pipeline/s]Optimization Progress: 4301pipeline [33:27,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4301pipeline [33:28,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:25:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4301pipeline [33:28,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4301pipeline [33:29,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:25:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4301pipeline [33:29,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4301pipeline [33:29,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 4301pipeline [33:29,  2.48pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4302/4400 [33:30<00:39,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4303/4400 [33:30<00:39,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4304/4400 [33:30<00:38,  2.48pipeline/s]Optimization Progress:  98%|█████████▊| 4305/4400 [33:30<00:44,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4305/4400 [33:30<00:44,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4306/4400 [33:30<00:43,  2.15pipeline/s]Optimization Progress:  98%|█████████▊| 4308/4400 [33:32<00:56,  1.62pipeline/s]Optimization Progress: 100%|█████████▉| 4388/4400 [33:36<00:05,  2.25pipeline/s]
Generation 43 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208595236.18029132	LinearSVR(FastICA(input_matrix, FastICA__tol=0.6000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-3	-205355994.98699751	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.65), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-203929215.44574183	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4401pipeline [33:36,  2.25pipeline/s]Optimization Progress: 4401pipeline [33:36,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4401pipeline [33:39,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:25:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4401pipeline [33:39,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 4401pipeline [33:40,  3.10pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4403/4500 [33:40<00:31,  3.10pipeline/s]Optimization Progress:  98%|█████████▊| 4404/4500 [33:40<00:59,  1.63pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4404/4500 [33:40<00:59,  1.63pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4405/4500 [33:40<00:58,  1.63pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4406/4500 [33:40<00:57,  1.63pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4407/4500 [33:40<00:57,  1.63pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4408/4500 [33:40<00:56,  1.63pipeline/s]Optimization Progress:  98%|█████████▊| 4409/4500 [34:00<00:55,  1.63pipeline/s]Optimization Progress:  98%|█████████▊| 4410/4500 [34:07<02:38,  1.76s/pipeline]Optimization Progress: 100%|█████████▉| 4490/4500 [34:10<00:12,  1.24s/pipeline]
Generation 44 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-205355994.98699751	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.65), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-203929215.44574183	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4501pipeline [34:11,  1.24s/pipeline]Optimization Progress: 4501pipeline [34:11,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 4501pipeline [34:11,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4501pipeline [34:12,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 4501pipeline [34:12,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4501pipeline [34:12,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 4501pipeline [34:12,  1.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 4501pipeline [34:13,  1.13pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4501/4600 [34:14<01:27,  1.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4502/4600 [34:14<01:26,  1.13pipeline/s]Optimization Progress:  98%|█████████▊| 4503/4600 [34:14<01:51,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4503/4600 [34:14<01:51,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4504/4600 [34:14<01:50,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4505/4600 [34:14<01:49,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4506/4600 [34:14<01:48,  1.15s/pipeline]Optimization Progress:  98%|█████████▊| 4508/4600 [34:39<03:33,  2.32s/pipeline]Optimization Progress: 100%|█████████▉| 4588/4600 [34:43<00:19,  1.64s/pipeline]
Generation 45 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-203929215.44574183	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:27:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4601pipeline [34:44,  1.64s/pipeline]Optimization Progress: 4601pipeline [34:44,  1.17s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4601pipeline [34:44,  1.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:27:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4601pipeline [34:44,  1.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4601pipeline [34:44,  1.17s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4601/4700 [34:46<01:55,  1.17s/pipeline]Optimization Progress:  98%|█████████▊| 4602/4700 [34:46<02:14,  1.38s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4602/4700 [34:46<02:14,  1.38s/pipeline]Optimization Progress:  98%|█████████▊| 4604/4700 [35:15<08:25,  5.26s/pipeline]Optimization Progress: 100%|█████████▉| 4684/4700 [35:33<01:00,  3.75s/pipeline]
Generation 46 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-203929215.44574183	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4701pipeline [35:34,  3.75s/pipeline]Optimization Progress: 4701pipeline [35:34,  2.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4701pipeline [35:35,  2.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4701pipeline [35:35,  2.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4701pipeline [35:37,  2.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4701pipeline [35:38,  2.63s/pipeline]Optimization Progress:  98%|█████████▊| 4704/4800 [35:39<03:44,  2.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4704/4800 [35:39<03:44,  2.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4705/4800 [35:39<03:41,  2.34s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4706/4800 [35:39<03:39,  2.34s/pipeline]Optimization Progress:  98%|█████████▊| 4707/4800 [35:50<03:37,  2.34s/pipeline]Optimization Progress:  98%|█████████▊| 4708/4800 [36:05<05:32,  3.61s/pipeline]Optimization Progress: 100%|█████████▉| 4788/4800 [36:36<00:31,  2.64s/pipeline]
Generation 47 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-202526977.67005816	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4801pipeline [36:36,  2.64s/pipeline]Optimization Progress: 4801pipeline [36:36,  1.87s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:28:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4801pipeline [36:36,  1.87s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4801pipeline [36:36,  1.87s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4801pipeline [36:38,  1.87s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4801pipeline [36:38,  1.87s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:29:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4801pipeline [36:40,  1.87s/pipeline]Optimization Progress:  98%|█████████▊| 4802/4900 [36:41<04:28,  2.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4802/4900 [36:41<04:28,  2.74s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4803/4900 [36:41<04:25,  2.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4804/4900 [36:41<04:23,  2.74s/pipeline]Optimization Progress:  98%|█████████▊| 4806/4900 [37:08<06:10,  3.94s/pipeline]Optimization Progress: 100%|█████████▉| 4886/4900 [37:23<00:39,  2.81s/pipeline]
Generation 48 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-202526977.67005816	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  2.81s/pipeline]Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=2 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=3 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=4 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=5 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:23,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:24,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:29:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:29:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4901pipeline [37:26,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4901pipeline [37:27,  1.98s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4902/5000 [37:27<03:13,  1.98s/pipeline]Optimization Progress:  98%|█████████▊| 4903/5000 [37:27<03:11,  1.97s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4903/5000 [37:27<03:11,  1.97s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4904/5000 [37:27<03:09,  1.97s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4905/5000 [37:27<03:07,  1.97s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4906/5000 [37:27<03:05,  1.97s/pipeline]Optimization Progress:  98%|█████████▊| 4907/5000 [37:40<03:03,  1.97s/pipeline]Optimization Progress:  98%|█████████▊| 4908/5000 [37:44<03:38,  2.38s/pipeline]Optimization Progress: 100%|█████████▉| 4988/5000 [37:45<00:20,  1.67s/pipeline]
Generation 49 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-206053124.94568133	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-202526977.67005816	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 5001pipeline [37:45,  1.67s/pipeline]Optimization Progress: 5001pipeline [37:45,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5001pipeline [37:46,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5001pipeline [37:46,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5001pipeline [37:47,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5001pipeline [37:47,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 5001pipeline [37:48,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5001pipeline [37:48,  1.18s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5003/5100 [37:48<01:54,  1.18s/pipeline]Optimization Progress:  98%|█████████▊| 5004/5100 [37:48<01:47,  1.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5004/5100 [37:48<01:47,  1.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5005/5100 [37:48<01:46,  1.12s/pipeline]Optimization Progress:  98%|█████████▊| 5006/5100 [38:00<01:45,  1.12s/pipeline]Optimization Progress:  98%|█████████▊| 5007/5100 [38:21<06:14,  4.03s/pipeline]Optimization Progress: 100%|█████████▉| 5087/5100 [38:24<00:36,  2.83s/pipeline]
Generation 50 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205919546.56882972	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-204802495.64793244	LinearSVR(Binarizer(FastICA(input_matrix, FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-202526977.67005816	LinearSVR(Binarizer(FastICA(CombineDFs(input_matrix, MaxAbsScaler(input_matrix)), FastICA__tol=0.2), Binarizer__threshold=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5101pipeline [38:24,  2.83s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5101pipeline [38:24,  2.83s/pipeline]Optimization Progress: 5101pipeline [38:24,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5101pipeline [38:24,  1.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 5101pipeline [38:26,  1.98s/pipeline]Optimization Progress:  98%|█████████▊| 5102/5200 [38:40<03:14,  1.98s/pipeline]Optimization Progress:  98%|█████████▊| 5103/5200 [39:06<12:29,  7.73s/pipeline]Optimization Progress: 100%|█████████▉| 5183/5200 [39:07<01:32,  5.42s/pipeline]
Generation 51 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205919546.56882972	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 5201pipeline [39:08,  5.42s/pipeline]Optimization Progress: 5201pipeline [39:08,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:31:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5201pipeline [39:08,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5201pipeline [39:09,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 5201pipeline [39:10,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5201pipeline [39:11,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:31:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5201pipeline [39:11,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5201pipeline [39:11,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5201pipeline [39:11,  3.79s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5201pipeline [39:11,  3.79s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5202/5300 [39:11<06:11,  3.79s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5203/5300 [39:11<06:08,  3.79s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5204/5300 [39:11<06:04,  3.79s/pipeline]Optimization Progress:  98%|█████████▊| 5205/5300 [39:20<06:00,  3.79s/pipeline]Optimization Progress:  98%|█████████▊| 5206/5300 [39:33<06:31,  4.17s/pipeline]Optimization Progress: 100%|█████████▉| 5286/5300 [39:35<00:40,  2.93s/pipeline]
Generation 52 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205725389.99168563	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5301pipeline [39:35,  2.93s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:35,  2.93s/pipeline]Optimization Progress: 5301pipeline [39:35,  2.05s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:35,  2.05s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=2 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:35,  2.05s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=3 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:35,  2.05s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=4 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:35,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5301pipeline [39:36,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 5301pipeline [39:36,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 5301pipeline [39:36,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:31:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5301pipeline [39:37,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5301pipeline [39:38,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5301pipeline [39:38,  2.05s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5302/5400 [39:38<03:21,  2.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5303/5400 [39:38<03:18,  2.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5304/5400 [39:38<03:16,  2.05s/pipeline]Optimization Progress:  98%|█████████▊| 5305/5400 [39:38<02:37,  1.65s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5305/5400 [39:38<02:37,  1.65s/pipeline]Optimization Progress:  98%|█████████▊| 5308/5400 [39:42<02:19,  1.51s/pipeline]Optimization Progress: 100%|█████████▉| 5387/5400 [39:44<00:13,  1.07s/pipeline]
Generation 53 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205725389.99168563	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5401pipeline [39:44,  1.07s/pipeline]Optimization Progress: 5401pipeline [39:44,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5401pipeline [39:45,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5401pipeline [39:45,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5401pipeline [39:45,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5401pipeline [39:45,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5401pipeline [39:46,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 5401pipeline [39:46,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5401pipeline [39:46,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5401pipeline [39:47,  1.33pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5402/5500 [39:47<01:13,  1.33pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5403/5500 [39:47<01:12,  1.33pipeline/s]Optimization Progress:  98%|█████████▊| 5404/5500 [40:00<01:11,  1.33pipeline/s]Optimization Progress:  98%|█████████▊| 5405/5500 [40:25<05:38,  3.56s/pipeline]Optimization Progress: 100%|█████████▉| 5485/5500 [40:26<00:37,  2.50s/pipeline]
Generation 54 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205725389.99168563	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5501pipeline [40:26,  2.50s/pipeline]Optimization Progress: 5501pipeline [40:26,  1.75s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5501pipeline [40:26,  1.75s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5501pipeline [40:26,  1.75s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5501pipeline [40:27,  1.75s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5504/5600 [40:28<02:48,  1.75s/pipeline]Optimization Progress:  98%|█████████▊| 5505/5600 [40:28<02:08,  1.36s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5505/5600 [40:28<02:08,  1.36s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5506/5600 [40:28<02:07,  1.36s/pipeline]Optimization Progress:  98%|█████████▊| 5508/5600 [40:32<02:04,  1.36s/pipeline]Optimization Progress: 100%|█████████▉| 5588/5600 [40:34<00:11,  1.04pipeline/s]
Generation 55 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205725389.99168563	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5601pipeline [40:34,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.04pipeline/s]Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=2 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=3 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:34,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 5601pipeline [40:35,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 5601pipeline [40:35,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5601pipeline [40:35,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:32:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5601pipeline [40:35,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5601pipeline [40:37,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5601pipeline [40:37,  1.48pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5601/5700 [40:37<01:06,  1.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5602/5700 [40:37<01:05,  1.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5603/5700 [40:37<01:05,  1.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5604/5700 [40:37<01:04,  1.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5605/5700 [40:37<01:03,  1.48pipeline/s]Optimization Progress:  98%|█████████▊| 5606/5700 [40:37<01:02,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5606/5700 [40:37<01:02,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5607/5700 [40:37<01:02,  1.50pipeline/s]Optimization Progress:  98%|█████████▊| 5610/5700 [40:41<01:08,  1.31pipeline/s]Optimization Progress: 100%|█████████▉| 5689/5700 [40:46<00:06,  1.81pipeline/s]
Generation 56 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205725389.99168563	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5701pipeline [40:47,  1.81pipeline/s]Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5701pipeline [40:47,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5701pipeline [40:48,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 5701pipeline [40:48,  2.48pipeline/s]Optimization Progress:  98%|█████████▊| 5704/5800 [40:49<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5704/5800 [40:49<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5705/5800 [40:49<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5706/5800 [40:49<00:44,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5707/5800 [40:49<00:44,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5708/5800 [40:49<00:43,  2.10pipeline/s]Optimization Progress:  98%|█████████▊| 5709/5800 [41:00<00:43,  2.10pipeline/s]Optimization Progress:  98%|█████████▊| 5710/5800 [41:07<01:53,  1.26s/pipeline]Optimization Progress: 100%|█████████▉| 5790/5800 [41:11<00:08,  1.11pipeline/s]
Generation 57 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205452101.12356657	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5801pipeline [41:11,  1.11pipeline/s]Optimization Progress: 5801pipeline [41:11,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5801pipeline [41:12,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5801pipeline [41:13,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5801pipeline [41:14,  1.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 5801pipeline [41:14,  1.57pipeline/s]Optimization Progress:  98%|█████████▊| 5805/5900 [41:14<01:02,  1.51pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5805/5900 [41:14<01:02,  1.51pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5806/5900 [41:14<01:02,  1.51pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5807/5900 [41:14<01:01,  1.51pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5808/5900 [41:14<01:00,  1.51pipeline/s]Optimization Progress:  98%|█████████▊| 5810/5900 [41:17<01:00,  1.50pipeline/s]Optimization Progress: 100%|█████████▉| 5890/5900 [41:19<00:04,  2.10pipeline/s]
Generation 58 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205452101.12356657	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 5901pipeline [41:20,  2.10pipeline/s]Optimization Progress: 5901pipeline [41:20,  2.84pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [08:33:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 5901pipeline [41:20,  2.84pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5901pipeline [41:20,  2.84pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5901pipeline [41:21,  2.84pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5901pipeline [41:22,  2.84pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5901pipeline [41:22,  2.84pipeline/s]Optimization Progress:  98%|█████████▊| 5905/6000 [41:22<00:38,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5905/6000 [41:22<00:38,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5906/6000 [41:22<00:37,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5907/6000 [41:22<00:37,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5908/6000 [41:22<00:37,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5909/6000 [41:22<00:36,  2.48pipeline/s]Optimization Progress:  99%|█████████▊| 5911/6000 [41:25<00:37,  2.35pipeline/s]Optimization Progress: 100%|█████████▉| 5991/6000 [41:28<00:02,  3.26pipeline/s]
Generation 59 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205452101.12356657	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 6001pipeline [41:28,  3.26pipeline/s]Optimization Progress: 6001pipeline [41:28,  4.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6001pipeline [41:29,  4.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6001pipeline [41:29,  4.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6001pipeline [41:29,  4.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6001pipeline [41:29,  4.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6001pipeline [41:29,  4.16pipeline/s]Optimization Progress:  98%|█████████▊| 6005/6100 [41:29<00:23,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6005/6100 [41:29<00:23,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6006/6100 [41:29<00:22,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6007/6100 [41:29<00:22,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6008/6100 [41:29<00:22,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6009/6100 [41:29<00:22,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6010/6100 [41:29<00:22,  4.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6011/6100 [41:29<00:21,  4.09pipeline/s]Optimization Progress:  99%|█████████▊| 6013/6100 [41:33<00:25,  3.38pipeline/s]Optimization Progress: 100%|█████████▉| 6093/6100 [41:34<00:01,  4.76pipeline/s]
Generation 60 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205452101.12356657	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6101pipeline [41:34,  4.76pipeline/s]Optimization Progress: 6101pipeline [41:34,  5.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6101pipeline [41:35,  5.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6101pipeline [41:36,  5.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 6101pipeline [41:36,  5.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6101pipeline [41:36,  5.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:33:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6101pipeline [41:36,  5.99pipeline/s]Optimization Progress:  98%|█████████▊| 6104/6200 [41:36<00:30,  3.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6104/6200 [41:36<00:30,  3.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6105/6200 [41:36<00:29,  3.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6106/6200 [41:36<00:29,  3.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6107/6200 [41:36<00:29,  3.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6108/6200 [41:36<00:28,  3.17pipeline/s]Optimization Progress:  99%|█████████▊| 6110/6200 [41:39<00:33,  2.72pipeline/s]Optimization Progress: 100%|█████████▉| 6190/6200 [41:40<00:02,  3.81pipeline/s]
Generation 61 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205452101.12356657	LinearSVR(FastICA(input_matrix, FastICA__tol=0.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 6201pipeline [41:40,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6201pipeline [41:41,  3.81pipeline/s]Optimization Progress: 6201pipeline [41:41,  5.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:34:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6201pipeline [41:43,  5.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6201pipeline [41:43,  5.00pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6202/6300 [41:43<00:19,  5.00pipeline/s]Optimization Progress:  98%|█████████▊| 6204/6300 [41:47<01:08,  1.41pipeline/s]Optimization Progress: 100%|█████████▉| 6284/6300 [41:47<00:07,  2.01pipeline/s]
Generation 62 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 6301pipeline [41:47,  2.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 6301pipeline [41:49,  2.01pipeline/s]Optimization Progress: 6301pipeline [41:49,  2.68pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6304/6400 [41:50<00:35,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6305/6400 [41:50<00:35,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6306/6400 [41:50<00:35,  2.68pipeline/s]Optimization Progress:  99%|█████████▊| 6307/6400 [41:50<00:30,  3.06pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6307/6400 [41:50<00:30,  3.06pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6308/6400 [41:50<00:30,  3.06pipeline/s]Optimization Progress:  99%|█████████▊| 6311/6400 [41:54<00:44,  1.98pipeline/s]Optimization Progress: 100%|█████████▉| 6390/6400 [41:54<00:03,  2.81pipeline/s]
Generation 63 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 6401pipeline [41:55,  2.81pipeline/s]Optimization Progress: 6401pipeline [41:55,  3.82pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6401pipeline [41:55,  3.82pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:34:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6401pipeline [41:56,  3.82pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6401pipeline [41:56,  3.82pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6404/6500 [41:57<00:25,  3.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6405/6500 [41:57<00:24,  3.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6406/6500 [41:57<00:24,  3.82pipeline/s]Optimization Progress:  99%|█████████▊| 6407/6500 [41:57<00:26,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6407/6500 [41:57<00:26,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6408/6500 [41:57<00:26,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6409/6500 [41:57<00:26,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6410/6500 [41:57<00:25,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6411/6500 [41:57<00:25,  3.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6412/6500 [41:57<00:25,  3.48pipeline/s]Optimization Progress:  99%|█████████▊| 6414/6500 [42:01<00:32,  2.67pipeline/s]Optimization Progress: 100%|█████████▉| 6494/6500 [42:02<00:01,  3.76pipeline/s]
Generation 64 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6501pipeline [42:02,  3.76pipeline/s]Optimization Progress: 6501pipeline [42:02,  4.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:34:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6501pipeline [42:04,  4.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 6501pipeline [42:05,  4.76pipeline/s]Optimization Progress:  99%|█████████▊| 6505/6600 [42:05<00:35,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6505/6600 [42:05<00:35,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6506/6600 [42:05<00:34,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6507/6600 [42:05<00:34,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6508/6600 [42:05<00:33,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6509/6600 [42:05<00:33,  2.71pipeline/s]Optimization Progress:  99%|█████████▊| 6511/6600 [42:09<00:40,  2.18pipeline/s]Optimization Progress: 100%|█████████▉| 6591/6600 [42:10<00:02,  3.07pipeline/s]
Generation 65 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 6601pipeline [42:11,  3.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 6601pipeline [42:11,  3.07pipeline/s]Optimization Progress: 6601pipeline [42:11,  4.22pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6601pipeline [42:12,  4.22pipeline/s]Optimization Progress:  99%|█████████▊| 6605/6700 [42:14<00:36,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6605/6700 [42:14<00:36,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6606/6700 [42:14<00:36,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6607/6700 [42:14<00:35,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6608/6700 [42:14<00:35,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6609/6700 [42:14<00:35,  2.59pipeline/s]Optimization Progress:  99%|█████████▊| 6611/6700 [42:19<00:48,  1.82pipeline/s]Optimization Progress: 100%|█████████▉| 6691/6700 [42:22<00:03,  2.53pipeline/s]
Generation 66 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:34:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6701pipeline [42:22,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 6701pipeline [42:23,  2.53pipeline/s]Optimization Progress: 6701pipeline [42:23,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6701pipeline [42:23,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6701pipeline [42:23,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6701pipeline [42:24,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 6701pipeline [42:25,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6701pipeline [42:25,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 6701pipeline [42:25,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [08:34:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 6701pipeline [42:25,  3.57pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6703/6800 [42:25<00:27,  3.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6704/6800 [42:25<00:26,  3.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6705/6800 [42:25<00:26,  3.57pipeline/s]Optimization Progress:  99%|█████████▊| 6706/6800 [42:25<00:33,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6706/6800 [42:25<00:33,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6707/6800 [42:25<00:32,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6708/6800 [42:25<00:32,  2.84pipeline/s]Optimization Progress:  99%|█████████▊| 6710/6800 [42:29<00:51,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 6790/6800 [42:32<00:04,  2.45pipeline/s]
Generation 67 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6801pipeline [42:32,  2.45pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6801pipeline [42:32,  2.45pipeline/s]Optimization Progress: 6801pipeline [42:32,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6801pipeline [42:33,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6801pipeline [42:33,  3.43pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6804/6900 [42:35<00:27,  3.43pipeline/s]Optimization Progress:  99%|█████████▊| 6805/6900 [42:35<00:37,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6805/6900 [42:35<00:37,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6806/6900 [42:35<00:37,  2.52pipeline/s]Optimization Progress:  99%|█████████▊| 6808/6900 [42:38<00:56,  1.63pipeline/s]Optimization Progress: 100%|█████████▉| 6888/6900 [42:41<00:05,  2.28pipeline/s]
Generation 68 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 6901pipeline [42:43,  2.28pipeline/s]Optimization Progress: 6901pipeline [42:43,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 6901pipeline [42:44,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 6901pipeline [42:44,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6901pipeline [42:44,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6901pipeline [42:44,  2.79pipeline/s]Optimization Progress:  99%|█████████▊| 6905/7000 [42:45<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6905/7000 [42:45<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6906/7000 [42:45<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6907/7000 [42:45<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6908/7000 [42:45<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6909/7000 [42:45<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6910/7000 [42:45<00:33,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6911/7000 [42:45<00:33,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6912/7000 [42:45<00:32,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 6913/7000 [42:45<00:32,  2.67pipeline/s]Optimization Progress:  99%|█████████▉| 6915/7000 [42:49<00:33,  2.57pipeline/s]Optimization Progress: 100%|█████████▉| 6995/7000 [42:49<00:01,  3.64pipeline/s]
Generation 69 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 7001pipeline [42:50,  3.64pipeline/s]Optimization Progress: 7001pipeline [42:50,  4.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:35:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7001pipeline [42:51,  4.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:35:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7001pipeline [42:51,  4.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [08:35:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7001pipeline [42:51,  4.41pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7004/7100 [42:52<00:21,  4.41pipeline/s]Optimization Progress:  99%|█████████▊| 7005/7100 [42:52<00:27,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7005/7100 [42:52<00:27,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7006/7100 [42:52<00:27,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7007/7100 [42:52<00:26,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7008/7100 [42:52<00:26,  3.45pipeline/s]Optimization Progress:  99%|█████████▊| 7010/7100 [42:55<00:36,  2.44pipeline/s]Optimization Progress: 100%|█████████▉| 7090/7100 [42:58<00:02,  3.39pipeline/s]
Generation 70 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 7101pipeline [42:58,  3.39pipeline/s]Optimization Progress: 7101pipeline [42:58,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7101pipeline [42:59,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7101pipeline [42:59,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7101pipeline [42:59,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7101pipeline [42:59,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7101pipeline [42:59,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 7101pipeline [43:00,  4.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7101pipeline [43:00,  4.72pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7101/7200 [43:01<00:20,  4.72pipeline/s]Optimization Progress:  99%|█████████▊| 7105/7200 [43:06<01:13,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 7183/7200 [43:08<00:09,  1.82pipeline/s]
Generation 71 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7201pipeline [43:08,  1.82pipeline/s]Optimization Progress: 7201pipeline [43:08,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7201pipeline [43:08,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7201pipeline [43:10,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:35:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7201pipeline [43:11,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 7201pipeline [43:11,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7201pipeline [43:11,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:35:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7201pipeline [43:12,  2.56pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7203/7300 [43:12<00:37,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7204/7300 [43:12<00:37,  2.56pipeline/s]Optimization Progress:  99%|█████████▊| 7205/7300 [43:12<00:51,  1.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7205/7300 [43:12<00:51,  1.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7206/7300 [43:12<00:50,  1.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7207/7300 [43:12<00:50,  1.85pipeline/s]Optimization Progress:  99%|█████████▉| 7209/7300 [43:17<01:10,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 7289/7300 [43:19<00:06,  1.81pipeline/s]
Generation 72 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7301pipeline [43:20,  1.81pipeline/s]Optimization Progress: 7301pipeline [43:20,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7301pipeline [43:20,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 7301pipeline [43:21,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7301pipeline [43:21,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7301pipeline [43:21,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 7301pipeline [43:22,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7301pipeline [43:22,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7301pipeline [43:22,  2.53pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7303/7400 [43:23<00:38,  2.53pipeline/s]Optimization Progress:  99%|█████████▊| 7304/7400 [43:23<00:56,  1.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7304/7400 [43:23<00:56,  1.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7305/7400 [43:23<00:56,  1.69pipeline/s]Optimization Progress:  99%|█████████▊| 7307/7400 [43:27<01:14,  1.25pipeline/s]Optimization Progress: 100%|█████████▉| 7387/7400 [43:28<00:07,  1.76pipeline/s]
Generation 73 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7401pipeline [43:29,  1.76pipeline/s]Optimization Progress: 7401pipeline [43:29,  2.47pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 7401pipeline [43:29,  2.47pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7401pipeline [43:29,  2.47pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7401pipeline [43:30,  2.47pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7403/7500 [43:32<00:39,  2.47pipeline/s]Optimization Progress:  99%|█████████▊| 7404/7500 [43:32<00:54,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7404/7500 [43:32<00:54,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7405/7500 [43:32<00:53,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7406/7500 [43:32<00:52,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7407/7500 [43:32<00:52,  1.77pipeline/s]Optimization Progress:  99%|█████████▉| 7408/7500 [43:50<00:51,  1.77pipeline/s]Optimization Progress:  99%|█████████▉| 7409/7500 [43:52<02:26,  1.61s/pipeline]Optimization Progress: 100%|█████████▉| 7489/7500 [43:56<00:12,  1.14s/pipeline]
Generation 74 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7501pipeline [43:58,  1.14s/pipeline]Optimization Progress: 7501pipeline [43:58,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7501pipeline [43:58,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7501pipeline [43:59,  1.18pipeline/s]Optimization Progress:  99%|█████████▊| 7503/7600 [43:59<01:24,  1.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7503/7600 [43:59<01:24,  1.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7504/7600 [43:59<01:23,  1.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7505/7600 [43:59<01:22,  1.15pipeline/s]Optimization Progress:  99%|█████████▉| 7507/7600 [44:04<01:25,  1.09pipeline/s]Optimization Progress: 100%|█████████▉| 7587/7600 [44:07<00:08,  1.53pipeline/s]
Generation 75 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7601pipeline [44:08,  1.53pipeline/s]Optimization Progress: 7601pipeline [44:08,  2.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7601pipeline [44:10,  2.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7601pipeline [44:10,  2.10pipeline/s]Optimization Progress:  99%|█████████▉| 7604/7700 [44:10<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7604/7700 [44:10<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7605/7700 [44:10<00:56,  1.68pipeline/s]Optimization Progress:  99%|█████████▉| 7607/7700 [44:15<01:22,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 7687/7700 [44:19<00:08,  1.57pipeline/s]
Generation 76 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-193174459.766619	LinearSVR(MinMaxScaler(FastICA(PCA(input_matrix, PCA__iterated_power=1, PCA__svd_solver=randomized), FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 7701pipeline [44:20,  1.57pipeline/s]Optimization Progress: 7701pipeline [44:20,  2.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7701pipeline [44:20,  2.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7701pipeline [44:21,  2.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7701pipeline [44:21,  2.20pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7702/7800 [44:22<00:44,  2.20pipeline/s]Optimization Progress:  99%|█████████▉| 7703/7800 [44:22<01:05,  1.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7703/7800 [44:22<01:05,  1.48pipeline/s]Optimization Progress:  99%|█████████▉| 7705/7800 [44:25<01:32,  1.03pipeline/s]Optimization Progress: 100%|█████████▉| 7785/7800 [44:28<00:10,  1.45pipeline/s]
Generation 77 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-193174459.766619	LinearSVR(MinMaxScaler(FastICA(PCA(input_matrix, PCA__iterated_power=1, PCA__svd_solver=randomized), FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7801pipeline [44:28,  1.45pipeline/s]Optimization Progress: 7801pipeline [44:28,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7801pipeline [44:28,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7801pipeline [44:28,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:36:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 7801pipeline [44:29,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 7801pipeline [44:29,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7801pipeline [44:29,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7801pipeline [44:29,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 7801pipeline [44:29,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 7801pipeline [44:30,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7801pipeline [44:30,  2.04pipeline/s]Optimization Progress:  99%|█████████▉| 7804/7900 [44:31<00:59,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7804/7900 [44:31<00:59,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7805/7900 [44:31<00:58,  1.61pipeline/s]Optimization Progress:  99%|█████████▉| 7807/7900 [44:35<01:22,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 7887/7900 [44:38<00:08,  1.58pipeline/s]
Generation 78 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199063896.48088413	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-193174459.766619	LinearSVR(MinMaxScaler(FastICA(PCA(input_matrix, PCA__iterated_power=1, PCA__svd_solver=randomized), FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7901pipeline [44:39,  1.58pipeline/s]Optimization Progress: 7901pipeline [44:39,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7901pipeline [44:39,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7901pipeline [44:39,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7901pipeline [44:39,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7901pipeline [44:40,  2.16pipeline/s]Optimization Progress:  99%|█████████▉| 7903/8000 [44:40<00:51,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7903/8000 [44:40<00:51,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7904/8000 [44:40<00:51,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7905/8000 [44:40<00:50,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7906/8000 [44:40<00:50,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7907/8000 [44:40<00:49,  1.87pipeline/s]Optimization Progress:  99%|█████████▉| 7909/8000 [44:45<00:58,  1.55pipeline/s]Optimization Progress: 100%|█████████▉| 7989/8000 [44:48<00:05,  2.17pipeline/s]
Generation 79 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-193237687.70036408	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-193174459.766619	LinearSVR(MinMaxScaler(FastICA(PCA(input_matrix, PCA__iterated_power=1, PCA__svd_solver=randomized), FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 8001pipeline [44:48,  2.17pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8001pipeline [44:48,  2.17pipeline/s]Optimization Progress: 8001pipeline [44:48,  3.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8001pipeline [44:49,  3.06pipeline/s]Optimization Progress:  99%|█████████▉| 8006/8100 [44:50<00:33,  2.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8006/8100 [44:50<00:33,  2.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8007/8100 [44:50<00:33,  2.80pipeline/s]Optimization Progress:  99%|█████████▉| 8009/8100 [44:59<01:41,  1.12s/pipeline]Optimization Progress: 100%|█████████▉| 8089/8100 [45:01<00:08,  1.26pipeline/s]
Generation 80 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-193237687.70036408	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-193174459.766619	LinearSVR(MinMaxScaler(FastICA(PCA(input_matrix, PCA__iterated_power=1, PCA__svd_solver=randomized), FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 8101pipeline [45:03,  1.26pipeline/s]Optimization Progress: 8101pipeline [45:03,  1.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8101pipeline [45:03,  1.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 8101pipeline [45:03,  1.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 8101pipeline [45:04,  1.70pipeline/s]Optimization Progress:  99%|█████████▉| 8104/8200 [45:04<00:56,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8104/8200 [45:04<00:56,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8105/8200 [45:04<00:55,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8106/8200 [45:04<00:55,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8107/8200 [45:04<00:54,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8108/8200 [45:04<00:53,  1.70pipeline/s]Optimization Progress:  99%|█████████▉| 8110/8200 [45:09<00:56,  1.59pipeline/s]Optimization Progress: 100%|█████████▉| 8190/8200 [45:09<00:04,  2.26pipeline/s]
Generation 81 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8201pipeline [45:11,  2.26pipeline/s]Optimization Progress: 8201pipeline [45:11,  2.90pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8201pipeline [45:11,  2.90pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8201/8300 [45:12<00:34,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8202/8300 [45:12<00:33,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8203/8300 [45:12<00:33,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8204/8300 [45:12<00:33,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 8205/8300 [45:12<00:30,  3.07pipeline/s]Optimization Progress:  99%|█████████▉| 8208/8300 [45:16<01:02,  1.47pipeline/s]Optimization Progress: 100%|█████████▉| 8286/8300 [45:17<00:06,  2.08pipeline/s]
Generation 82 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 8301pipeline [45:18,  2.08pipeline/s]Optimization Progress: 8301pipeline [45:18,  2.79pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8301/8400 [45:20<00:35,  2.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8302/8400 [45:20<00:35,  2.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8303/8400 [45:20<00:34,  2.79pipeline/s]Optimization Progress:  99%|█████████▉| 8305/8400 [45:26<01:21,  1.17pipeline/s]Optimization Progress: 100%|█████████▉| 8385/8400 [45:30<00:09,  1.63pipeline/s]
Generation 83 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8401pipeline [45:30,  1.63pipeline/s]Optimization Progress: 8401pipeline [45:30,  2.30pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 8401pipeline [45:30,  2.30pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8401pipeline [45:31,  2.30pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:37:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 8401pipeline [45:31,  2.30pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8401pipeline [45:31,  2.30pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8401pipeline [45:31,  2.30pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8401/8500 [45:33<00:43,  2.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8402/8500 [45:33<00:42,  2.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8403/8500 [45:33<00:42,  2.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8404/8500 [45:33<00:41,  2.30pipeline/s]Optimization Progress:  99%|█████████▉| 8405/8500 [45:33<00:46,  2.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8405/8500 [45:33<00:46,  2.04pipeline/s]Optimization Progress:  99%|█████████▉| 8408/8500 [45:37<01:09,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 8487/8500 [45:41<00:07,  1.84pipeline/s]
Generation 84 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 8501pipeline [45:42,  1.84pipeline/s]Optimization Progress: 8501pipeline [45:42,  2.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8501pipeline [45:42,  2.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8501pipeline [45:42,  2.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8501pipeline [45:43,  2.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 8501pipeline [45:43,  2.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8501pipeline [45:44,  2.61pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8502/8600 [45:44<00:37,  2.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8503/8600 [45:44<00:37,  2.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8504/8600 [45:44<00:36,  2.61pipeline/s]Optimization Progress:  99%|█████████▉| 8505/8600 [45:44<00:42,  2.21pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8505/8600 [45:44<00:42,  2.21pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8506/8600 [45:44<00:42,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 8507/8600 [46:00<00:42,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 8508/8600 [46:01<03:04,  2.01s/pipeline]Optimization Progress: 100%|█████████▉| 8588/8600 [46:02<00:16,  1.41s/pipeline]
Generation 85 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 8601pipeline [46:02,  1.41s/pipeline]Optimization Progress: 8601pipeline [46:02,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8601pipeline [46:02,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 8601pipeline [46:03,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8601pipeline [46:03,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [46:03,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8601pipeline [46:04,  1.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [46:04,  1.00s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8601/8700 [46:04<01:39,  1.00s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8602/8700 [46:04<01:38,  1.00s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8603/8700 [46:04<01:37,  1.00s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8604/8700 [46:04<01:36,  1.00s/pipeline]Optimization Progress:  99%|█████████▉| 8605/8700 [46:04<01:22,  1.16pipeline/s]Optimization Progress:  99%|█████████▉| 8608/8700 [46:09<01:43,  1.12s/pipeline]Optimization Progress: 100%|█████████▉| 8686/8700 [46:14<00:11,  1.25pipeline/s]
Generation 86 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 8701pipeline [46:15,  1.25pipeline/s]Optimization Progress: 8701pipeline [46:15,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [08:38:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 8701pipeline [46:15,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8701pipeline [46:16,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 8701pipeline [46:16,  1.72pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8702/8800 [46:17<00:57,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 8703/8800 [46:17<01:00,  1.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8703/8800 [46:17<01:00,  1.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8704/8800 [46:17<01:00,  1.59pipeline/s]Optimization Progress:  99%|█████████▉| 8706/8800 [46:19<01:08,  1.37pipeline/s]Optimization Progress: 100%|█████████▉| 8786/8800 [46:22<00:07,  1.92pipeline/s]
Generation 87 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8801pipeline [46:23,  1.92pipeline/s]Optimization Progress: 8801pipeline [46:23,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8801pipeline [46:25,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8801pipeline [46:25,  2.60pipeline/s]Optimization Progress:  99%|█████████▉| 8804/8900 [46:25<00:47,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8804/8900 [46:25<00:47,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8805/8900 [46:25<00:47,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8806/8900 [46:25<00:46,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8807/8900 [46:25<00:46,  2.01pipeline/s]Optimization Progress:  99%|█████████▉| 8809/8900 [46:30<00:57,  1.58pipeline/s]Optimization Progress: 100%|█████████▉| 8889/8900 [46:32<00:04,  2.22pipeline/s]
Generation 88 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8901pipeline [46:32,  2.22pipeline/s]Optimization Progress: 8901pipeline [46:32,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 8901pipeline [46:33,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8901pipeline [46:33,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8901pipeline [46:34,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:38:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 8901pipeline [46:34,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8901pipeline [46:34,  3.10pipeline/s]Optimization Progress:  99%|█████████▉| 8905/9000 [46:34<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8905/9000 [46:34<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8906/9000 [46:34<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8907/9000 [46:34<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8908/9000 [46:34<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8909/9000 [46:34<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8910/9000 [46:34<00:33,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8911/9000 [46:34<00:33,  2.67pipeline/s]Optimization Progress:  99%|█████████▉| 8913/9000 [46:37<00:32,  2.68pipeline/s]Optimization Progress: 100%|█████████▉| 8993/9000 [46:41<00:01,  3.66pipeline/s]
Generation 89 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9001pipeline [46:41,  3.66pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9001pipeline [46:41,  3.66pipeline/s]Optimization Progress: 9001pipeline [46:41,  5.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 9001pipeline [46:42,  5.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:39:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9001pipeline [46:42,  5.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:39:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9001pipeline [46:42,  5.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 9001pipeline [46:43,  5.05pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:39:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9001pipeline [46:44,  5.05pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9001/9100 [46:44<00:19,  5.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9002/9100 [46:44<00:19,  5.05pipeline/s]Optimization Progress:  99%|█████████▉| 9004/9100 [46:47<01:14,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 9084/9100 [46:49<00:08,  1.83pipeline/s]
Generation 90 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9101pipeline [46:49,  1.83pipeline/s]Optimization Progress: 9101pipeline [46:49,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 9101pipeline [46:49,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9101pipeline [46:49,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9101pipeline [46:51,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9101pipeline [46:51,  2.60pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 9101pipeline [46:52,  2.60pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9101/9200 [46:52<00:38,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9102/9200 [46:52<00:37,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9103/9200 [46:52<00:37,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9104/9200 [46:52<00:36,  2.60pipeline/s]Optimization Progress:  99%|█████████▉| 9108/9200 [46:57<00:54,  1.68pipeline/s]Optimization Progress: 100%|█████████▉| 9186/9200 [46:59<00:05,  2.35pipeline/s]
Generation 91 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9201pipeline [46:59,  2.35pipeline/s]Optimization Progress: 9201pipeline [46:59,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9201pipeline [47:00,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9201pipeline [47:01,  3.25pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9201/9300 [47:02<00:30,  3.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9202/9300 [47:02<00:30,  3.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9203/9300 [47:02<00:29,  3.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9204/9300 [47:02<00:29,  3.25pipeline/s]Optimization Progress:  99%|█████████▉| 9206/9300 [47:06<00:55,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 9286/9300 [47:08<00:05,  2.37pipeline/s]
Generation 92 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-192103582.44288212	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9301pipeline [47:08,  2.37pipeline/s]Optimization Progress: 9301pipeline [47:08,  3.29pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9301pipeline [47:08,  3.29pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9301pipeline [47:10,  3.29pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9301pipeline [47:10,  3.29pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9301/9400 [47:11<00:30,  3.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9302/9400 [47:11<00:29,  3.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9303/9400 [47:11<00:29,  3.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9304/9400 [47:11<00:29,  3.29pipeline/s]Optimization Progress:  99%|█████████▉| 9305/9400 [47:11<00:37,  2.54pipeline/s]Optimization Progress:  99%|█████████▉| 9308/9400 [47:15<01:03,  1.45pipeline/s]Optimization Progress: 100%|█████████▉| 9386/9400 [47:17<00:06,  2.04pipeline/s]
Generation 93 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-192103582.44288212	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9401pipeline [47:17,  2.04pipeline/s]Optimization Progress: 9401pipeline [47:17,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 9401pipeline [47:18,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9401pipeline [47:18,  2.79pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9401pipeline [47:19,  2.79pipeline/s]Optimization Progress:  99%|█████████▉| 9405/9500 [47:20<00:41,  2.32pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9405/9500 [47:20<00:41,  2.32pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9406/9500 [47:20<00:40,  2.32pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9407/9500 [47:20<00:40,  2.32pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9408/9500 [47:20<00:39,  2.32pipeline/s]Optimization Progress:  99%|█████████▉| 9410/9500 [47:31<01:26,  1.04pipeline/s]Optimization Progress: 100%|█████████▉| 9490/9500 [47:33<00:06,  1.46pipeline/s]
Generation 94 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-192103582.44288212	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 9501pipeline [47:34,  1.46pipeline/s]Optimization Progress: 9501pipeline [47:34,  1.97pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9501pipeline [47:35,  1.97pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [08:39:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9501pipeline [47:35,  1.97pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9501pipeline [47:36,  1.97pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9502/9600 [47:36<00:49,  1.97pipeline/s]Optimization Progress:  99%|█████████▉| 9503/9600 [47:36<01:03,  1.53pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9503/9600 [47:36<01:03,  1.53pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9504/9600 [47:36<01:02,  1.53pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9505/9600 [47:36<01:01,  1.53pipeline/s]Optimization Progress:  99%|█████████▉| 9507/9600 [47:40<01:05,  1.41pipeline/s]Optimization Progress: 100%|█████████▉| 9587/9600 [47:40<00:06,  2.01pipeline/s]
Generation 95 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-192103582.44288212	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [08:40:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9601pipeline [47:40,  2.01pipeline/s]Optimization Progress: 9601pipeline [47:40,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9601pipeline [47:41,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 9601pipeline [47:42,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:40:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9601pipeline [47:42,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [47:43,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9601pipeline [47:43,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [47:43,  2.85pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9601/9700 [47:44<00:34,  2.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9602/9700 [47:44<00:34,  2.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9603/9700 [47:44<00:34,  2.85pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9604/9700 [47:44<00:33,  2.85pipeline/s]Optimization Progress:  99%|█████████▉| 9608/9700 [47:48<00:52,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 9686/9700 [47:51<00:05,  2.44pipeline/s]
Generation 96 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-191795214.851522	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.45))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 9701pipeline [47:51,  2.44pipeline/s]Optimization Progress: 9701pipeline [47:51,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9701pipeline [47:51,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9701pipeline [47:52,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:40:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9701pipeline [47:52,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 9701pipeline [47:52,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [08:40:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f16b38d3dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f16b39e4669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f16b39f1f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f16b39d8cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f16b38c5f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0cc16809dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0cc1680067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f0cc169827e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f0cc1698cb4]

.
Optimization Progress: 9701pipeline [47:53,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9701pipeline [47:53,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 9701pipeline [47:53,  3.38pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 9701pipeline [47:53,  3.38pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9701/9800 [47:54<00:29,  3.38pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9702/9800 [47:54<00:29,  3.38pipeline/s]Optimization Progress:  99%|█████████▉| 9705/9800 [48:04<01:50,  1.16s/pipeline]Optimization Progress: 100%|█████████▉| 9784/9800 [48:08<00:13,  1.21pipeline/s]
Generation 97 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-191795214.851522	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.45))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9801pipeline [48:08,  1.21pipeline/s]Optimization Progress: 9801pipeline [48:08,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9801pipeline [48:08,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 9801pipeline [48:09,  1.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9801pipeline [48:10,  1.72pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9801/9900 [48:11<00:57,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9802/9900 [48:11<00:57,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9803/9900 [48:11<00:56,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 9805/9900 [48:14<01:24,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 9885/9900 [48:17<00:09,  1.58pipeline/s]
Generation 98 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-191795214.851522	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.45))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9901pipeline [48:17,  1.58pipeline/s]Optimization Progress: 9901pipeline [48:17,  2.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 9901pipeline [48:17,  2.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 9901pipeline [48:18,  2.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9901pipeline [48:18,  2.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9901pipeline [48:18,  2.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9901pipeline [48:20,  2.21pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9902/10000 [48:20<00:44,  2.21pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9903/10000 [48:20<00:43,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 9904/10000 [48:20<00:55,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 9906/10000 [48:26<01:58,  1.26s/pipeline]Optimization Progress: 100%|█████████▉| 9985/10000 [48:28<00:13,  1.12pipeline/s]
Generation 99 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-191795214.851522	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.45))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [48:29,  1.12pipeline/s]Optimization Progress: 10001pipeline [48:29,  1.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 10001pipeline [48:29,  1.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [48:29,  1.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10001pipeline [48:31,  1.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [48:31,  1.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10001pipeline [48:31,  1.57pipeline/s]Optimization Progress:  99%|█████████▉| 10005/10100 [48:31<00:59,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10005/10100 [48:31<00:59,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10006/10100 [48:31<00:58,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10007/10100 [48:31<00:57,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10008/10100 [48:31<00:57,  1.61pipeline/s]Optimization Progress:  99%|█████████▉| 10010/10100 [48:39<01:20,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 10090/10100 [48:41<00:06,  1.57pipeline/s]
Generation 100 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205386766.28384495	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-192374839.08477044	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.6000000000000001)), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-191795214.851522	LinearSVR(MinMaxScaler(MaxAbsScaler(FastICA(input_matrix, FastICA__tol=0.45))), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  Best pipeline:
0. FastICA(tol=0.45)
1. MaxAbsScaler()
2. MinMaxScaler()
3. LinearSVR(C=20.0, epsilon=0.0001, tol=0.01)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
