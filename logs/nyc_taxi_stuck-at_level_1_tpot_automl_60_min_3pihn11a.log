30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   9%|▉         | 9/100 [00:06<01:04,  1.42pipeline/s]Optimization Progress:  89%|████████▉ | 89/100 [00:07<00:05,  2.01pipeline/s]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:08<00:00,  2.01pipeline/s]Optimization Progress: 100%|██████████| 100/100 [00:08<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:08<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:10<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  2.77pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  2.77pipeline/s]Optimization Progress:  53%|█████▎    | 106/200 [00:11<00:41,  2.26pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  53%|█████▎    | 106/200 [00:11<00:41,  2.26pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  54%|█████▎    | 107/200 [00:11<00:41,  2.26pipeline/s]Optimization Progress:  55%|█████▍    | 109/200 [00:16<01:09,  1.31pipeline/s]Optimization Progress:  94%|█████████▍| 189/200 [00:19<00:05,  1.83pipeline/s]
Generation 1 - Current Pareto front scores:
-1	-359341990.89345485	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  1.83pipeline/s]Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  2.48pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  2.48pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 200/200 [00:22<00:00,  2.48pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 200/200 [00:22<00:00,  2.48pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [16:18:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 200/200 [00:22<00:00,  2.48pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  67%|██████▋   | 201/300 [00:24<00:39,  2.48pipeline/s]Optimization Progress:  67%|██████▋   | 202/300 [00:24<01:25,  1.15pipeline/s]Optimization Progress:  68%|██████▊   | 204/300 [00:27<01:39,  1.04s/pipeline]Optimization Progress:  94%|█████████▍| 283/300 [00:29<00:12,  1.36pipeline/s]
Generation 2 - Current Pareto front scores:
-1	-359341990.89345485	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:30<00:00,  1.36pipeline/s]Optimization Progress: 100%|██████████| 300/300 [00:30<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:31<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:31<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:33<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [00:33<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 300/300 [00:34<00:00,  1.87pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 300/300 [00:34<00:00,  1.87pipeline/s]Optimization Progress:  76%|███████▌  | 304/400 [00:36<01:14,  1.28pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▌  | 304/400 [00:36<01:14,  1.28pipeline/s]Optimization Progress:  76%|███████▋  | 305/400 [00:50<01:14,  1.28pipeline/s]Optimization Progress:  76%|███████▋  | 306/400 [01:47<17:31, 11.19s/pipeline]Optimization Progress:  96%|█████████▋| 386/400 [01:49<01:49,  7.84s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-359341990.89345485	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [01:51<00:00,  7.84s/pipeline]Optimization Progress: 100%|██████████| 400/400 [01:51<00:00,  5.52s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [01:52<00:00,  5.52s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [01:53<00:00,  5.52s/pipeline]Optimization Progress:  80%|████████  | 401/500 [01:54<07:55,  4.80s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  80%|████████  | 401/500 [01:54<07:55,  4.80s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  80%|████████  | 402/500 [01:54<07:50,  4.80s/pipeline]Optimization Progress:  81%|████████  | 404/500 [02:28<10:50,  6.78s/pipeline]Optimization Progress:  97%|█████████▋| 484/500 [02:31<01:16,  4.76s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 500/500 [02:31<00:00,  4.76s/pipeline]Optimization Progress: 100%|██████████| 500/500 [02:31<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 500/500 [02:32<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [02:32<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 500/500 [02:33<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 500/500 [02:33<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 500/500 [02:33<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [02:35<00:00,  3.34s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 500/500 [02:36<00:00,  3.34s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [02:36<05:08,  3.15s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▎ | 502/600 [02:36<05:08,  3.15s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▍ | 503/600 [02:37<05:05,  3.15s/pipeline]Optimization Progress:  84%|████████▍ | 505/600 [02:48<05:13,  3.30s/pipeline]Optimization Progress:  98%|█████████▊| 585/600 [02:51<00:34,  2.33s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [02:51<00:00,  2.33s/pipeline]Optimization Progress: 100%|██████████| 600/600 [02:51<00:00,  1.63s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [02:52<00:00,  1.63s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 600/600 [02:53<00:00,  1.63s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 600/600 [02:53<00:00,  1.63s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 600/600 [02:54<00:00,  1.63s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [16:20:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 600/600 [02:54<00:00,  1.63s/pipeline]Optimization Progress:  86%|████████▌ | 603/700 [02:56<02:39,  1.65s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▌ | 603/700 [02:56<02:39,  1.65s/pipeline]Optimization Progress:  86%|████████▋ | 604/700 [03:10<02:38,  1.65s/pipeline]Optimization Progress:  86%|████████▋ | 605/700 [03:10<05:03,  3.20s/pipeline]Optimization Progress:  98%|█████████▊| 685/700 [03:13<00:33,  2.25s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 700/700 [03:14<00:00,  2.25s/pipeline]Optimization Progress: 100%|██████████| 700/700 [03:14<00:00,  1.60s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 100%|██████████| 700/700 [03:15<00:00,  1.60s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [03:15<00:00,  1.60s/pipeline]Optimization Progress:  88%|████████▊ | 702/800 [03:19<02:53,  1.77s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 702/800 [03:19<02:53,  1.77s/pipeline]Optimization Progress:  88%|████████▊ | 704/800 [03:45<08:13,  5.14s/pipeline]Optimization Progress:  98%|█████████▊| 784/800 [03:58<00:58,  3.65s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [03:59<00:00,  3.65s/pipeline]Optimization Progress: 100%|██████████| 800/800 [03:59<00:00,  2.57s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 800/800 [04:00<00:00,  2.57s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [04:00<00:00,  2.57s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 800/800 [04:01<00:00,  2.57s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..
Optimization Progress: 100%|██████████| 800/800 [04:01<00:00,  2.57s/pipeline]Optimization Progress:  89%|████████▉ | 801/900 [04:02<04:37,  2.80s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [04:29<16:10,  9.90s/pipeline]Optimization Progress:  98%|█████████▊| 882/900 [04:31<02:04,  6.94s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 900/900 [04:32<00:00,  6.94s/pipeline]Optimization Progress: 100%|██████████| 900/900 [04:32<00:00,  4.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [04:34<00:00,  4.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [04:34<00:00,  4.88s/pipeline]Optimization Progress:  90%|█████████ | 901/1000 [04:50<08:02,  4.88s/pipeline]Optimization Progress:  90%|█████████ | 902/1000 [05:02<12:45,  7.81s/pipeline]Optimization Progress:  98%|█████████▊| 982/1000 [05:05<01:38,  5.48s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 [16:23:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [05:06<00:00,  5.48s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [05:06<00:00,  3.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [05:06<00:00,  3.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [05:07<00:00,  3.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1000/1000 [05:08<00:00,  3.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [05:08<00:00,  3.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [05:08<00:00,  3.84s/pipeline]Optimization Progress:  91%|█████████ | 1002/1100 [05:08<05:00,  3.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  91%|█████████ | 1002/1100 [05:08<05:00,  3.06s/pipeline]Optimization Progress:  91%|█████████ | 1003/1100 [05:20<04:56,  3.06s/pipeline]Optimization Progress:  91%|█████████▏| 1004/1100 [05:33<09:20,  5.84s/pipeline]Optimization Progress:  99%|█████████▊| 1084/1100 [05:43<01:06,  4.13s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217542714.11824536	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  4.13s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [05:44<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [05:45<00:00,  2.90s/pipeline]Optimization Progress:  92%|█████████▏| 1104/1200 [05:46<03:29,  2.18s/pipeline]Optimization Progress:  92%|█████████▏| 1105/1200 [06:27<21:59, 13.89s/pipeline]Optimization Progress:  99%|█████████▉| 1185/1200 [06:41<02:26,  9.78s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-354430467.84295344	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217534108.4653058	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  9.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  9.78s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [06:41<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:42<00:00,  6.85s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:43<00:00,  6.85s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1203/1300 [06:43<11:04,  6.85s/pipeline]Optimization Progress:  93%|█████████▎| 1204/1300 [07:00<10:57,  6.85s/pipeline]Optimization Progress:  93%|█████████▎| 1205/1300 [07:09<10:14,  6.46s/pipeline]Optimization Progress:  99%|█████████▉| 1285/1300 [07:23<01:08,  4.58s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-354139907.41786724	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217534108.4653058	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204947254.91314107	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:25:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 1300/1300 [07:23<00:00,  4.58s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [07:23<00:00,  3.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1300/1300 [07:24<00:00,  3.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [07:24<00:00,  3.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 1300/1300 [07:25<00:00,  3.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1300/1300 [07:25<00:00,  3.22s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1301/1400 [07:26<05:18,  3.22s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [07:26<04:13,  2.58s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1302/1400 [07:26<04:13,  2.58s/pipeline]Optimization Progress:  93%|█████████▎| 1304/1400 [10:21<44:52, 28.05s/pipeline]Optimization Progress:  99%|█████████▉| 1384/1400 [10:31<05:14, 19.67s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-354139907.41786724	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217534108.4653058	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-3	-217519495.1826833	LinearSVR(FastICA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=15, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100), FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-204947254.91314107	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-204931999.29642016	LinearSVR(MaxAbsScaler(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 1400/1400 [10:31<00:00, 19.67s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [10:31<00:00, 13.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1400/1400 [10:32<00:00, 13.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1400/1400 [10:32<00:00, 13.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 1400/1400 [10:32<00:00, 13.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:28:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 1400/1400 [10:33<00:00, 13.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:28:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 100%|██████████| 1400/1400 [10:33<00:00, 13.78s/pipeline]Optimization Progress:  94%|█████████▍| 1407/1500 [10:34<15:06,  9.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1407/1500 [10:34<15:06,  9.74s/pipeline]Optimization Progress:  94%|█████████▍| 1408/1500 [10:50<14:56,  9.74s/pipeline]Optimization Progress:  94%|█████████▍| 1409/1500 [10:54<15:01,  9.90s/pipeline]Optimization Progress:  99%|█████████▉| 1489/1500 [13:30<01:22,  7.52s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-354139907.41786724	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217336671.42243218	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-204947254.91314107	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-204931999.29642016	LinearSVR(MaxAbsScaler(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [13:31<00:00,  7.52s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [13:31<00:00,  5.29s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1500/1500 [13:32<00:00,  5.29s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1500/1500 [13:32<00:00,  5.29s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1500/1500 [13:32<00:00,  5.29s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 1500/1500 [13:33<00:00,  5.29s/pipeline]Optimization Progress:  94%|█████████▍| 1502/1600 [13:34<06:48,  4.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1502/1600 [13:34<06:48,  4.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1503/1600 [13:34<06:44,  4.17s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress:  94%|█████████▍| 1505/1600 [18:38<52:39, 33.26s/pipeline]                                                                                Skipped pipeline #1571 due to time out. Continuing to the next pipeline.
Optimization Progress:  98%|█████████▊| 1571/1600 [18:38<16:04, 33.26s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress:  99%|█████████▉| 1586/1600 [18:56<05:26, 23.35s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-354139907.41786724	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-216282992.71604142	LinearSVR(FastICA(input_matrix, FastICA__tol=0.9), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-210224537.5833694	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204947254.91314107	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-204926164.62763873	LinearSVR(OneHotEncoder(Normalizer(FastICA(MinMaxScaler(input_matrix), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1601pipeline [18:57, 23.35s/pipeline]Optimization Progress: 1601pipeline [18:57, 16.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1601pipeline [18:57, 16.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1601pipeline [18:58, 16.37s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1601pipeline [18:58, 16.37s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [18:59<18:56, 11.72s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1603/1700 [18:59<18:56, 11.72s/pipeline]Optimization Progress:  94%|█████████▍| 1605/1700 [23:59<1:24:21, 53.28s/pipeline]                                                                                  Skipped pipeline #1623 due to time out. Continuing to the next pipeline.
Optimization Progress:  95%|█████████▌| 1623/1700 [23:59<1:08:22, 53.28s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress:  99%|█████████▉| 1686/1700 [24:03<08:42, 37.31s/pipeline]  
Generation 16 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204906898.5630713	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=l1), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1702pipeline [24:03, 37.31s/pipeline]Optimization Progress: 1702pipeline [24:03, 26.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1702pipeline [24:03, 26.13s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1702pipeline [24:04, 26.13s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1702pipeline [24:04, 26.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 1702pipeline [24:05, 26.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1702pipeline [24:05, 26.13s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1702pipeline [24:06, 26.13s/pipeline]Optimization Progress:  95%|█████████▍| 1704/1800 [24:06<29:54, 18.69s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1704/1800 [24:06<29:54, 18.69s/pipeline]Optimization Progress:  95%|█████████▍| 1706/1800 [24:11<21:43, 13.87s/pipeline]Optimization Progress:  99%|█████████▉| 1786/1800 [24:14<02:16,  9.72s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204906898.5630713	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=l1), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1802pipeline [24:14,  9.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1802pipeline [24:15,  9.72s/pipeline]Optimization Progress: 1802pipeline [24:15,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1802pipeline [24:15,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:42:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 1802pipeline [24:15,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1802pipeline [24:15,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1802pipeline [24:16,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1802pipeline [24:16,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 1802pipeline [24:16,  6.81s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:42:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 1802pipeline [24:17,  6.81s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1803/1900 [24:17<11:00,  6.81s/pipeline]Optimization Progress:  95%|█████████▍| 1804/1900 [24:17<08:07,  5.07s/pipeline]Optimization Progress:  95%|█████████▌| 1806/1900 [24:22<06:52,  4.39s/pipeline]Optimization Progress:  99%|█████████▉| 1885/1900 [24:40<00:47,  3.14s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204906898.5630713	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=l1), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1902pipeline [24:40,  3.14s/pipeline]Optimization Progress: 1902pipeline [24:40,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 1902pipeline [24:40,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 1902pipeline [24:41,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 1902pipeline [24:41,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1902pipeline [24:42,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values..
Optimization Progress: 1902pipeline [24:42,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1902pipeline [24:42,  2.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1902pipeline [24:42,  2.20s/pipeline]Optimization Progress:  95%|█████████▌| 1905/2000 [24:49<03:52,  2.45s/pipeline]Optimization Progress:  99%|█████████▉| 1984/2000 [24:53<00:27,  1.73s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204906898.5630713	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=l1), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-191588221.59310693	SGDRegressor(Normalizer(MaxAbsScaler(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14))), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..
Optimization Progress: 2002pipeline [24:54,  1.73s/pipeline]Optimization Progress: 2002pipeline [24:54,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2002pipeline [24:54,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 2002pipeline [24:54,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2002pipeline [24:55,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2002pipeline [24:55,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2002pipeline [24:55,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2002pipeline [24:55,  1.22s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2002pipeline [24:55,  1.22s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2003/2100 [24:56<01:58,  1.22s/pipeline]Optimization Progress:  95%|█████████▌| 2004/2100 [25:10<01:56,  1.22s/pipeline]Optimization Progress:  95%|█████████▌| 2005/2100 [25:22<05:47,  3.66s/pipeline]Optimization Progress:  99%|█████████▉| 2085/2100 [25:23<00:38,  2.56s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-204906898.5630713	LinearSVR(OneHotEncoder(Normalizer(FastICA(input_matrix, FastICA__tol=0.9), Normalizer__norm=l1), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-191588221.59310693	SGDRegressor(Normalizer(MaxAbsScaler(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14))), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2102pipeline [25:23,  2.56s/pipeline]Optimization Progress: 2102pipeline [25:23,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:43:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2102pipeline [25:24,  1.80s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2102pipeline [25:25,  1.80s/pipeline]Optimization Progress:  96%|█████████▌| 2106/2200 [25:25<02:11,  1.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2106/2200 [25:25<02:11,  1.40s/pipeline]Optimization Progress:  96%|█████████▌| 2107/2200 [25:40<02:09,  1.40s/pipeline]Optimization Progress:  96%|█████████▌| 2108/2200 [25:45<06:06,  3.98s/pipeline]Optimization Progress:  99%|█████████▉| 2188/2200 [26:02<00:34,  2.85s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-191588614.50367996	SGDRegressor(Normalizer(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14)), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-5	-191588221.59310693	SGDRegressor(Normalizer(MaxAbsScaler(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14))), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 2202pipeline [26:02,  2.85s/pipeline]Optimization Progress: 2202pipeline [26:02,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2202pipeline [26:02,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2202pipeline [26:02,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2202pipeline [26:03,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2202pipeline [26:03,  2.00s/pipeline]Optimization Progress:  96%|█████████▌| 2204/2300 [26:03<02:33,  1.59s/pipeline]Optimization Progress:  96%|█████████▌| 2205/2300 [26:08<03:48,  2.41s/pipeline]Optimization Progress:  99%|█████████▉| 2285/2300 [26:10<00:25,  1.69s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-215993803.2651933	LinearSVR(FastICA(input_matrix, FastICA__tol=0.2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-208693600.20938498	LinearSVR(Nystroem(RBFSampler(input_matrix, RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.5, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-191588614.50367996	SGDRegressor(Normalizer(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14)), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-5	-191588221.59310693	SGDRegressor(Normalizer(MaxAbsScaler(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14))), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2302pipeline [26:11,  1.69s/pipeline]Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:44:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2302pipeline [26:11,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 2302pipeline [26:12,  1.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 2302pipeline [26:12,  1.20s/pipeline]Optimization Progress:  96%|█████████▌| 2306/2400 [26:12<01:28,  1.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2306/2400 [26:12<01:28,  1.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2307/2400 [26:12<01:27,  1.07pipeline/s]Optimization Progress:  96%|█████████▌| 2309/2400 [26:29<03:33,  2.35s/pipeline]Optimization Progress: 100%|█████████▉| 2389/2400 [26:31<00:18,  1.65s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205650513.63733792	LinearSVR(FastICA(input_matrix, FastICA__tol=0.15000000000000002), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-191588614.50367996	SGDRegressor(Normalizer(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14)), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-5	-189444874.7712676	LinearSVR(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.65s/pipeline]Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=2 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=3 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=4 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=5 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=6 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=7 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=8 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=9 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2402pipeline [26:32,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:33,  1.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 2402pipeline [26:33,  1.18s/pipeline]Optimization Progress:  96%|█████████▌| 2404/2500 [26:33<01:31,  1.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2404/2500 [26:33<01:31,  1.05pipeline/s]Optimization Progress:  96%|█████████▌| 2406/2500 [28:05<22:38, 14.45s/pipeline]Optimization Progress:  99%|█████████▉| 2486/2500 [28:07<02:21, 10.12s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205650513.63733792	LinearSVR(FastICA(input_matrix, FastICA__tol=0.15000000000000002), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-191588042.5891054	SGDRegressor(Normalizer(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14)), Normalizer__norm=max), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-5	-189444874.7712676	LinearSVR(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2502pipeline [28:07, 10.12s/pipeline]Optimization Progress: 2502pipeline [28:07,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 2502pipeline [28:07,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:46:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2502pipeline [28:07,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:46:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2502pipeline [28:07,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 2502pipeline [28:08,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2502pipeline [28:08,  7.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 2502pipeline [28:09,  7.09s/pipeline]Optimization Progress:  96%|█████████▋| 2505/2600 [28:09<08:06,  5.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2505/2600 [28:09<08:06,  5.12s/pipeline]Optimization Progress:  96%|█████████▋| 2507/2600 [28:11<06:01,  3.89s/pipeline]Optimization Progress: 100%|█████████▉| 2587/2600 [28:13<00:35,  2.73s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205650513.63733792	LinearSVR(FastICA(input_matrix, FastICA__tol=0.15000000000000002), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-191359083.14357218	SGDRegressor(Normalizer(MaxAbsScaler(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=14)), Normalizer__norm=max), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0)
-5	-189239041.355588	LinearSVR(Nystroem(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-187959131.68087286	LinearSVR(OneHotEncoder(Normalizer(FastICA(LassoLarsCV(MinMaxScaler(input_matrix), LassoLarsCV__normalize=False), FastICA__tol=0.9), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 2602pipeline [28:14,  2.73s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:46:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2602pipeline [28:14,  2.73s/pipeline]Optimization Progress: 2602pipeline [28:14,  1.92s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2602pipeline [28:14,  1.92s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2602pipeline [28:15,  1.92s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 2602pipeline [28:15,  1.92s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2603/2700 [28:15<03:05,  1.92s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2604/2700 [28:15<03:03,  1.92s/pipeline]Optimization Progress:  97%|█████████▋| 2606/2700 [28:19<02:44,  1.75s/pipeline]Optimization Progress:  99%|█████████▉| 2685/2700 [28:30<00:26,  1.75s/pipeline]Optimization Progress:  99%|█████████▉| 2686/2700 [28:35<00:18,  1.29s/pipeline]
Generation 26 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205650513.63733792	LinearSVR(FastICA(input_matrix, FastICA__tol=0.15000000000000002), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-180189763.51789385	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2702pipeline [28:36,  1.29s/pipeline]Optimization Progress: 2702pipeline [28:36,  1.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2702pipeline [28:36,  1.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:46:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2702pipeline [28:36,  1.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 2702pipeline [28:37,  1.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 2702pipeline [28:37,  1.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2702pipeline [28:37,  1.09pipeline/s]Optimization Progress:  97%|█████████▋| 2705/2800 [28:37<01:07,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2705/2800 [28:37<01:07,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2706/2800 [28:37<01:07,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2707/2800 [28:37<01:06,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2708/2800 [28:37<01:05,  1.40pipeline/s]Optimization Progress:  97%|█████████▋| 2710/2800 [28:55<02:21,  1.57s/pipeline]Optimization Progress: 100%|█████████▉| 2790/2800 [28:59<00:11,  1.11s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-180189763.51789385	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2802pipeline [29:00,  1.11s/pipeline]Optimization Progress: 2802pipeline [29:00,  1.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 2802pipeline [29:00,  1.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 2802pipeline [29:00,  1.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2802pipeline [29:01,  1.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 2802pipeline [29:01,  1.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2802pipeline [29:01,  1.27pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2802/2900 [29:02<01:17,  1.27pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2803/2900 [29:02<01:16,  1.27pipeline/s]Optimization Progress:  97%|█████████▋| 2804/2900 [29:02<01:20,  1.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2804/2900 [29:02<01:20,  1.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2805/2900 [29:02<01:20,  1.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2806/2900 [29:02<01:19,  1.19pipeline/s]Optimization Progress:  97%|█████████▋| 2808/2900 [29:44<05:45,  3.75s/pipeline]Optimization Progress: 100%|█████████▉| 2888/2900 [29:46<00:31,  2.64s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-179577027.6095084	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 2902pipeline [29:46,  2.64s/pipeline]Optimization Progress: 2902pipeline [29:46,  1.85s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2902pipeline [29:47,  1.85s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 2902pipeline [29:47,  1.85s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2902pipeline [29:47,  1.85s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2902pipeline [29:47,  1.85s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:47:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2902pipeline [29:48,  1.85s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:47:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 2902pipeline [29:48,  1.85s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2904/3000 [29:49<02:57,  1.85s/pipeline]Optimization Progress:  97%|█████████▋| 2905/3000 [29:49<02:27,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2905/3000 [29:49<02:27,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2906/3000 [29:49<02:25,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2907/3000 [29:49<02:24,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2908/3000 [29:49<02:22,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2909/3000 [29:49<02:21,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2910/3000 [29:49<02:19,  1.55s/pipeline]Optimization Progress:  97%|█████████▋| 2912/3000 [29:52<01:48,  1.23s/pipeline]Optimization Progress: 100%|█████████▉| 2992/3000 [29:56<00:07,  1.14pipeline/s]
Generation 29 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3002pipeline [29:57,  1.14pipeline/s]Optimization Progress: 3002pipeline [29:57,  1.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3002pipeline [29:57,  1.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 3002pipeline [29:57,  1.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3002pipeline [29:58,  1.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3002pipeline [29:58,  1.55pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3003/3100 [29:58<01:02,  1.55pipeline/s]Optimization Progress:  97%|█████████▋| 3004/3100 [29:58<01:08,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3004/3100 [29:58<01:08,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3005/3100 [29:58<01:07,  1.40pipeline/s]Optimization Progress:  97%|█████████▋| 3007/3100 [30:02<01:18,  1.18pipeline/s]Optimization Progress: 100%|█████████▉| 3087/3100 [30:22<00:08,  1.50pipeline/s]
Generation 30 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-176572663.03311327	LinearSVR(LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 3102pipeline [30:22,  1.50pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3102pipeline [30:23,  1.50pipeline/s]Optimization Progress: 3102pipeline [30:23,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3102pipeline [30:23,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3102pipeline [30:23,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3102pipeline [30:23,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3102pipeline [30:23,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3102pipeline [30:24,  2.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3102pipeline [30:24,  2.04pipeline/s]Optimization Progress:  97%|█████████▋| 3104/3200 [30:24<00:53,  1.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3104/3200 [30:24<00:53,  1.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3105/3200 [30:24<00:53,  1.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3106/3200 [30:24<00:52,  1.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3107/3200 [30:24<00:52,  1.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3108/3200 [30:24<00:51,  1.79pipeline/s]Optimization Progress:  97%|█████████▋| 3110/3200 [30:32<01:09,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 3190/3200 [30:35<00:05,  1.80pipeline/s]
Generation 31 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-176572663.03311327	LinearSVR(LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3202pipeline [30:35,  1.80pipeline/s]Optimization Progress: 3202pipeline [30:35,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3202pipeline [30:36,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3202pipeline [30:36,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3202pipeline [30:36,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3202pipeline [30:37,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3202pipeline [30:37,  2.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3202pipeline [30:37,  2.53pipeline/s]Optimization Progress:  97%|█████████▋| 3205/3300 [30:37<00:44,  2.12pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3205/3300 [30:37<00:44,  2.12pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3206/3300 [30:37<00:44,  2.12pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3207/3300 [30:37<00:43,  2.12pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3208/3300 [30:37<00:43,  2.12pipeline/s]Optimization Progress:  97%|█████████▋| 3210/3300 [30:47<01:20,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 3290/3300 [30:49<00:06,  1.59pipeline/s]
Generation 32 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-176572663.03311327	LinearSVR(LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3302pipeline [30:49,  1.59pipeline/s]Optimization Progress: 3302pipeline [30:49,  2.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3302pipeline [30:49,  2.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3302pipeline [30:49,  2.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3302pipeline [30:50,  2.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:48:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3302pipeline [30:50,  2.23pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3303/3400 [30:50<00:43,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3304/3400 [30:50<00:43,  2.23pipeline/s]Optimization Progress:  97%|█████████▋| 3305/3400 [30:50<00:40,  2.34pipeline/s]Optimization Progress:  97%|█████████▋| 3305/3400 [31:10<00:40,  2.34pipeline/s]Optimization Progress:  97%|█████████▋| 3306/3400 [31:10<09:52,  6.30s/pipeline]Optimization Progress: 100%|█████████▉| 3386/3400 [31:13<01:01,  4.42s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-176995176.52829787	LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-176572663.03311327	LinearSVR(LinearSVR(RBFSampler(MinMaxScaler(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0)), RBFSampler__gamma=0.30000000000000004), LinearSVR__C=0.0001, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:49:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3402pipeline [31:13,  4.42s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:49:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3402pipeline [31:13,  4.42s/pipeline]Optimization Progress: 3402pipeline [31:13,  3.11s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:49:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3402pipeline [31:14,  3.11s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3402pipeline [31:14,  3.11s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 3402pipeline [31:15,  3.11s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 3402pipeline [31:15,  3.11s/pipeline]Optimization Progress:  97%|█████████▋| 3404/3500 [31:15<03:50,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3404/3500 [31:15<03:50,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3405/3500 [31:15<03:48,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3406/3500 [31:15<03:45,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3407/3500 [31:15<03:43,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3408/3500 [31:15<03:40,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3409/3500 [31:15<03:38,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3410/3500 [31:15<03:36,  2.40s/pipeline]Optimization Progress:  97%|█████████▋| 3412/3500 [31:22<02:50,  1.94s/pipeline]Optimization Progress: 100%|█████████▉| 3492/3500 [31:24<00:10,  1.37s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-170484602.5313365	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3502pipeline [31:24,  1.37s/pipeline]Optimization Progress: 3502pipeline [31:24,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3502pipeline [31:25,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3502pipeline [31:25,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:49:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3502pipeline [31:25,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:49:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3502pipeline [31:26,  1.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3502pipeline [31:26,  1.04pipeline/s]Optimization Progress:  97%|█████████▋| 3507/3600 [31:26<01:14,  1.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3507/3600 [31:26<01:14,  1.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3508/3600 [31:26<01:13,  1.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3509/3600 [31:26<01:12,  1.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3510/3600 [31:26<01:11,  1.26pipeline/s]Optimization Progress:  98%|█████████▊| 3512/3600 [31:33<01:26,  1.01pipeline/s]Optimization Progress: 100%|█████████▉| 3592/3600 [31:34<00:05,  1.44pipeline/s]
Generation 35 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-170484602.5313365	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3602pipeline [31:34,  1.44pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3602pipeline [31:35,  1.44pipeline/s]Optimization Progress: 3602pipeline [31:35,  1.98pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3602pipeline [31:35,  1.98pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3602pipeline [31:35,  1.98pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3605/3700 [31:37<00:48,  1.98pipeline/s]Optimization Progress:  97%|█████████▋| 3606/3700 [31:37<00:50,  1.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3606/3700 [31:37<00:50,  1.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3607/3700 [31:37<00:49,  1.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3608/3700 [31:37<00:49,  1.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3609/3700 [31:37<00:48,  1.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3610/3700 [31:37<00:47,  1.88pipeline/s]Optimization Progress:  98%|█████████▊| 3612/3700 [31:42<00:54,  1.62pipeline/s]Optimization Progress: 100%|█████████▉| 3692/3700 [31:45<00:03,  2.25pipeline/s]
Generation 36 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-178373687.3216359	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-170484602.5313365	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3702pipeline [31:46,  2.25pipeline/s]Optimization Progress: 3702pipeline [31:46,  3.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3702pipeline [31:46,  3.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3702pipeline [31:46,  3.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3702pipeline [31:46,  3.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3702pipeline [31:47,  3.04pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3702pipeline [31:47,  3.04pipeline/s]Optimization Progress:  97%|█████████▋| 3704/3800 [31:47<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3704/3800 [31:47<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3705/3800 [31:47<00:45,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3706/3800 [31:47<00:44,  2.10pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3707/3800 [31:47<00:44,  2.10pipeline/s]Optimization Progress:  98%|█████████▊| 3709/3800 [31:50<00:45,  1.98pipeline/s]Optimization Progress: 100%|█████████▉| 3789/3800 [31:53<00:04,  2.74pipeline/s]
Generation 37 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177364274.2956425	LinearSVR(Nystroem(MinMaxScaler(input_matrix), Nystroem__gamma=0.7000000000000001, Nystroem__kernel=rbf, Nystroem__n_components=2), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-170484602.5313365	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 3802pipeline [31:54,  2.74pipeline/s]Optimization Progress: 3802pipeline [31:54,  3.64pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3802pipeline [31:54,  3.64pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3802pipeline [31:55,  3.64pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3802pipeline [31:55,  3.64pipeline/s]Optimization Progress:  98%|█████████▊| 3805/3900 [31:56<00:34,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3805/3900 [31:56<00:34,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3806/3900 [31:56<00:33,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3807/3900 [31:56<00:33,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3808/3900 [31:56<00:33,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3809/3900 [31:56<00:32,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3810/3900 [31:56<00:32,  2.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3811/3900 [31:56<00:32,  2.78pipeline/s]Optimization Progress:  98%|█████████▊| 3813/3900 [32:01<00:37,  2.32pipeline/s]Optimization Progress: 100%|█████████▉| 3893/3900 [32:03<00:02,  3.21pipeline/s]
Generation 38 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-169338014.40136665	LinearSVR(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:50:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 3902pipeline [32:03,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3902pipeline [32:04,  3.21pipeline/s]Optimization Progress: 3902pipeline [32:04,  3.99pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3902pipeline [32:05,  3.99pipeline/s]Optimization Progress:  98%|█████████▊| 3904/4000 [32:06<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3904/4000 [32:06<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3905/4000 [32:06<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3906/4000 [32:06<00:35,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3907/4000 [32:06<00:34,  2.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3908/4000 [32:06<00:34,  2.67pipeline/s]Optimization Progress:  98%|█████████▊| 3910/4000 [32:09<00:40,  2.20pipeline/s]Optimization Progress: 100%|█████████▉| 3990/4000 [32:12<00:03,  3.04pipeline/s]
Generation 39 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-168924492.35327715	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:50:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4002pipeline [32:13,  3.04pipeline/s]Optimization Progress: 4002pipeline [32:13,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4002pipeline [32:14,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4002pipeline [32:14,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4002pipeline [32:14,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4002pipeline [32:15,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4002pipeline [32:15,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4002pipeline [32:15,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 4002pipeline [32:15,  4.00pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4003/4100 [32:15<00:24,  4.00pipeline/s]Optimization Progress:  98%|█████████▊| 4004/4100 [32:15<00:44,  2.16pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4004/4100 [32:15<00:44,  2.16pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4005/4100 [32:15<00:43,  2.16pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4006/4100 [32:15<00:43,  2.16pipeline/s]Optimization Progress:  98%|█████████▊| 4008/4100 [32:22<01:15,  1.21pipeline/s]Optimization Progress: 100%|█████████▉| 4088/4100 [32:26<00:07,  1.69pipeline/s]
Generation 40 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-168924492.35327715	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4102pipeline [32:26,  1.69pipeline/s]Optimization Progress: 4102pipeline [32:26,  2.36pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4102pipeline [32:26,  2.36pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4102pipeline [32:27,  2.36pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4102pipeline [32:27,  2.36pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4102pipeline [32:28,  2.36pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4102pipeline [32:29,  2.36pipeline/s]Optimization Progress:  98%|█████████▊| 4104/4200 [32:29<01:01,  1.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4104/4200 [32:29<01:01,  1.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4105/4200 [32:29<01:00,  1.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4106/4200 [32:29<00:59,  1.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4107/4200 [32:29<00:59,  1.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4108/4200 [32:29<00:58,  1.57pipeline/s]Optimization Progress:  98%|█████████▊| 4109/4200 [32:40<00:57,  1.57pipeline/s]Optimization Progress:  98%|█████████▊| 4110/4200 [32:46<01:57,  1.31s/pipeline]Optimization Progress: 100%|█████████▉| 4190/4200 [32:48<00:09,  1.08pipeline/s]
Generation 41 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-168924492.35327715	LinearSVR(OneHotEncoder(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4202pipeline [32:49,  1.08pipeline/s]Optimization Progress: 4202pipeline [32:49,  1.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 4202pipeline [32:49,  1.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4202pipeline [32:49,  1.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:50:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4202pipeline [32:49,  1.51pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4202/4300 [32:51<01:05,  1.51pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4203/4300 [32:51<01:04,  1.51pipeline/s]Optimization Progress:  98%|█████████▊| 4204/4300 [32:51<01:13,  1.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4204/4300 [32:51<01:13,  1.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4205/4300 [32:51<01:12,  1.30pipeline/s]Optimization Progress:  98%|█████████▊| 4207/4300 [32:56<01:37,  1.04s/pipeline]Optimization Progress: 100%|█████████▉| 4287/4300 [32:59<00:09,  1.35pipeline/s]
Generation 42 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 4302pipeline [32:59,  1.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 4302pipeline [32:59,  1.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 4302pipeline [32:59,  1.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4302pipeline [32:59,  1.35pipeline/s]Optimization Progress: 4302pipeline [32:59,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4302pipeline [33:00,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4302pipeline [33:00,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4302pipeline [33:00,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4302pipeline [33:00,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4302pipeline [33:01,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4302pipeline [33:01,  1.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4302pipeline [33:01,  1.92pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4305/4400 [33:01<00:49,  1.92pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4306/4400 [33:01<00:49,  1.92pipeline/s]Optimization Progress:  98%|█████████▊| 4307/4400 [33:01<00:46,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4307/4400 [33:01<00:46,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4308/4400 [33:01<00:45,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4309/4400 [33:01<00:45,  2.02pipeline/s]Optimization Progress:  98%|█████████▊| 4311/4400 [33:05<00:54,  1.65pipeline/s]Optimization Progress: 100%|█████████▉| 4391/4400 [33:08<00:03,  2.29pipeline/s]
Generation 43 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170501840.00688946	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4402pipeline [33:08,  2.29pipeline/s]Optimization Progress: 4402pipeline [33:08,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4402pipeline [33:08,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4402pipeline [33:08,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4402pipeline [33:08,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4402pipeline [33:09,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4402pipeline [33:09,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4402pipeline [33:09,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 4402pipeline [33:09,  3.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 4402pipeline [33:09,  3.21pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4405/4500 [33:10<00:29,  3.21pipeline/s]Optimization Progress:  98%|█████████▊| 4406/4500 [33:10<00:33,  2.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4406/4500 [33:10<00:33,  2.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4407/4500 [33:10<00:33,  2.79pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4408/4500 [33:10<00:32,  2.79pipeline/s]Optimization Progress:  98%|█████████▊| 4410/4500 [33:15<00:55,  1.62pipeline/s]Optimization Progress: 100%|█████████▉| 4490/4500 [33:18<00:04,  2.24pipeline/s]
Generation 44 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170177503.2303336	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 4502pipeline [33:18,  2.24pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4502pipeline [33:19,  2.24pipeline/s]Optimization Progress: 4502pipeline [33:19,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float64')..
Optimization Progress: 4502pipeline [33:19,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4502pipeline [33:19,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4502pipeline [33:19,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4502pipeline [33:20,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4502pipeline [33:20,  3.10pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4502pipeline [33:21,  3.10pipeline/s]Optimization Progress:  98%|█████████▊| 4507/4600 [33:21<00:31,  2.93pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4507/4600 [33:21<00:31,  2.93pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4508/4600 [33:21<00:31,  2.93pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4509/4600 [33:21<00:31,  2.93pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4510/4600 [33:21<00:30,  2.93pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4511/4600 [33:21<00:30,  2.93pipeline/s]Optimization Progress:  98%|█████████▊| 4513/4600 [33:26<00:41,  2.07pipeline/s]Optimization Progress: 100%|█████████▉| 4593/4600 [33:28<00:02,  2.88pipeline/s]
Generation 45 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170177503.2303336	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4602pipeline [33:28,  2.88pipeline/s]Optimization Progress: 4602pipeline [33:28,  4.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 4602pipeline [33:29,  4.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 4602pipeline [33:29,  4.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:51:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 4602pipeline [33:29,  4.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4602pipeline [33:29,  4.02pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4603/4700 [33:30<00:24,  4.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4604/4700 [33:30<00:23,  4.02pipeline/s]Optimization Progress:  98%|█████████▊| 4605/4700 [33:30<00:33,  2.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4605/4700 [33:30<00:33,  2.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4606/4700 [33:30<00:33,  2.80pipeline/s]Optimization Progress:  98%|█████████▊| 4608/4700 [33:33<00:50,  1.81pipeline/s]Optimization Progress: 100%|█████████▉| 4688/4700 [33:37<00:04,  2.50pipeline/s]
Generation 46 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170177503.2303336	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4702pipeline [33:38,  2.50pipeline/s]Optimization Progress: 4702pipeline [33:38,  3.27pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4702pipeline [33:38,  3.27pipeline/s]Optimization Progress:  98%|█████████▊| 4704/4800 [33:39<00:34,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4704/4800 [33:39<00:34,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4705/4800 [33:39<00:33,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4706/4800 [33:39<00:33,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4707/4800 [33:39<00:32,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4708/4800 [33:39<00:32,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4709/4800 [33:39<00:32,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4710/4800 [33:39<00:31,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4711/4800 [33:39<00:31,  2.82pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4712/4800 [33:39<00:31,  2.82pipeline/s]Optimization Progress:  98%|█████████▊| 4714/4800 [33:44<00:34,  2.53pipeline/s]Optimization Progress: 100%|█████████▉| 4794/4800 [33:46<00:01,  3.50pipeline/s]
Generation 47 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-170177503.2303336	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4802pipeline [33:46,  3.50pipeline/s]Optimization Progress: 4802pipeline [33:46,  4.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 4802pipeline [33:48,  4.55pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4802pipeline [33:48,  4.55pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4802/4900 [33:48<00:21,  4.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4803/4900 [33:48<00:21,  4.55pipeline/s]Optimization Progress:  98%|█████████▊| 4804/4900 [33:48<00:39,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4804/4900 [33:48<00:39,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4805/4900 [33:48<00:39,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4806/4900 [33:48<00:39,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4807/4900 [33:48<00:38,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4808/4900 [33:48<00:38,  2.41pipeline/s]Optimization Progress:  98%|█████████▊| 4810/4900 [33:51<00:40,  2.23pipeline/s]Optimization Progress: 100%|█████████▉| 4890/4900 [33:53<00:03,  3.13pipeline/s]
Generation 48 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169726820.13869914	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=6), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4902pipeline [33:54,  3.13pipeline/s]Optimization Progress: 4902pipeline [33:54,  3.90pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 4902pipeline [33:54,  3.90pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4902pipeline [33:54,  3.90pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 4902pipeline [33:55,  3.90pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 4902pipeline [33:56,  3.90pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4904/5000 [33:56<00:24,  3.90pipeline/s]Optimization Progress:  98%|█████████▊| 4905/5000 [33:56<00:34,  2.74pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4905/5000 [33:56<00:34,  2.74pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4906/5000 [33:56<00:34,  2.74pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4907/5000 [33:56<00:33,  2.74pipeline/s]Optimization Progress:  98%|█████████▊| 4909/5000 [33:59<00:44,  2.06pipeline/s]Optimization Progress: 100%|█████████▉| 4989/5000 [34:01<00:03,  2.87pipeline/s]
Generation 49 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169726820.13869914	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=6), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156033191.71500555	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5002pipeline [34:03,  2.87pipeline/s]Optimization Progress: 5002pipeline [34:03,  3.58pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5002pipeline [34:03,  3.58pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5002/5100 [34:04<00:27,  3.58pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5003/5100 [34:04<00:27,  3.58pipeline/s]Optimization Progress:  98%|█████████▊| 5004/5100 [34:04<00:36,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5004/5100 [34:04<00:36,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5005/5100 [34:04<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5006/5100 [34:04<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5007/5100 [34:04<00:35,  2.65pipeline/s]Optimization Progress:  98%|█████████▊| 5009/5100 [34:09<00:50,  1.81pipeline/s]Optimization Progress: 100%|█████████▉| 5089/5100 [34:12<00:04,  2.52pipeline/s]
Generation 50 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169726820.13869914	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=6), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-156006724.6800311	LinearSVR(PolynomialFeatures(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=4), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 5102pipeline [34:13,  2.52pipeline/s]Optimization Progress: 5102pipeline [34:13,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:52:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5102pipeline [34:13,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5102pipeline [34:14,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5102pipeline [34:14,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5102pipeline [34:14,  3.25pipeline/s]Optimization Progress:  98%|█████████▊| 5105/5200 [34:14<00:32,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5105/5200 [34:14<00:32,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5106/5200 [34:14<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5107/5200 [34:14<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5108/5200 [34:14<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5109/5200 [34:14<00:30,  2.96pipeline/s]Optimization Progress:  98%|█████████▊| 5111/5200 [34:20<00:49,  1.79pipeline/s]Optimization Progress: 100%|█████████▉| 5191/5200 [34:24<00:03,  2.48pipeline/s]
Generation 51 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-148893216.25293145	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.5), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5202pipeline [34:24,  2.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:52:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5202pipeline [34:24,  2.48pipeline/s]Optimization Progress: 5202pipeline [34:24,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5202pipeline [34:25,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5202pipeline [34:25,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:52:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5202pipeline [34:26,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5202pipeline [34:26,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5202pipeline [34:26,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5202pipeline [34:26,  3.49pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5202pipeline [34:27,  3.49pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5203/5300 [34:27<00:27,  3.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5204/5300 [34:27<00:27,  3.49pipeline/s]Optimization Progress:  98%|█████████▊| 5206/5300 [34:34<01:28,  1.07pipeline/s]Optimization Progress: 100%|█████████▉| 5286/5300 [34:39<00:09,  1.48pipeline/s]
Generation 52 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 5302pipeline [34:39,  1.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5302pipeline [34:39,  1.48pipeline/s]Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=2 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=3 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=4 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=5 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=6 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=7 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=8 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=9 array must not contain infs or NaNs.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 5302pipeline [34:39,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 5302pipeline [34:40,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5302pipeline [34:41,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 5302pipeline [34:41,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5302pipeline [34:42,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:52:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5302pipeline [34:42,  2.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 5302pipeline [34:42,  2.09pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5304/5400 [34:42<00:45,  2.09pipeline/s]Optimization Progress:  98%|█████████▊| 5305/5400 [34:42<00:59,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5305/5400 [34:42<00:59,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5306/5400 [34:42<00:58,  1.60pipeline/s]Optimization Progress:  98%|█████████▊| 5308/5400 [34:47<01:25,  1.07pipeline/s]Optimization Progress: 100%|█████████▉| 5388/5400 [34:48<00:07,  1.52pipeline/s]
Generation 53 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5402pipeline [34:48,  1.52pipeline/s]Optimization Progress: 5402pipeline [34:48,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5402pipeline [34:49,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5402pipeline [34:49,  2.16pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:52:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5402pipeline [34:50,  2.16pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5407/5500 [34:51<00:43,  2.16pipeline/s]Optimization Progress:  98%|█████████▊| 5408/5500 [34:51<00:44,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5408/5500 [34:51<00:44,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5409/5500 [34:51<00:43,  2.09pipeline/s]Optimization Progress:  98%|█████████▊| 5412/5500 [34:55<00:54,  1.60pipeline/s]Optimization Progress: 100%|█████████▉| 5491/5500 [34:58<00:04,  2.23pipeline/s]
Generation 54 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5502pipeline [34:59,  2.23pipeline/s]Optimization Progress: 5502pipeline [34:59,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5502pipeline [34:59,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5502pipeline [35:00,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5502pipeline [35:00,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 5502pipeline [35:00,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 5502pipeline [35:00,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5502pipeline [35:01,  3.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5502pipeline [35:01,  3.01pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5504/5600 [35:02<00:31,  3.01pipeline/s]Optimization Progress:  98%|█████████▊| 5505/5600 [35:02<00:48,  1.97pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5505/5600 [35:02<00:48,  1.97pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5506/5600 [35:02<00:47,  1.97pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5507/5600 [35:02<00:47,  1.97pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5508/5600 [35:02<00:46,  1.97pipeline/s]Optimization Progress:  98%|█████████▊| 5510/5600 [35:07<00:57,  1.56pipeline/s]Optimization Progress: 100%|█████████▉| 5590/5600 [35:09<00:04,  2.18pipeline/s]
Generation 55 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 5602pipeline [35:10,  2.18pipeline/s]Optimization Progress: 5602pipeline [35:10,  3.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5602pipeline [35:11,  3.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5602pipeline [35:11,  3.00pipeline/s]Optimization Progress:  98%|█████████▊| 5608/5700 [35:12<00:31,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5608/5700 [35:12<00:31,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5609/5700 [35:12<00:31,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5610/5700 [35:12<00:31,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5611/5700 [35:12<00:30,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5612/5700 [35:12<00:30,  2.88pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5613/5700 [35:12<00:30,  2.88pipeline/s]Optimization Progress:  99%|█████████▊| 5615/5700 [35:17<00:37,  2.27pipeline/s]Optimization Progress: 100%|█████████▉| 5695/5700 [35:20<00:01,  3.13pipeline/s]
Generation 56 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5702pipeline [35:20,  3.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:53:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5702pipeline [35:20,  3.13pipeline/s]Optimization Progress: 5702pipeline [35:20,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5702pipeline [35:22,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 5702pipeline [35:22,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5702pipeline [35:22,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:53:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5702pipeline [35:22,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 5702pipeline [35:23,  3.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5702pipeline [35:23,  3.96pipeline/s]Optimization Progress:  98%|█████████▊| 5708/5800 [35:23<00:28,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5708/5800 [35:23<00:28,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5709/5800 [35:23<00:28,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5710/5800 [35:23<00:28,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5711/5800 [35:23<00:27,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5712/5800 [35:23<00:27,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5713/5800 [35:23<00:27,  3.20pipeline/s]Optimization Progress:  99%|█████████▊| 5715/5800 [35:28<00:34,  2.44pipeline/s]Optimization Progress: 100%|█████████▉| 5795/5800 [35:31<00:01,  3.35pipeline/s]
Generation 57 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127677106.8370121	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118852139.92579016	LinearSVR(PolynomialFeatures(MaxAbsScaler(Nystroem(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:53:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5802pipeline [35:32,  3.35pipeline/s]Optimization Progress: 5802pipeline [35:32,  3.88pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 5802pipeline [35:32,  3.88pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5802pipeline [35:32,  3.88pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5802pipeline [35:32,  3.88pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 5802pipeline [35:32,  3.88pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5802pipeline [35:33,  3.88pipeline/s]Optimization Progress:  98%|█████████▊| 5805/5900 [35:34<00:34,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5805/5900 [35:34<00:34,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5806/5900 [35:34<00:34,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5807/5900 [35:34<00:34,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5808/5900 [35:34<00:33,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5809/5900 [35:34<00:33,  2.72pipeline/s]Optimization Progress:  98%|█████████▊| 5811/5900 [35:39<00:45,  1.95pipeline/s]Optimization Progress: 100%|█████████▉| 5891/5900 [35:42<00:03,  2.70pipeline/s]
Generation 58 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 5902pipeline [35:42,  2.70pipeline/s]Optimization Progress: 5902pipeline [35:42,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 5902pipeline [35:43,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5902pipeline [35:43,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 5902pipeline [35:43,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 5902pipeline [35:43,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:53:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5902pipeline [35:43,  3.70pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:53:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 5902pipeline [35:44,  3.70pipeline/s]Optimization Progress:  98%|█████████▊| 5906/6000 [35:45<00:33,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5906/6000 [35:45<00:33,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5907/6000 [35:45<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5908/6000 [35:45<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5909/6000 [35:45<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5910/6000 [35:45<00:31,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 5911/6000 [35:45<00:31,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 5912/6000 [35:45<00:31,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 5913/6000 [35:45<00:30,  2.83pipeline/s]Optimization Progress:  99%|█████████▊| 5915/6000 [35:50<00:36,  2.33pipeline/s]Optimization Progress: 100%|█████████▉| 5995/6000 [35:53<00:01,  3.22pipeline/s]
Generation 59 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 6002pipeline [35:54,  3.22pipeline/s]Optimization Progress: 6002pipeline [35:54,  3.87pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 6002pipeline [35:54,  3.87pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 6002pipeline [35:54,  3.87pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6002pipeline [35:55,  3.87pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 6002pipeline [35:55,  3.87pipeline/s]Optimization Progress:  99%|█████████▊| 6011/6100 [35:56<00:23,  3.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6011/6100 [35:56<00:23,  3.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6012/6100 [35:56<00:23,  3.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6013/6100 [35:56<00:23,  3.76pipeline/s]Optimization Progress:  99%|█████████▊| 6015/6100 [36:01<00:43,  1.96pipeline/s]Optimization Progress: 100%|█████████▉| 6095/6100 [36:04<00:01,  2.70pipeline/s]
Generation 60 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6102pipeline [36:04,  2.70pipeline/s]Optimization Progress: 6102pipeline [36:04,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 6102pipeline [36:04,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6102pipeline [36:04,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6102pipeline [36:04,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6102pipeline [36:04,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6102pipeline [36:05,  3.61pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 6102pipeline [36:06,  3.61pipeline/s]Optimization Progress:  98%|█████████▊| 6106/6200 [36:07<00:39,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6106/6200 [36:07<00:39,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6107/6200 [36:07<00:38,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6108/6200 [36:07<00:38,  2.40pipeline/s]Optimization Progress:  99%|█████████▊| 6110/6200 [36:14<01:10,  1.27pipeline/s]Optimization Progress: 100%|█████████▉| 6190/6200 [36:17<00:05,  1.78pipeline/s]
Generation 61 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6202pipeline [36:17,  1.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 6202pipeline [36:18,  1.78pipeline/s]Optimization Progress: 6202pipeline [36:18,  2.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6202pipeline [36:19,  2.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6202pipeline [36:19,  2.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6202pipeline [36:19,  2.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6202pipeline [36:20,  2.41pipeline/s]Optimization Progress:  99%|█████████▊| 6210/6300 [36:20<00:33,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6210/6300 [36:20<00:33,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6211/6300 [36:20<00:32,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6212/6300 [36:20<00:32,  2.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6213/6300 [36:20<00:32,  2.72pipeline/s]Optimization Progress:  99%|█████████▊| 6215/6300 [36:23<00:40,  2.08pipeline/s]Optimization Progress: 100%|█████████▉| 6295/6300 [36:25<00:01,  2.93pipeline/s]
Generation 62 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6302pipeline [36:25,  2.93pipeline/s]Optimization Progress: 6302pipeline [36:25,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6302pipeline [36:25,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6302pipeline [36:26,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6302pipeline [36:26,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6302pipeline [36:27,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6302pipeline [36:28,  4.01pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6302pipeline [36:28,  4.01pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6304/6400 [36:29<00:23,  4.01pipeline/s]Optimization Progress:  99%|█████████▊| 6305/6400 [36:29<00:52,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6305/6400 [36:29<00:52,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6306/6400 [36:29<00:51,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6307/6400 [36:29<00:51,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6308/6400 [36:29<00:50,  1.81pipeline/s]Optimization Progress:  99%|█████████▊| 6310/6400 [36:34<01:01,  1.46pipeline/s]Optimization Progress: 100%|█████████▉| 6390/6400 [36:35<00:04,  2.06pipeline/s]
Generation 63 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6402pipeline [36:36,  2.06pipeline/s]Optimization Progress: 6402pipeline [36:36,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6402pipeline [36:38,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6402pipeline [36:39,  2.85pipeline/s]Optimization Progress:  99%|█████████▊| 6405/6500 [36:39<00:57,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6405/6500 [36:39<00:57,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6406/6500 [36:39<00:56,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6407/6500 [36:39<00:56,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6408/6500 [36:40<00:55,  1.65pipeline/s]Optimization Progress:  99%|█████████▊| 6410/6500 [36:44<01:00,  1.49pipeline/s]Optimization Progress: 100%|█████████▉| 6490/6500 [36:46<00:04,  2.09pipeline/s]
Generation 64 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6502pipeline [36:46,  2.09pipeline/s]Optimization Progress: 6502pipeline [36:46,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 6502pipeline [36:46,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:54:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6502pipeline [36:46,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6502pipeline [36:47,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 6502pipeline [36:47,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6502pipeline [36:48,  2.96pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 6502pipeline [36:49,  2.96pipeline/s]Optimization Progress:  99%|█████████▊| 6507/6600 [36:49<00:38,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6507/6600 [36:49<00:38,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6508/6600 [36:49<00:38,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6509/6600 [36:49<00:37,  2.41pipeline/s]Optimization Progress:  99%|█████████▊| 6511/6600 [36:53<00:50,  1.75pipeline/s]Optimization Progress: 100%|█████████▉| 6591/6600 [36:56<00:03,  2.43pipeline/s]
Generation 65 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6602pipeline [36:56,  2.43pipeline/s]Optimization Progress: 6602pipeline [36:56,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 6602pipeline [36:56,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6602pipeline [36:57,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6602pipeline [36:58,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6602pipeline [36:58,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6602pipeline [36:58,  3.34pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6602pipeline [36:59,  3.34pipeline/s]Optimization Progress:  99%|█████████▊| 6610/6700 [36:59<00:27,  3.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6610/6700 [36:59<00:27,  3.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6611/6700 [36:59<00:27,  3.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6612/6700 [36:59<00:27,  3.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6613/6700 [36:59<00:26,  3.26pipeline/s]Optimization Progress:  99%|█████████▊| 6615/6700 [37:02<00:35,  2.36pipeline/s]Optimization Progress: 100%|█████████▉| 6695/6700 [37:04<00:01,  3.30pipeline/s]
Generation 66 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6702pipeline [37:05,  3.30pipeline/s]Optimization Progress: 6702pipeline [37:05,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6702pipeline [37:05,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6702pipeline [37:05,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6702pipeline [37:05,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6702pipeline [37:06,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6702pipeline [37:06,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6702pipeline [37:06,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6702pipeline [37:06,  4.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6702pipeline [37:06,  4.06pipeline/s]Optimization Progress:  99%|█████████▊| 6711/6800 [37:07<00:21,  4.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6711/6800 [37:07<00:21,  4.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6712/6800 [37:07<00:21,  4.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6713/6800 [37:07<00:20,  4.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6714/6800 [37:07<00:20,  4.19pipeline/s]Optimization Progress:  99%|█████████▉| 6716/6800 [37:11<00:33,  2.50pipeline/s]Optimization Progress: 100%|█████████▉| 6796/6800 [37:13<00:01,  3.47pipeline/s]
Generation 67 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6802pipeline [37:14,  3.47pipeline/s]Optimization Progress: 6802pipeline [37:14,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6802pipeline [37:14,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6802pipeline [37:15,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6802pipeline [37:15,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6802pipeline [37:16,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6802pipeline [37:16,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6802pipeline [37:16,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6802pipeline [37:17,  4.00pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:55:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6802pipeline [37:17,  4.00pipeline/s]Optimization Progress:  99%|█████████▊| 6810/6900 [37:17<00:25,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6810/6900 [37:17<00:25,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6811/6900 [37:17<00:25,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6812/6900 [37:17<00:25,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6813/6900 [37:17<00:25,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 6814/6900 [37:17<00:24,  3.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 6815/6900 [37:17<00:24,  3.46pipeline/s]Optimization Progress:  99%|█████████▉| 6817/6900 [37:22<00:36,  2.29pipeline/s]Optimization Progress: 100%|█████████▉| 6897/6900 [37:24<00:00,  3.20pipeline/s]
Generation 68 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-118727517.90684621	LinearSVR(SGDRegressor(PolynomialFeatures(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 6902pipeline [37:25,  3.20pipeline/s]Optimization Progress: 6902pipeline [37:25,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6902pipeline [37:25,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 6902pipeline [37:26,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6902pipeline [37:26,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6902pipeline [37:26,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6902pipeline [37:26,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6902pipeline [37:27,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6902pipeline [37:27,  4.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6902pipeline [37:28,  4.07pipeline/s]Optimization Progress:  99%|█████████▊| 6909/7000 [37:28<00:29,  3.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6909/7000 [37:28<00:29,  3.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6910/7000 [37:28<00:29,  3.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6911/7000 [37:28<00:28,  3.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6912/7000 [37:28<00:28,  3.08pipeline/s]Optimization Progress:  99%|█████████▉| 6914/7000 [37:34<00:48,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 6994/7000 [37:37<00:02,  2.45pipeline/s]
Generation 69 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-127583454.57283206	LinearSVR(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-112924518.41141245	LinearSVR(SelectPercentile(SGDRegressor(Nystroem(RobustScaler(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8500000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=10, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=5), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.1), SelectPercentile__percentile=48), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7002pipeline [37:37,  2.45pipeline/s]Optimization Progress: 7002pipeline [37:37,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7002pipeline [37:37,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7002pipeline [37:38,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7002pipeline [37:38,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7002pipeline [37:39,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7002pipeline [37:39,  3.26pipeline/s]Optimization Progress:  99%|█████████▊| 7006/7100 [37:41<00:47,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7006/7100 [37:41<00:47,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7007/7100 [37:41<00:46,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7008/7100 [37:41<00:46,  1.99pipeline/s]Optimization Progress:  99%|█████████▊| 7010/7100 [37:45<00:55,  1.61pipeline/s]Optimization Progress: 100%|█████████▉| 7090/7100 [37:48<00:04,  2.24pipeline/s]
Generation 70 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-109018005.40599069	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7102pipeline [37:49,  2.24pipeline/s]Optimization Progress: 7102pipeline [37:49,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7102pipeline [37:49,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 7102pipeline [37:49,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 7102pipeline [37:49,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:55:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7102pipeline [37:50,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7102pipeline [37:50,  3.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 7102pipeline [37:50,  3.02pipeline/s]Optimization Progress:  99%|█████████▊| 7106/7200 [37:51<00:39,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7106/7200 [37:51<00:39,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7107/7200 [37:51<00:39,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7108/7200 [37:51<00:38,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7109/7200 [37:51<00:38,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7110/7200 [37:51<00:38,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7111/7200 [37:51<00:37,  2.36pipeline/s]Optimization Progress:  99%|█████████▉| 7113/7200 [37:56<00:41,  2.07pipeline/s]Optimization Progress: 100%|█████████▉| 7193/7200 [37:58<00:02,  2.89pipeline/s]
Generation 71 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-109018005.40599069	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7202pipeline [37:58,  2.89pipeline/s]Optimization Progress: 7202pipeline [37:58,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7202pipeline [37:59,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:56:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7202pipeline [37:59,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 7202pipeline [38:00,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 7202pipeline [38:00,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7202pipeline [38:01,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7202pipeline [38:01,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7202pipeline [38:01,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7202pipeline [38:02,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 7202pipeline [38:02,  3.92pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7202pipeline [38:02,  3.92pipeline/s]Optimization Progress:  99%|█████████▊| 7205/7300 [38:02<00:51,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7205/7300 [38:02<00:51,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7206/7300 [38:02<00:51,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7207/7300 [38:02<00:50,  1.84pipeline/s]Optimization Progress:  99%|█████████▉| 7209/7300 [38:06<01:01,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 7289/7300 [38:08<00:05,  2.08pipeline/s]
Generation 72 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-137371143.05341777	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-109018005.40599069	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7302pipeline [38:08,  2.08pipeline/s]Optimization Progress: 7302pipeline [38:08,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 7302pipeline [38:08,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7302pipeline [38:09,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 7302pipeline [38:10,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 7302pipeline [38:11,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 7302pipeline [38:11,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7302pipeline [38:11,  2.95pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7305/7400 [38:12<00:32,  2.95pipeline/s]Optimization Progress:  99%|█████████▊| 7306/7400 [38:12<00:49,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7306/7400 [38:12<00:49,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7307/7400 [38:12<00:48,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7308/7400 [38:12<00:48,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7309/7400 [38:12<00:47,  1.91pipeline/s]Optimization Progress:  99%|█████████▉| 7311/7400 [38:18<01:07,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 7391/7400 [38:21<00:04,  1.85pipeline/s]
Generation 73 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-109018005.40599069	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 7402pipeline [38:22,  1.85pipeline/s]Optimization Progress: 7402pipeline [38:22,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7402pipeline [38:22,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7402pipeline [38:22,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:56:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7402pipeline [38:22,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7402pipeline [38:23,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7402pipeline [38:23,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7402pipeline [38:24,  2.56pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7402pipeline [38:24,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 7408/7500 [38:24<00:37,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7408/7500 [38:24<00:37,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7409/7500 [38:24<00:36,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7410/7500 [38:24<00:36,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7411/7500 [38:24<00:35,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7412/7500 [38:24<00:35,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7413/7500 [38:24<00:35,  2.48pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7414/7500 [38:24<00:34,  2.48pipeline/s]Optimization Progress:  99%|█████████▉| 7416/7500 [38:28<00:35,  2.34pipeline/s]Optimization Progress: 100%|█████████▉| 7496/7500 [38:32<00:01,  3.20pipeline/s]
Generation 74 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7502pipeline [38:33,  3.20pipeline/s]Optimization Progress: 7502pipeline [38:33,  3.62pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7502pipeline [38:33,  3.62pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7502pipeline [38:33,  3.62pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7502pipeline [38:35,  3.62pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:56:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7502pipeline [38:35,  3.62pipeline/s]Optimization Progress:  99%|█████████▉| 7508/7600 [38:36<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7508/7600 [38:36<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7509/7600 [38:36<00:30,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7510/7600 [38:36<00:30,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7511/7600 [38:36<00:30,  2.96pipeline/s]Optimization Progress:  99%|█████████▉| 7513/7600 [38:41<00:47,  1.83pipeline/s]Optimization Progress: 100%|█████████▉| 7593/7600 [38:42<00:02,  2.58pipeline/s]
Generation 75 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169611864.64141154	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 7602pipeline [38:42,  2.58pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7602pipeline [38:42,  2.58pipeline/s]Optimization Progress: 7602pipeline [38:42,  3.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7602pipeline [38:43,  3.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7602pipeline [38:44,  3.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7602pipeline [38:44,  3.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7602pipeline [38:46,  3.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:56:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7602pipeline [38:46,  3.46pipeline/s]Optimization Progress:  99%|█████████▉| 7608/7700 [38:46<00:35,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7608/7700 [38:46<00:35,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7609/7700 [38:46<00:35,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7610/7700 [38:46<00:35,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7611/7700 [38:46<00:34,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7612/7700 [38:46<00:34,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7613/7700 [38:46<00:33,  2.57pipeline/s]Optimization Progress:  99%|█████████▉| 7615/7700 [38:56<00:58,  1.44pipeline/s]Optimization Progress: 100%|█████████▉| 7695/7700 [38:59<00:02,  2.01pipeline/s]
Generation 76 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169446151.02495238	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 7702pipeline [38:59,  2.01pipeline/s]Optimization Progress: 7702pipeline [38:59,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7702pipeline [38:59,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7702pipeline [39:00,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7702pipeline [39:00,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7702pipeline [39:00,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7702pipeline [39:01,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7702pipeline [39:02,  2.80pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7702pipeline [39:02,  2.80pipeline/s]Optimization Progress:  99%|█████████▉| 7710/7800 [39:02<00:33,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7710/7800 [39:02<00:33,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7711/7800 [39:02<00:32,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7712/7800 [39:02<00:32,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7713/7800 [39:02<00:32,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7714/7800 [39:02<00:31,  2.71pipeline/s]Optimization Progress:  99%|█████████▉| 7716/7800 [39:07<00:39,  2.12pipeline/s]Optimization Progress: 100%|█████████▉| 7796/7800 [39:10<00:01,  2.91pipeline/s]
Generation 77 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169446151.02495238	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7802pipeline [39:11,  2.91pipeline/s]Optimization Progress: 7802pipeline [39:11,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7802pipeline [39:11,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7802pipeline [39:12,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7802pipeline [39:12,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7802pipeline [39:13,  3.57pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7802pipeline [39:13,  3.57pipeline/s]Optimization Progress:  99%|█████████▉| 7809/7900 [39:13<00:26,  3.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7809/7900 [39:13<00:26,  3.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7810/7900 [39:13<00:25,  3.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7811/7900 [39:13<00:25,  3.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7812/7900 [39:13<00:25,  3.47pipeline/s]Optimization Progress:  99%|█████████▉| 7814/7900 [39:17<00:38,  2.21pipeline/s]Optimization Progress: 100%|█████████▉| 7894/7900 [39:19<00:01,  3.11pipeline/s]
Generation 78 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169446151.02495238	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7902pipeline [39:19,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 7902pipeline [39:19,  3.11pipeline/s]Optimization Progress: 7902pipeline [39:19,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7902pipeline [39:20,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7902pipeline [39:20,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:57:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7902pipeline [39:20,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 7902pipeline [39:20,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 7902pipeline [39:21,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7902pipeline [39:21,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 7902pipeline [39:21,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 7902pipeline [39:21,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 7902pipeline [39:22,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7902pipeline [39:22,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7902pipeline [39:22,  3.93pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 7902pipeline [39:22,  3.93pipeline/s]Optimization Progress:  99%|█████████▉| 7909/8000 [39:22<00:27,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7909/8000 [39:22<00:27,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7910/8000 [39:22<00:27,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7911/8000 [39:22<00:27,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7912/8000 [39:22<00:26,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7913/8000 [39:22<00:26,  3.28pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7914/8000 [39:22<00:26,  3.28pipeline/s]Optimization Progress:  99%|█████████▉| 7916/8000 [39:26<00:31,  2.67pipeline/s]Optimization Progress: 100%|█████████▉| 7996/8000 [39:27<00:01,  3.73pipeline/s]
Generation 79 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-169446151.02495238	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8002pipeline [39:28,  3.73pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8002pipeline [39:28,  3.73pipeline/s]Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8002pipeline [39:28,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8002pipeline [39:29,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:57:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8002pipeline [39:29,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8002pipeline [39:30,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 8002pipeline [39:31,  4.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8002pipeline [39:31,  4.86pipeline/s]Optimization Progress:  99%|█████████▉| 8008/8100 [39:32<00:31,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8008/8100 [39:32<00:31,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8009/8100 [39:32<00:31,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8010/8100 [39:32<00:31,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8011/8100 [39:32<00:30,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8012/8100 [39:32<00:30,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8013/8100 [39:32<00:30,  2.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8014/8100 [39:32<00:29,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 8016/8100 [39:36<00:33,  2.53pipeline/s]Optimization Progress: 100%|█████████▉| 8096/8100 [39:37<00:01,  3.55pipeline/s]
Generation 80 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8102pipeline [39:38,  3.55pipeline/s]Optimization Progress: 8102pipeline [39:38,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8102pipeline [39:39,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8102pipeline [39:39,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 8102pipeline [39:39,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8102pipeline [39:39,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8102pipeline [39:39,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8102pipeline [39:40,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8102pipeline [39:40,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8102pipeline [39:40,  4.48pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8102pipeline [39:40,  4.48pipeline/s]Optimization Progress:  99%|█████████▉| 8110/8200 [39:40<00:22,  4.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8110/8200 [39:40<00:22,  4.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8111/8200 [39:40<00:21,  4.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8112/8200 [39:40<00:21,  4.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8113/8200 [39:40<00:21,  4.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8114/8200 [39:40<00:21,  4.07pipeline/s]Optimization Progress:  99%|█████████▉| 8116/8200 [39:44<00:29,  2.83pipeline/s]Optimization Progress: 100%|█████████▉| 8196/8200 [39:47<00:01,  3.86pipeline/s]
Generation 81 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8202pipeline [39:48,  3.86pipeline/s]Optimization Progress: 8202pipeline [39:48,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8202pipeline [39:48,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:57:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:57:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 8202pipeline [39:49,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8202pipeline [39:50,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 8202pipeline [39:50,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8202pipeline [39:50,  4.35pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8202pipeline [39:51,  4.35pipeline/s]Optimization Progress:  99%|█████████▉| 8206/8300 [39:51<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8206/8300 [39:51<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8207/8300 [39:51<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8208/8300 [39:51<00:34,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8209/8300 [39:51<00:34,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8210/8300 [39:51<00:33,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8211/8300 [39:51<00:33,  2.65pipeline/s]Optimization Progress:  99%|█████████▉| 8213/8300 [39:58<00:48,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 8293/8300 [39:59<00:02,  2.50pipeline/s]
Generation 82 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8302pipeline [40:00,  2.50pipeline/s]Optimization Progress: 8302pipeline [40:00,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8302pipeline [40:00,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 8302pipeline [40:01,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8302pipeline [40:02,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8302pipeline [40:02,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 8302pipeline [40:02,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8302pipeline [40:02,  3.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:58:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8302pipeline [40:02,  3.25pipeline/s]Optimization Progress:  99%|█████████▉| 8308/8400 [40:04<00:35,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8308/8400 [40:04<00:35,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8309/8400 [40:04<00:34,  2.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8310/8400 [40:04<00:34,  2.60pipeline/s]Optimization Progress:  99%|█████████▉| 8312/8400 [40:08<00:55,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 8392/8400 [40:10<00:03,  2.21pipeline/s]
Generation 83 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-136447260.70810765	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8402pipeline [40:11,  2.21pipeline/s]Optimization Progress: 8402pipeline [40:11,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 8402pipeline [40:13,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8402pipeline [40:14,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8402pipeline [40:14,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8402pipeline [40:14,  2.95pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8402pipeline [40:14,  2.95pipeline/s]Optimization Progress:  99%|█████████▉| 8408/8500 [40:15<00:38,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8408/8500 [40:15<00:38,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8409/8500 [40:15<00:37,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8410/8500 [40:15<00:37,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8411/8500 [40:15<00:36,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8412/8500 [40:15<00:36,  2.42pipeline/s]Optimization Progress:  99%|█████████▉| 8414/8500 [40:20<00:46,  1.85pipeline/s]Optimization Progress: 100%|█████████▉| 8494/8500 [40:24<00:02,  2.54pipeline/s]
Generation 84 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-135918737.08879408	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8502pipeline [40:24,  2.54pipeline/s]Optimization Progress: 8502pipeline [40:24,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 8502pipeline [40:24,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8502pipeline [40:25,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8502pipeline [40:25,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8502pipeline [40:25,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8502pipeline [40:25,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:58:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8502pipeline [40:26,  3.51pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8502pipeline [40:27,  3.51pipeline/s]Optimization Progress:  99%|█████████▉| 8508/8600 [40:28<00:35,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8508/8600 [40:28<00:35,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8509/8600 [40:28<00:34,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8510/8600 [40:28<00:34,  2.62pipeline/s]Optimization Progress:  99%|█████████▉| 8512/8600 [40:32<00:52,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 8592/8600 [40:33<00:03,  2.38pipeline/s]
Generation 85 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-135918737.08879408	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 8602pipeline [40:34,  2.38pipeline/s]Optimization Progress: 8602pipeline [40:34,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8602pipeline [40:35,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8602pipeline [40:35,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8602pipeline [40:35,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8602pipeline [40:35,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8602pipeline [40:35,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8602pipeline [40:36,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8602pipeline [40:37,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8602pipeline [40:37,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8602pipeline [40:38,  3.09pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8602pipeline [40:38,  3.09pipeline/s]Optimization Progress:  99%|█████████▉| 8609/8700 [40:38<00:34,  2.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8609/8700 [40:38<00:34,  2.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8610/8700 [40:38<00:34,  2.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8611/8700 [40:38<00:34,  2.61pipeline/s]Optimization Progress:  99%|█████████▉| 8613/8700 [40:44<01:00,  1.43pipeline/s]Optimization Progress: 100%|█████████▉| 8693/8700 [40:49<00:03,  1.97pipeline/s]
Generation 86 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-135918737.08879408	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8702pipeline [40:49,  1.97pipeline/s]Optimization Progress: 8702pipeline [40:49,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8702pipeline [40:49,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8702pipeline [40:49,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 8702pipeline [40:50,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8702pipeline [40:50,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8702pipeline [40:50,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8702pipeline [40:50,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8702pipeline [40:51,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:58:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8702pipeline [40:52,  2.78pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8702pipeline [40:53,  2.78pipeline/s]Optimization Progress:  99%|█████████▉| 8706/8800 [40:53<00:52,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8706/8800 [40:53<00:52,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8707/8800 [40:53<00:51,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8708/8800 [40:53<00:51,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8709/8800 [40:53<00:50,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8710/8800 [40:53<00:50,  1.80pipeline/s]Optimization Progress:  99%|█████████▉| 8712/8800 [40:58<00:55,  1.59pipeline/s]Optimization Progress: 100%|█████████▉| 8792/8800 [41:00<00:03,  2.21pipeline/s]
Generation 87 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131947254.76901786	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8802pipeline [41:01,  2.21pipeline/s]Optimization Progress: 8802pipeline [41:01,  3.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8802pipeline [41:01,  3.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8802pipeline [41:02,  3.13pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8802pipeline [41:02,  3.13pipeline/s]Optimization Progress:  99%|█████████▉| 8812/8900 [41:04<00:27,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8812/8900 [41:04<00:27,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8813/8900 [41:04<00:27,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8814/8900 [41:04<00:26,  3.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8815/8900 [41:04<00:26,  3.20pipeline/s]Optimization Progress:  99%|█████████▉| 8817/8900 [41:08<00:39,  2.10pipeline/s]Optimization Progress: 100%|█████████▉| 8897/8900 [41:10<00:01,  2.94pipeline/s]
Generation 88 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 8902pipeline [41:10,  2.94pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8902pipeline [41:10,  2.94pipeline/s]Optimization Progress: 8902pipeline [41:10,  3.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 8902pipeline [41:11,  3.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8902pipeline [41:11,  3.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:59:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 8902pipeline [41:11,  3.72pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8902pipeline [41:12,  3.72pipeline/s]Optimization Progress:  99%|█████████▉| 8908/9000 [41:14<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8908/9000 [41:14<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8909/9000 [41:14<00:32,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8910/9000 [41:14<00:31,  2.83pipeline/s]Optimization Progress:  99%|█████████▉| 8912/9000 [41:18<00:49,  1.77pipeline/s]Optimization Progress: 100%|█████████▉| 8992/9000 [41:20<00:03,  2.47pipeline/s]
Generation 89 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9002pipeline [41:20,  2.47pipeline/s]Optimization Progress: 9002pipeline [41:20,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:59:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9002pipeline [41:20,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9002pipeline [41:21,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9002pipeline [41:22,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9002pipeline [41:22,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9002pipeline [41:23,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 9002pipeline [41:23,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9002pipeline [41:23,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9002pipeline [41:24,  3.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9002pipeline [41:24,  3.43pipeline/s]Optimization Progress:  99%|█████████▉| 9009/9100 [41:25<00:34,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9009/9100 [41:25<00:34,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9010/9100 [41:25<00:34,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9011/9100 [41:25<00:33,  2.64pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 9012/9100 [41:25<00:33,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9013/9100 [41:25<00:32,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9014/9100 [41:25<00:32,  2.64pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9015/9100 [41:25<00:32,  2.64pipeline/s]Optimization Progress:  99%|█████████▉| 9017/9100 [41:35<00:53,  1.55pipeline/s]Optimization Progress: 100%|█████████▉| 9097/9100 [41:37<00:01,  2.17pipeline/s]
Generation 90 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-104410471.49595295	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(ZeroCount(input_matrix)), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9102pipeline [41:37,  2.17pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9102pipeline [41:37,  2.17pipeline/s]Optimization Progress: 9102pipeline [41:37,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9102pipeline [41:38,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9102pipeline [41:39,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9102pipeline [41:39,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9102pipeline [41:39,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9102pipeline [41:39,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9102pipeline [41:39,  2.85pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9102pipeline [41:40,  2.85pipeline/s]Optimization Progress:  99%|█████████▉| 9112/9200 [41:40<00:28,  3.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9112/9200 [41:40<00:28,  3.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9113/9200 [41:40<00:28,  3.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9114/9200 [41:40<00:28,  3.05pipeline/s]Optimization Progress:  99%|█████████▉| 9116/9200 [41:45<00:47,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 9196/9200 [41:46<00:01,  2.51pipeline/s]
Generation 91 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9202pipeline [41:47,  2.51pipeline/s]Optimization Progress: 9202pipeline [41:47,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9202pipeline [41:47,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [16:59:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [16:59:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9202pipeline [41:48,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 9202pipeline [41:49,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9202pipeline [41:50,  3.26pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9202pipeline [41:50,  3.26pipeline/s]Optimization Progress:  99%|█████████▉| 9210/9300 [41:51<00:33,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9210/9300 [41:51<00:33,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9211/9300 [41:51<00:33,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9212/9300 [41:51<00:32,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9213/9300 [41:51<00:32,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9214/9300 [41:51<00:32,  2.68pipeline/s]Optimization Progress:  99%|█████████▉| 9216/9300 [41:55<00:39,  2.15pipeline/s]Optimization Progress: 100%|█████████▉| 9296/9300 [42:00<00:01,  2.91pipeline/s]
Generation 92 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102497577.87581691	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(ExtraTreesRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9302pipeline [42:00,  2.91pipeline/s]Optimization Progress: 9302pipeline [42:00,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9302pipeline [42:01,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9302pipeline [42:01,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9302pipeline [42:02,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9302pipeline [42:03,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9302pipeline [42:04,  3.81pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9303/9400 [42:05<00:25,  3.81pipeline/s]Optimization Progress:  99%|█████████▉| 9304/9400 [42:05<01:21,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9304/9400 [42:05<01:21,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9305/9400 [42:05<01:20,  1.17pipeline/s]Optimization Progress:  99%|█████████▉| 9307/9400 [42:10<01:46,  1.14s/pipeline]Optimization Progress: 100%|█████████▉| 9387/9400 [42:14<00:10,  1.23pipeline/s]
Generation 93 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102497577.87581691	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(ExtraTreesRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9402pipeline [42:14,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9402pipeline [42:14,  1.23pipeline/s]Optimization Progress: 9402pipeline [42:14,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9402pipeline [42:15,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9402pipeline [42:15,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=3 [17:00:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9402pipeline [42:16,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9402pipeline [42:17,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9402pipeline [42:18,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9402pipeline [42:18,  1.74pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 9402pipeline [42:18,  1.74pipeline/s]Optimization Progress:  99%|█████████▉| 9406/9500 [42:19<01:09,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9406/9500 [42:19<01:09,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9407/9500 [42:19<01:08,  1.36pipeline/s]Optimization Progress:  99%|█████████▉| 9409/9500 [42:25<01:41,  1.12s/pipeline]Optimization Progress: 100%|█████████▉| 9489/9500 [42:26<00:08,  1.27pipeline/s]
Generation 94 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102497577.87581691	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(ExtraTreesRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=20, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9502pipeline [42:27,  1.27pipeline/s]Optimization Progress: 9502pipeline [42:27,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9502pipeline [42:28,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9502pipeline [42:28,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 9502pipeline [42:28,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9502pipeline [42:28,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9502pipeline [42:29,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9502pipeline [42:29,  1.76pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9502pipeline [42:30,  1.76pipeline/s]Optimization Progress:  99%|█████████▉| 9505/9600 [42:30<01:10,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9505/9600 [42:30<01:10,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9506/9600 [42:30<01:09,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9507/9600 [42:30<01:08,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9508/9600 [42:30<01:07,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9509/9600 [42:30<01:07,  1.35pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9510/9600 [42:30<01:06,  1.35pipeline/s]Optimization Progress:  99%|█████████▉| 9512/9600 [42:35<01:02,  1.40pipeline/s]Optimization Progress: 100%|█████████▉| 9592/9600 [42:36<00:04,  1.98pipeline/s]
Generation 95 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-100121446.76975401	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(ExtraTreesRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9602pipeline [42:36,  1.98pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9602pipeline [42:36,  1.98pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9602pipeline [42:37,  1.98pipeline/s]Optimization Progress: 9602pipeline [42:37,  2.58pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9602pipeline [42:37,  2.58pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9602pipeline [42:37,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 9606/9700 [42:40<00:42,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9606/9700 [42:40<00:42,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9607/9700 [42:40<00:41,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9608/9700 [42:40<00:41,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9609/9700 [42:40<00:40,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9610/9700 [42:40<00:40,  2.22pipeline/s]Optimization Progress:  99%|█████████▉| 9612/9700 [42:45<00:51,  1.71pipeline/s]Optimization Progress: 100%|█████████▉| 9692/9700 [42:48<00:03,  2.39pipeline/s]
Generation 96 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-105234988.01300469	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-102514931.24108042	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-100121446.76975401	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(ElasticNetCV(ExtraTreesRegressor(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=7, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.1), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9702pipeline [42:49,  2.39pipeline/s]Optimization Progress: 9702pipeline [42:49,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9702pipeline [42:50,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9702pipeline [42:50,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9702pipeline [42:50,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9702pipeline [42:50,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9702pipeline [42:50,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9702pipeline [42:51,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:00:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9702pipeline [42:51,  3.11pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9702pipeline [42:52,  3.11pipeline/s]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9703/9800 [42:53<00:31,  3.11pipeline/s]Optimization Progress:  99%|█████████▉| 9704/9800 [42:53<01:16,  1.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9704/9800 [42:53<01:16,  1.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9705/9800 [42:53<01:15,  1.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9706/9800 [42:53<01:15,  1.25pipeline/s]Optimization Progress:  99%|█████████▉| 9708/9800 [42:57<01:23,  1.10pipeline/s]Optimization Progress: 100%|█████████▉| 9788/9800 [43:01<00:07,  1.53pipeline/s]
Generation 97 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:01,  1.53pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:02,  1.53pipeline/s]Optimization Progress: 9802pipeline [43:02,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:02,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 9802pipeline [43:03,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9802pipeline [43:03,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9802pipeline [43:03,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:03,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:04,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9802pipeline [43:04,  2.12pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9802pipeline [43:04,  2.12pipeline/s]Optimization Progress:  99%|█████████▉| 9813/9900 [43:05<00:35,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9813/9900 [43:05<00:35,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9814/9900 [43:05<00:35,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9815/9900 [43:05<00:35,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9816/9900 [43:05<00:34,  2.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9817/9900 [43:05<00:34,  2.42pipeline/s]Optimization Progress:  99%|█████████▉| 9819/9900 [43:11<00:47,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 9899/9900 [43:14<00:00,  2.37pipeline/s]
Generation 98 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107344869.80937597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9902pipeline [43:15,  2.37pipeline/s]Optimization Progress: 9902pipeline [43:15,  2.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9902pipeline [43:16,  2.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9902pipeline [43:17,  2.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9902pipeline [43:17,  2.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [17:01:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 9902pipeline [43:17,  2.15pipeline/s]Optimization Progress:  99%|█████████▉| 9909/10000 [43:18<00:38,  2.34pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9909/10000 [43:18<00:38,  2.34pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9910/10000 [43:18<00:38,  2.34pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9911/10000 [43:18<00:37,  2.34pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9912/10000 [43:18<00:37,  2.34pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9913/10000 [43:18<00:37,  2.34pipeline/s]                                                                                 Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 9914/10000 [43:18<00:36,  2.34pipeline/s]Optimization Progress:  99%|█████████▉| 9916/10000 [43:22<00:39,  2.10pipeline/s]Optimization Progress: 100%|█████████▉| 9996/10000 [43:25<00:01,  2.89pipeline/s]
Generation 99 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10002pipeline [43:26,  2.89pipeline/s]Optimization Progress: 10002pipeline [43:26,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 10002pipeline [43:26,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10002pipeline [43:27,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10002pipeline [43:27,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 10002pipeline [43:27,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10002pipeline [43:28,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10002pipeline [43:28,  3.54pipeline/s]Optimization Progress:  99%|█████████▉| 10014/10100 [43:29<00:22,  3.79pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10014/10100 [43:29<00:22,  3.79pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10015/10100 [43:29<00:22,  3.79pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10016/10100 [43:29<00:22,  3.79pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10017/10100 [43:29<00:21,  3.79pipeline/s]Optimization Progress:  99%|█████████▉| 10019/10100 [43:32<00:33,  2.40pipeline/s]Optimization Progress: 100%|█████████▉| 10099/10100 [43:33<00:00,  3.40pipeline/s]
Generation 100 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10102pipeline [43:35,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 10102pipeline [43:35,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 10102pipeline [43:35,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10102pipeline [43:35,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10102pipeline [43:36,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10102pipeline [43:37,  3.40pipeline/s]Optimization Progress:  99%|█████████▉| 10106/10200 [43:37<00:35,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10106/10200 [43:37<00:35,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10107/10200 [43:37<00:34,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10108/10200 [43:37<00:34,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10109/10200 [43:37<00:34,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10110/10200 [43:37<00:33,  2.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10111/10200 [43:37<00:33,  2.67pipeline/s]Optimization Progress:  99%|█████████▉| 10113/10200 [43:41<00:37,  2.30pipeline/s]Optimization Progress: 100%|█████████▉| 10193/10200 [43:44<00:02,  3.18pipeline/s]
Generation 101 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10202pipeline [43:44,  3.18pipeline/s]Optimization Progress: 10202pipeline [43:44,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10202pipeline [43:45,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10202pipeline [43:45,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10202pipeline [43:46,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10202pipeline [43:46,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10202pipeline [43:46,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10202pipeline [43:46,  4.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10202pipeline [43:47,  4.17pipeline/s]Optimization Progress:  99%|█████████▉| 10209/10300 [43:47<00:25,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10209/10300 [43:47<00:25,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10210/10300 [43:47<00:25,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10211/10300 [43:47<00:25,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10212/10300 [43:47<00:24,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10213/10300 [43:47<00:24,  3.53pipeline/s]Optimization Progress:  99%|█████████▉| 10215/10300 [43:51<00:35,  2.42pipeline/s]Optimization Progress: 100%|█████████▉| 10295/10300 [43:52<00:01,  3.43pipeline/s]
Generation 102 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10302pipeline [43:52,  3.43pipeline/s]Optimization Progress: 10302pipeline [43:52,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10302pipeline [43:53,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10302pipeline [43:53,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 10302pipeline [43:54,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 10302pipeline [43:54,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:01:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10302pipeline [43:55,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10302pipeline [43:56,  4.61pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10302pipeline [43:56,  4.61pipeline/s]Optimization Progress:  99%|█████████▉| 10308/10400 [43:56<00:32,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10308/10400 [43:56<00:32,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10309/10400 [43:56<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10310/10400 [43:56<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10311/10400 [43:56<00:31,  2.86pipeline/s]Optimization Progress:  99%|█████████▉| 10313/10400 [44:00<00:42,  2.07pipeline/s]Optimization Progress: 100%|█████████▉| 10393/10400 [44:03<00:02,  2.87pipeline/s]
Generation 103 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 10402pipeline [44:03,  2.87pipeline/s]Optimization Progress: 10402pipeline [44:03,  3.85pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10402pipeline [44:05,  3.85pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 10402pipeline [44:06,  3.85pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10402pipeline [44:06,  3.85pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10402pipeline [44:06,  3.85pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10402pipeline [44:06,  3.85pipeline/s]Optimization Progress:  99%|█████████▉| 10408/10500 [44:07<00:31,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10408/10500 [44:07<00:31,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10409/10500 [44:07<00:31,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10410/10500 [44:07<00:31,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10411/10500 [44:07<00:30,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 10413/10500 [44:12<00:46,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 10493/10500 [44:15<00:02,  2.57pipeline/s]
Generation 104 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10502pipeline [44:15,  2.57pipeline/s]Optimization Progress: 10502pipeline [44:15,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 10502pipeline [44:16,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10502pipeline [44:16,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10502pipeline [44:17,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:02:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10502pipeline [44:17,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10502pipeline [44:17,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10502pipeline [44:18,  3.59pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 10502pipeline [44:18,  3.59pipeline/s]Optimization Progress:  99%|█████████▉| 10510/10600 [44:18<00:28,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10510/10600 [44:18<00:28,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10511/10600 [44:18<00:28,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10512/10600 [44:19<00:28,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10513/10600 [44:19<00:27,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10514/10600 [44:19<00:27,  3.13pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10515/10600 [44:19<00:27,  3.13pipeline/s]Optimization Progress:  99%|█████████▉| 10517/10600 [44:23<00:35,  2.37pipeline/s]Optimization Progress: 100%|█████████▉| 10597/10600 [44:27<00:00,  3.24pipeline/s]
Generation 105 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10602pipeline [44:28,  3.24pipeline/s]Optimization Progress: 10602pipeline [44:28,  3.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 10602pipeline [44:29,  3.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10602pipeline [44:30,  3.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10602pipeline [44:30,  3.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10602pipeline [44:31,  3.17pipeline/s]Optimization Progress:  99%|█████████▉| 10609/10700 [44:31<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10609/10700 [44:31<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10610/10700 [44:31<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10611/10700 [44:31<00:31,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10612/10700 [44:31<00:30,  2.86pipeline/s]Optimization Progress:  99%|█████████▉| 10614/10700 [44:37<00:48,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 10694/10700 [44:38<00:02,  2.47pipeline/s]
Generation 106 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10702pipeline [44:40,  2.47pipeline/s]Optimization Progress: 10702pipeline [44:40,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10702pipeline [44:40,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10702pipeline [44:40,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10702pipeline [44:41,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10702pipeline [44:41,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10702pipeline [44:41,  3.01pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10702pipeline [44:42,  3.01pipeline/s]Optimization Progress:  99%|█████████▉| 10711/10800 [44:42<00:26,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10711/10800 [44:42<00:26,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10712/10800 [44:42<00:26,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10713/10800 [44:42<00:26,  3.32pipeline/s]Optimization Progress:  99%|█████████▉| 10715/10800 [44:47<00:52,  1.61pipeline/s]Optimization Progress: 100%|█████████▉| 10795/10800 [44:49<00:02,  2.27pipeline/s]
Generation 107 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131816601.31261186	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99204116.77048096	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 10802pipeline [44:49,  2.27pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 10802pipeline [44:49,  2.27pipeline/s]Optimization Progress: 10802pipeline [44:49,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10802pipeline [44:50,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 10802pipeline [44:51,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10802pipeline [44:51,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:02:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 10802pipeline [44:51,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10802pipeline [44:51,  3.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10802pipeline [44:52,  3.06pipeline/s]Optimization Progress:  99%|█████████▉| 10806/10900 [44:53<00:50,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10806/10900 [44:53<00:50,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10807/10900 [44:53<00:50,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10808/10900 [44:53<00:49,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10809/10900 [44:53<00:49,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10810/10900 [44:53<00:48,  1.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10811/10900 [44:53<00:48,  1.85pipeline/s]Optimization Progress:  99%|█████████▉| 10813/10900 [44:58<00:50,  1.71pipeline/s]Optimization Progress: 100%|█████████▉| 10893/10900 [45:01<00:02,  2.39pipeline/s]
Generation 108 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-107216850.60812575	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10902pipeline [45:01,  2.39pipeline/s]Optimization Progress: 10902pipeline [45:01,  3.30pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 10902pipeline [45:02,  3.30pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 10902pipeline [45:02,  3.30pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10902pipeline [45:03,  3.30pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10902pipeline [45:03,  3.30pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10902pipeline [45:04,  3.30pipeline/s]Optimization Progress:  99%|█████████▉| 10909/11000 [45:05<00:33,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10909/11000 [45:05<00:33,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10910/11000 [45:05<00:33,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10911/11000 [45:05<00:32,  2.72pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 10912/11000 [45:05<00:32,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10913/11000 [45:05<00:32,  2.72pipeline/s]Optimization Progress:  99%|█████████▉| 10915/11000 [45:10<00:42,  1.98pipeline/s]Optimization Progress: 100%|█████████▉| 10995/11000 [45:12<00:01,  2.76pipeline/s]
Generation 109 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11002pipeline [45:12,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11002pipeline [45:13,  2.76pipeline/s]Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 11002pipeline [45:13,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 11002pipeline [45:14,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11002pipeline [45:15,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11002pipeline [45:15,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 11002pipeline [45:15,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11002pipeline [45:16,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11002pipeline [45:16,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 11002pipeline [45:16,  3.73pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11002pipeline [45:17,  3.73pipeline/s]Optimization Progress:  99%|█████████▉| 11009/11100 [45:17<00:33,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11009/11100 [45:17<00:33,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11010/11100 [45:17<00:33,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11011/11100 [45:17<00:32,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11012/11100 [45:17<00:32,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11013/11100 [45:17<00:32,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11014/11100 [45:17<00:31,  2.71pipeline/s]Optimization Progress:  99%|█████████▉| 11016/11100 [45:25<00:49,  1.68pipeline/s]Optimization Progress: 100%|█████████▉| 11096/11100 [45:28<00:01,  2.33pipeline/s]
Generation 110 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11102pipeline [45:28,  2.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11102pipeline [45:28,  2.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 11102pipeline [45:28,  2.33pipeline/s]Optimization Progress: 11102pipeline [45:28,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11102pipeline [45:29,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11102pipeline [45:29,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11102pipeline [45:29,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 11102pipeline [45:29,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11102pipeline [45:30,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11102pipeline [45:31,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11102pipeline [45:31,  3.03pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11102pipeline [45:31,  3.03pipeline/s]Optimization Progress:  99%|█████████▉| 11107/11200 [45:32<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11107/11200 [45:32<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11108/11200 [45:32<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11109/11200 [45:32<00:38,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11110/11200 [45:32<00:38,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11111/11200 [45:32<00:38,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11112/11200 [45:32<00:37,  2.34pipeline/s]Optimization Progress:  99%|█████████▉| 11114/11200 [45:38<00:50,  1.72pipeline/s]Optimization Progress: 100%|█████████▉| 11194/11200 [45:42<00:02,  2.37pipeline/s]
Generation 111 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11202pipeline [45:42,  2.37pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 11202pipeline [45:43,  2.37pipeline/s]Optimization Progress: 11202pipeline [45:43,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11202pipeline [45:43,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11202pipeline [45:43,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 11202pipeline [45:43,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11202pipeline [45:43,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:03:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11202pipeline [45:44,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 11202pipeline [45:44,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11202pipeline [45:44,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11202pipeline [45:44,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11202pipeline [45:44,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11202pipeline [45:45,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11202pipeline [45:45,  3.31pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11202pipeline [45:45,  3.31pipeline/s]Optimization Progress:  99%|█████████▉| 11210/11300 [45:45<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11210/11300 [45:45<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11211/11300 [45:45<00:27,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11212/11300 [45:45<00:27,  3.18pipeline/s]Optimization Progress:  99%|█████████▉| 11214/11300 [45:49<00:40,  2.12pipeline/s]Optimization Progress: 100%|█████████▉| 11294/11300 [45:50<00:02,  3.00pipeline/s]
Generation 112 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11302pipeline [45:50,  3.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11302pipeline [45:50,  3.00pipeline/s]Optimization Progress: 11302pipeline [45:50,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 11302pipeline [45:50,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 11302pipeline [45:51,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 11302pipeline [45:52,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11302pipeline [45:52,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11302pipeline [45:52,  4.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11302pipeline [45:53,  4.08pipeline/s]Optimization Progress:  99%|█████████▉| 11310/11400 [45:54<00:28,  3.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11310/11400 [45:54<00:28,  3.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11311/11400 [45:54<00:28,  3.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11312/11400 [45:54<00:28,  3.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11313/11400 [45:54<00:27,  3.14pipeline/s]Optimization Progress:  99%|█████████▉| 11315/11400 [46:00<00:49,  1.71pipeline/s]Optimization Progress: 100%|█████████▉| 11395/11400 [46:05<00:02,  2.34pipeline/s]
Generation 113 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11402pipeline [46:05,  2.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:04:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11402pipeline [46:06,  2.34pipeline/s]Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:04:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 11402pipeline [46:06,  3.02pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11402pipeline [46:08,  3.02pipeline/s]Optimization Progress:  99%|█████████▉| 11405/11500 [46:08<00:47,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11405/11500 [46:08<00:47,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11406/11500 [46:08<00:47,  2.00pipeline/s]Optimization Progress:  99%|█████████▉| 11408/11500 [46:12<01:04,  1.42pipeline/s]Optimization Progress: 100%|█████████▉| 11488/11500 [46:13<00:05,  2.01pipeline/s]
Generation 114 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-99053287.61461073	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11502pipeline [46:14,  2.01pipeline/s]Optimization Progress: 11502pipeline [46:14,  2.72pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11502pipeline [46:14,  2.72pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 11502pipeline [46:15,  2.72pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:04:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11502pipeline [46:16,  2.72pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 11502pipeline [46:16,  2.72pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 11502pipeline [46:17,  2.72pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11503/11600 [46:18<00:35,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11504/11600 [46:18<00:35,  2.72pipeline/s]Optimization Progress:  99%|█████████▉| 11505/11600 [46:18<01:02,  1.52pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11505/11600 [46:18<01:02,  1.52pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11506/11600 [46:18<01:01,  1.52pipeline/s]Optimization Progress:  99%|█████████▉| 11508/11600 [46:24<01:33,  1.02s/pipeline]Optimization Progress: 100%|█████████▉| 11588/11600 [46:27<00:08,  1.38pipeline/s]
Generation 115 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:04:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11602pipeline [46:27,  1.38pipeline/s]Optimization Progress: 11602pipeline [46:27,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:27,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:04:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11602pipeline [46:27,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:27,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:04:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11602pipeline [46:28,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11602pipeline [46:28,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:28,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:28,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:29,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11602pipeline [46:29,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11602pipeline [46:29,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:04:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11602pipeline [46:30,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 11602pipeline [46:30,  1.95pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 11602pipeline [46:30,  1.95pipeline/s]Optimization Progress:  99%|█████████▉| 11606/11700 [46:31<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11606/11700 [46:31<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11607/11700 [46:31<00:58,  1.60pipeline/s]Optimization Progress:  99%|█████████▉| 11609/11700 [46:36<01:28,  1.03pipeline/s]Optimization Progress: 100%|█████████▉| 11689/11700 [46:43<00:07,  1.41pipeline/s]
Generation 116 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-131472964.68473649	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-104627469.53356597	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11702pipeline [46:43,  1.41pipeline/s]Optimization Progress: 11702pipeline [46:43,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:43,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 11702pipeline [46:44,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:44,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11702pipeline [46:44,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 11702pipeline [46:44,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11702pipeline [46:45,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:45,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11702pipeline [46:46,  2.00pipeline/s]Optimization Progress:  99%|█████████▉| 11711/11800 [46:47<00:40,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11711/11800 [46:47<00:40,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11712/11800 [46:47<00:39,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11713/11800 [46:47<00:39,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11714/11800 [46:47<00:38,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11715/11800 [46:47<00:38,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11716/11800 [46:47<00:38,  2.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11717/11800 [46:47<00:37,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 11719/11800 [46:54<00:47,  1.72pipeline/s]Optimization Progress: 100%|█████████▉| 11799/11800 [46:54<00:00,  2.44pipeline/s]
Generation 117 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11802pipeline [46:55,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11802pipeline [46:56,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11802pipeline [46:56,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11802pipeline [46:56,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11802pipeline [46:58,  2.44pipeline/s]Optimization Progress:  99%|█████████▉| 11805/11900 [46:58<00:47,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11805/11900 [46:58<00:47,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11806/11900 [46:58<00:47,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11807/11900 [46:58<00:46,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11808/11900 [46:58<00:46,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11809/11900 [46:58<00:45,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11810/11900 [46:58<00:45,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11811/11900 [46:58<00:44,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11812/11900 [46:58<00:44,  1.99pipeline/s]Optimization Progress:  99%|█████████▉| 11814/11900 [47:04<00:47,  1.82pipeline/s]Optimization Progress: 100%|█████████▉| 11894/11900 [47:08<00:02,  2.52pipeline/s]
Generation 118 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11902pipeline [47:09,  2.52pipeline/s]Optimization Progress: 11902pipeline [47:09,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11902pipeline [47:09,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 11902pipeline [47:09,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11902pipeline [47:09,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 11902pipeline [47:10,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11902pipeline [47:10,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11902pipeline [47:11,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 11902pipeline [47:11,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11902pipeline [47:11,  3.19pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 11902pipeline [47:11,  3.19pipeline/s]Optimization Progress:  99%|█████████▉| 11907/12000 [47:11<00:36,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11907/12000 [47:11<00:36,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11908/12000 [47:11<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11909/12000 [47:11<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11910/12000 [47:12<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11911/12000 [47:12<00:34,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11912/12000 [47:12<00:34,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 11914/12000 [47:15<00:36,  2.38pipeline/s]Optimization Progress: 100%|█████████▉| 11994/12000 [47:18<00:01,  3.26pipeline/s]
Generation 119 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 12002pipeline [47:19,  3.26pipeline/s]Optimization Progress: 12002pipeline [47:19,  4.14pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12002pipeline [47:19,  4.14pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12002pipeline [47:22,  4.14pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12002pipeline [47:23,  4.14pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 12002pipeline [47:23,  4.14pipeline/s]Optimization Progress:  99%|█████████▉| 12008/12100 [47:23<00:32,  2.80pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12008/12100 [47:23<00:32,  2.80pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12009/12100 [47:23<00:32,  2.80pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12010/12100 [47:23<00:32,  2.80pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12011/12100 [47:23<00:31,  2.80pipeline/s]Optimization Progress:  99%|█████████▉| 12013/12100 [47:29<00:54,  1.60pipeline/s]Optimization Progress: 100%|█████████▉| 12093/12100 [47:32<00:03,  2.24pipeline/s]
Generation 120 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12102pipeline [47:33,  2.24pipeline/s]Optimization Progress: 12102pipeline [47:33,  2.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12102pipeline [47:34,  2.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12102pipeline [47:34,  2.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 12102pipeline [47:35,  2.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12102pipeline [47:36,  2.88pipeline/s]Optimization Progress:  99%|█████████▉| 12108/12200 [47:36<00:36,  2.50pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12108/12200 [47:36<00:36,  2.50pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12109/12200 [47:36<00:36,  2.50pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12110/12200 [47:36<00:35,  2.50pipeline/s]Optimization Progress:  99%|█████████▉| 12112/12200 [47:41<00:55,  1.58pipeline/s]Optimization Progress: 100%|█████████▉| 12192/12200 [47:43<00:03,  2.22pipeline/s]
Generation 121 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 12202pipeline [47:43,  2.22pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 12202pipeline [47:43,  2.22pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12202pipeline [47:43,  2.22pipeline/s]Optimization Progress: 12202pipeline [47:43,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12202pipeline [47:43,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12202pipeline [47:45,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12202pipeline [47:45,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12202pipeline [47:45,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12202pipeline [47:46,  3.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12202pipeline [47:46,  3.10pipeline/s]Optimization Progress:  99%|█████████▉| 12206/12300 [47:47<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12206/12300 [47:47<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12207/12300 [47:47<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12208/12300 [47:47<00:45,  2.00pipeline/s]Optimization Progress:  99%|█████████▉| 12210/12300 [47:51<00:58,  1.54pipeline/s]Optimization Progress: 100%|█████████▉| 12290/12300 [47:53<00:04,  2.16pipeline/s]
Generation 122 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 12302pipeline [47:54,  2.16pipeline/s]Optimization Progress: 12302pipeline [47:54,  2.90pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 12302pipeline [47:54,  2.90pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:05:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12302pipeline [47:55,  2.90pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12302pipeline [47:55,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 12306/12400 [47:57<00:43,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12306/12400 [47:57<00:43,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12307/12400 [47:57<00:42,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12308/12400 [47:57<00:42,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12309/12400 [47:57<00:41,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12310/12400 [47:57<00:41,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12311/12400 [47:57<00:40,  2.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12312/12400 [47:57<00:40,  2.17pipeline/s]Optimization Progress:  99%|█████████▉| 12313/12400 [48:10<00:40,  2.17pipeline/s]Optimization Progress:  99%|█████████▉| 12314/12400 [48:14<01:22,  1.04pipeline/s]Optimization Progress: 100%|█████████▉| 12394/12400 [48:17<00:04,  1.46pipeline/s]
Generation 123 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12402pipeline [48:18,  1.46pipeline/s]Optimization Progress: 12402pipeline [48:18,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:06:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12402pipeline [48:18,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:06:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12402pipeline [48:19,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12402pipeline [48:20,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 12402pipeline [48:20,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 12402pipeline [48:20,  1.96pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12402pipeline [48:20,  1.96pipeline/s]Optimization Progress:  99%|█████████▉| 12406/12500 [48:21<00:54,  1.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12406/12500 [48:21<00:54,  1.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12407/12500 [48:21<00:54,  1.71pipeline/s]Optimization Progress:  99%|█████████▉| 12409/12500 [48:26<01:19,  1.14pipeline/s]Optimization Progress: 100%|█████████▉| 12489/12500 [48:27<00:06,  1.62pipeline/s]
Generation 124 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12502pipeline [48:28,  1.62pipeline/s]Optimization Progress: 12502pipeline [48:28,  2.18pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12502pipeline [48:28,  2.18pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12502pipeline [48:28,  2.18pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12502pipeline [48:29,  2.18pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12502pipeline [48:30,  2.18pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12502pipeline [48:30,  2.18pipeline/s]Optimization Progress:  99%|█████████▉| 12507/12600 [48:31<00:45,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12507/12600 [48:31<00:45,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12508/12600 [48:31<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12509/12600 [48:31<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12510/12600 [48:31<00:43,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12511/12600 [48:31<00:43,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12512/12600 [48:31<00:42,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12513/12600 [48:31<00:42,  2.06pipeline/s]Optimization Progress:  99%|█████████▉| 12515/12600 [48:36<00:45,  1.88pipeline/s]Optimization Progress: 100%|█████████▉| 12595/12600 [48:37<00:01,  2.65pipeline/s]
Generation 125 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12602pipeline [48:37,  2.65pipeline/s]Optimization Progress: 12602pipeline [48:37,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12602pipeline [48:38,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12602pipeline [48:38,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12602pipeline [48:38,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 12602pipeline [48:39,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12602pipeline [48:39,  3.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 12602pipeline [48:40,  3.63pipeline/s]Optimization Progress:  99%|█████████▉| 12608/12700 [48:41<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12608/12700 [48:41<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12609/12700 [48:41<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12610/12700 [48:41<00:32,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12611/12700 [48:41<00:32,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12612/12700 [48:41<00:32,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12613/12700 [48:41<00:31,  2.74pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 12614/12700 [48:41<00:31,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12615/12700 [48:41<00:31,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12616/12700 [48:41<00:30,  2.74pipeline/s]Optimization Progress:  99%|█████████▉| 12618/12700 [48:46<00:33,  2.48pipeline/s]Optimization Progress: 100%|█████████▉| 12698/12700 [48:49<00:00,  3.40pipeline/s]
Generation 126 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12702pipeline [48:50,  3.40pipeline/s]Optimization Progress: 12702pipeline [48:50,  3.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12702pipeline [48:51,  3.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 12702pipeline [48:52,  3.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:06:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12702pipeline [48:52,  3.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:06:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12702pipeline [48:53,  3.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12702pipeline [48:53,  3.58pipeline/s]Optimization Progress:  99%|█████████▉| 12708/12800 [48:53<00:31,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12708/12800 [48:53<00:31,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12709/12800 [48:53<00:31,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12710/12800 [48:53<00:31,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12711/12800 [48:53<00:30,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12712/12800 [48:53<00:30,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12713/12800 [48:53<00:30,  2.89pipeline/s]Optimization Progress:  99%|█████████▉| 12715/12800 [48:57<00:36,  2.32pipeline/s]Optimization Progress: 100%|█████████▉| 12795/12800 [48:59<00:01,  3.24pipeline/s]
Generation 127 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12802pipeline [48:59,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12802pipeline [48:59,  3.24pipeline/s]Optimization Progress: 12802pipeline [48:59,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12802pipeline [49:00,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 12802pipeline [49:00,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 12802pipeline [49:01,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12802pipeline [49:01,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12802pipeline [49:02,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12802pipeline [49:02,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12802pipeline [49:02,  4.43pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 12802pipeline [49:03,  4.43pipeline/s]Optimization Progress:  99%|█████████▉| 12806/12900 [49:03<00:42,  2.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12806/12900 [49:03<00:42,  2.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12807/12900 [49:03<00:42,  2.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12808/12900 [49:03<00:41,  2.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12809/12900 [49:03<00:41,  2.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12810/12900 [49:03<00:40,  2.20pipeline/s]Optimization Progress:  99%|█████████▉| 12812/12900 [49:08<00:47,  1.85pipeline/s]Optimization Progress: 100%|█████████▉| 12892/12900 [49:08<00:03,  2.62pipeline/s]
Generation 128 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12902pipeline [49:08,  2.62pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 12902pipeline [49:09,  2.62pipeline/s]Optimization Progress: 12902pipeline [49:09,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12902pipeline [49:09,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12902pipeline [49:09,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12902pipeline [49:09,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12902pipeline [49:10,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12902pipeline [49:10,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 12902pipeline [49:10,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12902pipeline [49:11,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 12902pipeline [49:11,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 12902pipeline [49:11,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 12902pipeline [49:12,  3.64pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 12902pipeline [49:12,  3.64pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12905/13000 [49:12<00:26,  3.64pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12906/13000 [49:12<00:25,  3.64pipeline/s]Optimization Progress:  99%|█████████▉| 12907/13000 [49:12<00:36,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12907/13000 [49:12<00:36,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12908/13000 [49:12<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12909/13000 [49:12<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12910/13000 [49:12<00:35,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 12912/13000 [49:17<00:52,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 12992/13000 [49:21<00:03,  2.31pipeline/s]
Generation 129 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13002pipeline [49:21,  2.31pipeline/s]Optimization Progress: 13002pipeline [49:21,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 13002pipeline [49:22,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13002pipeline [49:22,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 13002pipeline [49:24,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13002pipeline [49:24,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 13002pipeline [49:24,  3.25pipeline/s]Optimization Progress:  99%|█████████▉| 13011/13100 [49:25<00:30,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13011/13100 [49:25<00:30,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13012/13100 [49:25<00:30,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13013/13100 [49:25<00:30,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13014/13100 [49:25<00:29,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13015/13100 [49:25<00:29,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13016/13100 [49:25<00:29,  2.88pipeline/s]Optimization Progress:  99%|█████████▉| 13018/13100 [49:29<00:34,  2.40pipeline/s]Optimization Progress: 100%|█████████▉| 13098/13100 [49:31<00:00,  3.33pipeline/s]
Generation 130 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 13102pipeline [49:31,  3.33pipeline/s]Optimization Progress: 13102pipeline [49:31,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13102pipeline [49:32,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13102pipeline [49:32,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 13102pipeline [49:33,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13102pipeline [49:33,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13102pipeline [49:34,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13102pipeline [49:34,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 13102pipeline [49:34,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13102pipeline [49:35,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13102pipeline [49:35,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 13102pipeline [49:35,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:07:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13102pipeline [49:35,  4.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13102pipeline [49:35,  4.60pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13104/13200 [49:35<00:20,  4.60pipeline/s]Optimization Progress:  99%|█████████▉| 13105/13200 [49:35<00:50,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13105/13200 [49:35<00:50,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13106/13200 [49:35<00:50,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13107/13200 [49:35<00:49,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13108/13200 [49:35<00:49,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13109/13200 [49:35<00:48,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13110/13200 [49:35<00:48,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13111/13200 [49:35<00:47,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13112/13200 [49:35<00:47,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13113/13200 [49:35<00:46,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13114/13200 [49:35<00:46,  1.87pipeline/s]Optimization Progress:  99%|█████████▉| 13116/13200 [49:40<00:42,  1.95pipeline/s]Optimization Progress: 100%|█████████▉| 13196/13200 [49:43<00:01,  2.70pipeline/s]
Generation 131 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13202pipeline [49:43,  2.70pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 13202pipeline [49:44,  2.70pipeline/s]Optimization Progress: 13202pipeline [49:44,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 13202pipeline [49:44,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13202pipeline [49:45,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13202pipeline [49:45,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 13202pipeline [49:45,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13202pipeline [49:46,  3.40pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 13202pipeline [49:46,  3.40pipeline/s]Optimization Progress:  99%|█████████▉| 13215/13300 [49:47<00:23,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13215/13300 [49:47<00:23,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13216/13300 [49:47<00:23,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13217/13300 [49:47<00:23,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13218/13300 [49:47<00:23,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13219/13300 [49:47<00:22,  3.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13220/13300 [49:47<00:22,  3.55pipeline/s]Optimization Progress:  99%|█████████▉| 13222/13300 [49:53<00:33,  2.35pipeline/s]Optimization Progress: 13302pipeline [49:53,  3.34pipeline/s]                     
Generation 132 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                             _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 13302pipeline [49:53,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13302pipeline [49:54,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:07:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13302pipeline [49:54,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13302pipeline [49:54,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13302pipeline [49:56,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13302pipeline [49:56,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13302pipeline [49:57,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 13302pipeline [49:57,  3.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=2 [17:08:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13302pipeline [49:57,  3.34pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13304/13400 [49:57<00:28,  3.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13305/13400 [49:57<00:28,  3.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13306/13400 [49:57<00:28,  3.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13307/13400 [49:57<00:27,  3.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13308/13400 [49:57<00:27,  3.34pipeline/s]Optimization Progress:  99%|█████████▉| 13309/13400 [49:57<00:35,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13309/13400 [49:57<00:35,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 13314/13400 [50:01<00:44,  1.93pipeline/s]Optimization Progress: 100%|█████████▉| 13391/13400 [50:04<00:03,  2.68pipeline/s]
Generation 133 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13402pipeline [50:05,  2.68pipeline/s]Optimization Progress: 13402pipeline [50:05,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13402pipeline [50:05,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 13402pipeline [50:07,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13402pipeline [50:07,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13402pipeline [50:08,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13402pipeline [50:08,  3.54pipeline/s]Optimization Progress:  99%|█████████▉| 13406/13500 [50:08<00:39,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13406/13500 [50:08<00:39,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13407/13500 [50:08<00:39,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13408/13500 [50:08<00:38,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13409/13500 [50:08<00:38,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13410/13500 [50:08<00:38,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13411/13500 [50:08<00:37,  2.36pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13412/13500 [50:08<00:37,  2.36pipeline/s]Optimization Progress:  99%|█████████▉| 13414/13500 [50:13<00:41,  2.09pipeline/s]Optimization Progress: 100%|█████████▉| 13494/13500 [50:14<00:02,  2.96pipeline/s]
Generation 134 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 13502pipeline [50:14,  2.96pipeline/s]Optimization Progress: 13502pipeline [50:14,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 13502pipeline [50:15,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 13502pipeline [50:15,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 13502pipeline [50:16,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 13502pipeline [50:16,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 13502pipeline [50:16,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 13502pipeline [50:16,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13502pipeline [50:17,  4.00pipeline/s]Optimization Progress:  99%|█████████▉| 13508/13600 [50:17<00:31,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13508/13600 [50:17<00:31,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13509/13600 [50:17<00:31,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13510/13600 [50:17<00:30,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13511/13600 [50:17<00:30,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13512/13600 [50:17<00:30,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13513/13600 [50:17<00:29,  2.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13514/13600 [50:17<00:29,  2.91pipeline/s]Optimization Progress:  99%|█████████▉| 13516/13600 [50:22<00:34,  2.42pipeline/s]Optimization Progress: 100%|█████████▉| 13596/13600 [50:23<00:01,  3.41pipeline/s]
Generation 135 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13602pipeline [50:23,  3.41pipeline/s]Optimization Progress: 13602pipeline [50:23,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 13602pipeline [50:24,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13602pipeline [50:24,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:08:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13602pipeline [50:24,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13602pipeline [50:25,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13602pipeline [50:26,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13602pipeline [50:27,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13602pipeline [50:27,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13602pipeline [50:27,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13602pipeline [50:27,  4.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 13602pipeline [50:27,  4.76pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13606/13700 [50:28<00:19,  4.76pipeline/s]Optimization Progress:  99%|█████████▉| 13607/13700 [50:28<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13607/13700 [50:28<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13608/13700 [50:28<00:39,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13609/13700 [50:28<00:39,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13610/13700 [50:28<00:38,  2.32pipeline/s]Optimization Progress:  99%|█████████▉| 13612/13700 [50:32<00:46,  1.91pipeline/s]Optimization Progress: 100%|█████████▉| 13692/13700 [50:42<00:03,  2.45pipeline/s]
Generation 136 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13702pipeline [50:43,  2.45pipeline/s]Optimization Progress: 13702pipeline [50:43,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:44,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13702pipeline [50:44,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 13702pipeline [50:44,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:44,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13702pipeline [50:45,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:46,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:46,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:46,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 13702pipeline [50:47,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13702pipeline [50:47,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=2 [17:08:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:47,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13702pipeline [50:48,  3.24pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:08:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13702pipeline [50:48,  3.24pipeline/s]Optimization Progress:  99%|█████████▉| 13703/13800 [50:48<02:38,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13703/13800 [50:48<02:38,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13704/13800 [50:48<02:36,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13705/13800 [50:48<02:35,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13706/13800 [50:48<02:33,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13707/13800 [50:48<02:31,  1.63s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13708/13800 [50:48<02:30,  1.63s/pipeline]Optimization Progress:  99%|█████████▉| 13710/13800 [50:53<02:00,  1.34s/pipeline]Optimization Progress: 100%|█████████▉| 13790/13800 [51:00<00:09,  1.04pipeline/s]
Generation 137 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13802pipeline [51:00,  1.04pipeline/s]Optimization Progress: 13802pipeline [51:00,  1.47pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13802pipeline [51:01,  1.47pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 13802pipeline [51:02,  1.47pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13802pipeline [51:03,  1.47pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13803/13900 [51:04<01:06,  1.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13804/13900 [51:04<01:05,  1.47pipeline/s]Optimization Progress:  99%|█████████▉| 13805/13900 [51:04<01:17,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13805/13900 [51:04<01:17,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13806/13900 [51:04<01:16,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13807/13900 [51:04<01:15,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13808/13900 [51:04<01:14,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13809/13900 [51:04<01:14,  1.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13810/13900 [51:04<01:13,  1.23pipeline/s]Optimization Progress:  99%|█████████▉| 13812/13900 [51:08<01:07,  1.30pipeline/s]Optimization Progress: 100%|█████████▉| 13892/13900 [51:12<00:04,  1.82pipeline/s]
Generation 138 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 13902pipeline [51:12,  1.82pipeline/s]Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 13902pipeline [51:12,  2.58pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:09:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 13902pipeline [51:15,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 13906/14000 [51:15<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13906/14000 [51:15<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13907/14000 [51:15<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13908/14000 [51:15<00:48,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13909/14000 [51:15<00:48,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13910/14000 [51:15<00:47,  1.89pipeline/s]Optimization Progress:  99%|█████████▉| 13912/14000 [51:20<00:52,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 13992/14000 [51:22<00:03,  2.34pipeline/s]
Generation 139 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14002pipeline [51:23,  2.34pipeline/s]Optimization Progress: 14002pipeline [51:23,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 14002pipeline [51:23,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 14002pipeline [51:23,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14002pipeline [51:23,  3.13pipeline/s]Optimization Progress:  99%|█████████▉| 14008/14100 [51:25<00:33,  2.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14008/14100 [51:25<00:33,  2.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14009/14100 [51:25<00:32,  2.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14010/14100 [51:25<00:32,  2.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14011/14100 [51:25<00:32,  2.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14012/14100 [51:25<00:31,  2.76pipeline/s]Optimization Progress:  99%|█████████▉| 14014/14100 [51:31<00:45,  1.89pipeline/s]Optimization Progress: 100%|█████████▉| 14094/14100 [51:32<00:02,  2.68pipeline/s]
Generation 140 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 14102pipeline [51:33,  2.68pipeline/s]Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:09:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:09:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:09:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14102pipeline [51:33,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 14102pipeline [51:34,  3.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14102pipeline [51:35,  3.36pipeline/s]Optimization Progress:  99%|█████████▉| 14108/14200 [51:36<00:34,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14108/14200 [51:36<00:34,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14109/14200 [51:36<00:33,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14110/14200 [51:36<00:33,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14111/14200 [51:36<00:32,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14112/14200 [51:36<00:32,  2.70pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14113/14200 [51:36<00:32,  2.70pipeline/s]Optimization Progress:  99%|█████████▉| 14115/14200 [51:42<00:43,  1.93pipeline/s]Optimization Progress: 100%|█████████▉| 14195/14200 [51:44<00:01,  2.69pipeline/s]
Generation 141 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128457975.89268343	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14202pipeline [51:44,  2.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14202pipeline [51:44,  2.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 14202pipeline [51:46,  2.69pipeline/s]Optimization Progress: 14202pipeline [51:46,  3.04pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14202pipeline [51:46,  3.04pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:09:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14202pipeline [51:47,  3.04pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14202pipeline [51:47,  3.04pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14202pipeline [51:47,  3.04pipeline/s]Optimization Progress:  99%|█████████▉| 14210/14300 [51:49<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14210/14300 [51:49<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14211/14300 [51:49<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14212/14300 [51:49<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14213/14300 [51:49<00:28,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14214/14300 [51:49<00:28,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14215/14300 [51:49<00:28,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14216/14300 [51:49<00:27,  3.01pipeline/s]Optimization Progress:  99%|█████████▉| 14218/14300 [51:52<00:30,  2.65pipeline/s]Optimization Progress: 100%|█████████▉| 14298/14300 [51:58<00:00,  3.51pipeline/s]
Generation 142 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14302pipeline [51:58,  3.51pipeline/s]Optimization Progress: 14302pipeline [51:58,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [51:59,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [51:59,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [51:59,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [51:59,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 14302pipeline [52:00,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [52:00,  4.39pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14302pipeline [52:01,  4.39pipeline/s]Optimization Progress:  99%|█████████▉| 14306/14400 [52:02<00:40,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14306/14400 [52:02<00:40,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14307/14400 [52:02<00:40,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14308/14400 [52:02<00:39,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14309/14400 [52:02<00:39,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14310/14400 [52:02<00:39,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14311/14400 [52:02<00:38,  2.30pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14312/14400 [52:02<00:38,  2.30pipeline/s]Optimization Progress:  99%|█████████▉| 14314/14400 [52:08<00:46,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 14394/14400 [52:09<00:02,  2.64pipeline/s]
Generation 143 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 14402pipeline [52:09,  2.64pipeline/s]Optimization Progress: 14402pipeline [52:09,  3.51pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14402pipeline [52:10,  3.51pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:10:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14402pipeline [52:11,  3.51pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14402pipeline [52:12,  3.51pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:10:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14402pipeline [52:12,  3.51pipeline/s]Optimization Progress:  99%|█████████▉| 14407/14500 [52:13<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14407/14500 [52:13<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14408/14500 [52:13<00:39,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14409/14500 [52:13<00:39,  2.32pipeline/s]Optimization Progress:  99%|█████████▉| 14411/14500 [52:18<00:59,  1.50pipeline/s]Optimization Progress: 100%|█████████▉| 14491/14500 [52:22<00:04,  2.09pipeline/s]
Generation 144 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 14502pipeline [52:23,  2.09pipeline/s]Optimization Progress: 14502pipeline [52:23,  2.81pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14502pipeline [52:24,  2.81pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 14502pipeline [52:25,  2.81pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14502pipeline [52:25,  2.81pipeline/s]Optimization Progress:  99%|█████████▉| 14505/14600 [52:25<00:50,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14505/14600 [52:25<00:50,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14506/14600 [52:25<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14507/14600 [52:25<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14508/14600 [52:25<00:48,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14509/14600 [52:25<00:48,  1.89pipeline/s]Optimization Progress:  99%|█████████▉| 14511/14600 [52:31<00:57,  1.54pipeline/s]Optimization Progress: 100%|█████████▉| 14591/14600 [52:34<00:04,  2.14pipeline/s]
Generation 145 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 14602pipeline [52:35,  2.14pipeline/s]Optimization Progress: 14602pipeline [52:35,  2.99pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 14602pipeline [52:35,  2.99pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14602pipeline [52:36,  2.99pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14602pipeline [52:37,  2.99pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:10:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14602pipeline [52:37,  2.99pipeline/s]Optimization Progress:  99%|█████████▉| 14610/14700 [52:38<00:33,  2.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14610/14700 [52:38<00:33,  2.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14611/14700 [52:38<00:33,  2.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14612/14700 [52:38<00:32,  2.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14613/14700 [52:38<00:32,  2.68pipeline/s]Optimization Progress:  99%|█████████▉| 14615/14700 [52:43<00:48,  1.75pipeline/s]Optimization Progress: 100%|█████████▉| 14695/14700 [52:44<00:02,  2.48pipeline/s]
Generation 146 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14702pipeline [52:44,  2.48pipeline/s]Optimization Progress: 14702pipeline [52:44,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14702pipeline [52:45,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:10:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14702pipeline [52:45,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 14702pipeline [52:45,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 14702pipeline [52:46,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14702pipeline [52:47,  3.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 14702pipeline [52:48,  3.45pipeline/s]Optimization Progress:  99%|█████████▉| 14708/14800 [52:48<00:35,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14708/14800 [52:48<00:35,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14709/14800 [52:48<00:35,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14710/14800 [52:48<00:34,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14711/14800 [52:48<00:34,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14712/14800 [52:48<00:33,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14713/14800 [52:48<00:33,  2.59pipeline/s]Optimization Progress:  99%|█████████▉| 14715/14800 [52:52<00:37,  2.24pipeline/s]Optimization Progress: 100%|█████████▉| 14795/14800 [52:55<00:01,  3.09pipeline/s]
Generation 147 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14802pipeline [52:55,  3.09pipeline/s]Optimization Progress: 14802pipeline [52:55,  4.23pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 14802pipeline [52:56,  4.23pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:11:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14802pipeline [52:57,  4.23pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:11:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14802pipeline [52:58,  4.23pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14802pipeline [52:59,  4.23pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14802pipeline [52:59,  4.23pipeline/s]Optimization Progress:  99%|█████████▉| 14806/14900 [52:59<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14806/14900 [52:59<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14807/14900 [52:59<00:40,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14808/14900 [52:59<00:39,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14809/14900 [52:59<00:39,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14810/14900 [52:59<00:38,  2.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14811/14900 [52:59<00:38,  2.32pipeline/s]Optimization Progress:  99%|█████████▉| 14813/14900 [53:03<00:41,  2.09pipeline/s]Optimization Progress: 100%|█████████▉| 14893/14900 [53:08<00:02,  2.83pipeline/s]
Generation 148 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14902pipeline [53:08,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14902pipeline [53:08,  2.83pipeline/s]Optimization Progress: 14902pipeline [53:08,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14902pipeline [53:09,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14902pipeline [53:10,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 14902pipeline [53:10,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 14902pipeline [53:10,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 14902pipeline [53:10,  3.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:11:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 14902pipeline [53:12,  3.88pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14902/15000 [53:12<00:25,  3.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14903/15000 [53:12<00:25,  3.88pipeline/s]Optimization Progress:  99%|█████████▉| 14904/15000 [53:12<01:16,  1.25pipeline/s]Optimization Progress:  99%|█████████▉| 14906/15000 [53:23<03:31,  2.25s/pipeline]Optimization Progress: 100%|█████████▉| 14985/15000 [53:27<00:23,  1.59s/pipeline]
Generation 149 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 15002pipeline [53:28,  1.59s/pipeline]Optimization Progress: 15002pipeline [53:28,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15002pipeline [53:28,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15002pipeline [53:29,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15002pipeline [53:29,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:11:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 15002pipeline [53:30,  1.12s/pipeline]Optimization Progress:  99%|█████████▉| 15006/15100 [53:31<01:35,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15006/15100 [53:31<01:35,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15007/15100 [53:31<01:34,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15008/15100 [53:31<01:33,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15009/15100 [53:31<01:32,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15010/15100 [53:31<01:31,  1.01s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15011/15100 [53:31<01:30,  1.01s/pipeline]Optimization Progress:  99%|█████████▉| 15013/15100 [53:36<01:19,  1.09pipeline/s]Optimization Progress: 100%|█████████▉| 15093/15100 [53:38<00:04,  1.54pipeline/s]
Generation 150 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15102pipeline [53:40,  1.54pipeline/s]Optimization Progress: 15102pipeline [53:40,  1.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 15102pipeline [53:41,  1.88pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15102pipeline [53:42,  1.88pipeline/s]Optimization Progress:  99%|█████████▉| 15107/15200 [53:42<00:45,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15107/15200 [53:42<00:45,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15108/15200 [53:42<00:45,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15109/15200 [53:42<00:44,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15110/15200 [53:42<00:44,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15111/15200 [53:42<00:43,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15112/15200 [53:42<00:43,  2.04pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15113/15200 [53:42<00:42,  2.04pipeline/s]Optimization Progress:  99%|█████████▉| 15115/15200 [53:47<00:44,  1.93pipeline/s]Optimization Progress: 100%|█████████▉| 15195/15200 [53:50<00:01,  2.66pipeline/s]
Generation 151 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15202pipeline [53:51,  2.66pipeline/s]Optimization Progress: 15202pipeline [53:51,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15202pipeline [53:52,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15202pipeline [53:52,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15202pipeline [53:52,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15202pipeline [53:53,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 15202pipeline [53:53,  3.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15202pipeline [53:54,  3.44pipeline/s]Optimization Progress:  99%|█████████▉| 15207/15300 [53:54<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15207/15300 [53:54<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15208/15300 [53:54<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15209/15300 [53:54<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15210/15300 [53:54<00:32,  2.74pipeline/s]Optimization Progress:  99%|█████████▉| 15212/15300 [53:58<00:45,  1.92pipeline/s]Optimization Progress: 100%|█████████▉| 15292/15300 [54:02<00:03,  2.65pipeline/s]
Generation 152 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15302pipeline [54:02,  2.65pipeline/s]Optimization Progress: 15302pipeline [54:02,  3.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 15302pipeline [54:02,  3.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15302pipeline [54:02,  3.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15302pipeline [54:04,  3.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15302pipeline [54:04,  3.69pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15302pipeline [54:04,  3.69pipeline/s]Optimization Progress:  99%|█████████▉| 15306/15400 [54:05<00:40,  2.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15306/15400 [54:05<00:40,  2.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15307/15400 [54:05<00:40,  2.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15308/15400 [54:05<00:39,  2.31pipeline/s]Optimization Progress:  99%|█████████▉| 15310/15400 [54:09<00:52,  1.71pipeline/s]Optimization Progress: 100%|█████████▉| 15390/15400 [54:11<00:04,  2.41pipeline/s]
Generation 153 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15402pipeline [54:11,  2.41pipeline/s]Optimization Progress: 15402pipeline [54:11,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 15402pipeline [54:11,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 15402pipeline [54:12,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15402pipeline [54:12,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15402pipeline [54:12,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 15402pipeline [54:13,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15402pipeline [54:13,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15402pipeline [54:13,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 15402pipeline [54:14,  3.33pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15402pipeline [54:14,  3.33pipeline/s]Optimization Progress:  99%|█████████▉| 15410/15500 [54:14<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15410/15500 [54:14<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15411/15500 [54:14<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15412/15500 [54:14<00:29,  3.01pipeline/s]Optimization Progress:  99%|█████████▉| 15414/15500 [54:20<00:58,  1.47pipeline/s]Optimization Progress: 100%|█████████▉| 15494/15500 [54:22<00:02,  2.07pipeline/s]
Generation 154 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 15502pipeline [54:22,  2.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15502pipeline [54:23,  2.07pipeline/s]Optimization Progress: 15502pipeline [54:23,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 15502pipeline [54:23,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15502pipeline [54:23,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 15502pipeline [54:24,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:12:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15502pipeline [54:24,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15502pipeline [54:26,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 15502pipeline [54:26,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 15502pipeline [54:26,  2.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 15502pipeline [54:26,  2.71pipeline/s]Optimization Progress:  99%|█████████▉| 15506/15600 [54:27<00:50,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15506/15600 [54:27<00:50,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15507/15600 [54:27<00:50,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15508/15600 [54:27<00:49,  1.86pipeline/s]Optimization Progress:  99%|█████████▉| 15510/15600 [54:32<01:05,  1.37pipeline/s]Optimization Progress: 100%|█████████▉| 15590/15600 [54:36<00:05,  1.90pipeline/s]
Generation 155 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15602pipeline [54:36,  1.90pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 15602pipeline [54:36,  1.90pipeline/s]Optimization Progress: 15602pipeline [54:36,  2.56pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15602pipeline [54:37,  2.56pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15602pipeline [54:37,  2.56pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15602pipeline [54:39,  2.56pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15602pipeline [54:39,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 15605/15700 [54:40<00:56,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15605/15700 [54:40<00:56,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15606/15700 [54:40<00:55,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15607/15700 [54:40<00:55,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15608/15700 [54:40<00:54,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15609/15700 [54:40<00:54,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15610/15700 [54:40<00:53,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15611/15700 [54:40<00:52,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15612/15700 [54:40<00:52,  1.68pipeline/s]Optimization Progress:  99%|█████████▉| 15614/15700 [54:46<00:52,  1.64pipeline/s]Optimization Progress: 100%|█████████▉| 15694/15700 [54:47<00:02,  2.31pipeline/s]
Generation 156 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15702pipeline [54:47,  2.31pipeline/s]Optimization Progress: 15702pipeline [54:47,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15702pipeline [54:47,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15702pipeline [54:48,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15702pipeline [54:48,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15702pipeline [54:48,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15702pipeline [54:48,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:12:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15702pipeline [54:48,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15702pipeline [54:49,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15702pipeline [54:49,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15702pipeline [54:50,  3.15pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15702pipeline [54:50,  3.15pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15704/15800 [54:51<00:30,  3.15pipeline/s]Optimization Progress:  99%|█████████▉| 15705/15800 [54:51<00:52,  1.81pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15705/15800 [54:51<00:52,  1.81pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15706/15800 [54:51<00:51,  1.81pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15707/15800 [54:51<00:51,  1.81pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15708/15800 [54:51<00:50,  1.81pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15709/15800 [54:51<00:50,  1.81pipeline/s]Optimization Progress:  99%|█████████▉| 15711/15800 [54:57<01:00,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 15791/15800 [55:00<00:04,  2.06pipeline/s]
Generation 157 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 15802pipeline [55:00,  2.06pipeline/s]Optimization Progress: 15802pipeline [55:00,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15802pipeline [55:01,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15802pipeline [55:01,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15802pipeline [55:01,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15802pipeline [55:02,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15802pipeline [55:02,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:13:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15802pipeline [55:02,  2.79pipeline/s]Optimization Progress:  99%|█████████▉| 15811/15900 [55:03<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15811/15900 [55:03<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15812/15900 [55:03<00:29,  3.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15813/15900 [55:03<00:28,  3.01pipeline/s]Optimization Progress:  99%|█████████▉| 15815/15900 [55:07<00:46,  1.84pipeline/s]Optimization Progress: 100%|█████████▉| 15895/15900 [55:08<00:01,  2.60pipeline/s]
Generation 158 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 15902pipeline [55:08,  2.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15902pipeline [55:08,  2.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15902pipeline [55:09,  2.60pipeline/s]Optimization Progress: 15902pipeline [55:09,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 15902pipeline [55:09,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15902pipeline [55:10,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 15902pipeline [55:11,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15902pipeline [55:11,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15902pipeline [55:11,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 15902pipeline [55:11,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 15902pipeline [55:12,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 15902pipeline [55:12,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:13:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 15902pipeline [55:12,  3.54pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 15902pipeline [55:12,  3.54pipeline/s]Optimization Progress:  99%|█████████▉| 15906/16000 [55:12<00:46,  2.03pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15906/16000 [55:12<00:46,  2.03pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15907/16000 [55:12<00:45,  2.03pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15908/16000 [55:12<00:45,  2.03pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15909/16000 [55:12<00:44,  2.03pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15910/16000 [55:12<00:44,  2.03pipeline/s]Optimization Progress:  99%|█████████▉| 15912/16000 [55:16<00:48,  1.83pipeline/s]Optimization Progress: 100%|█████████▉| 15992/16000 [55:20<00:03,  2.53pipeline/s]
Generation 159 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 16002pipeline [55:21,  2.53pipeline/s]Optimization Progress: 16002pipeline [55:21,  3.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 16002pipeline [55:21,  3.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16002pipeline [55:22,  3.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 16002pipeline [55:22,  3.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16002pipeline [55:23,  3.20pipeline/s]Optimization Progress:  99%|█████████▉| 16010/16100 [55:24<00:27,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16010/16100 [55:24<00:27,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16011/16100 [55:24<00:27,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16012/16100 [55:24<00:27,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16013/16100 [55:24<00:26,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16014/16100 [55:24<00:26,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16015/16100 [55:24<00:26,  3.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16016/16100 [55:24<00:25,  3.24pipeline/s]Optimization Progress:  99%|█████████▉| 16018/16100 [55:29<00:34,  2.38pipeline/s]Optimization Progress: 100%|█████████▉| 16098/16100 [55:32<00:00,  3.25pipeline/s]
Generation 160 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16102pipeline [55:33,  3.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:13:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16102pipeline [55:33,  3.25pipeline/s]Optimization Progress: 16102pipeline [55:33,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16102pipeline [55:33,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16102pipeline [55:34,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16102pipeline [55:34,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 16102pipeline [55:35,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16102pipeline [55:35,  3.71pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 16102pipeline [55:36,  3.71pipeline/s]Optimization Progress:  99%|█████████▉| 16110/16200 [55:37<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16110/16200 [55:37<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16111/16200 [55:37<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16112/16200 [55:37<00:27,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16113/16200 [55:37<00:27,  3.18pipeline/s]Optimization Progress:  99%|█████████▉| 16115/16200 [55:43<00:50,  1.68pipeline/s]Optimization Progress: 100%|█████████▉| 16195/16200 [55:46<00:02,  2.34pipeline/s]
Generation 161 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 16202pipeline [55:49,  2.34pipeline/s]Optimization Progress: 16202pipeline [55:49,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:13:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16202pipeline [55:49,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16202pipeline [55:49,  2.44pipeline/s]Optimization Progress:  99%|█████████▉| 16204/16300 [55:50<00:43,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16204/16300 [55:50<00:43,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16205/16300 [55:50<00:42,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16206/16300 [55:50<00:42,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16207/16300 [55:50<00:41,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16208/16300 [55:50<00:41,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16209/16300 [55:50<00:40,  2.23pipeline/s]Optimization Progress:  99%|█████████▉| 16211/16300 [55:57<00:56,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 16291/16300 [56:01<00:04,  2.17pipeline/s]
Generation 162 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-97504590.13872418	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16302pipeline [56:01,  2.17pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16302pipeline [56:03,  2.17pipeline/s]Optimization Progress: 16302pipeline [56:03,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16302pipeline [56:03,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 16302pipeline [56:04,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16302pipeline [56:05,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 16302pipeline [56:05,  2.79pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16302pipeline [56:06,  2.79pipeline/s]Optimization Progress:  99%|█████████▉| 16304/16400 [56:06<01:07,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16304/16400 [56:06<01:07,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16305/16400 [56:06<01:06,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16306/16400 [56:06<01:05,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16307/16400 [56:06<01:05,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16308/16400 [56:06<01:04,  1.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16309/16400 [56:06<01:03,  1.43pipeline/s]Optimization Progress:  99%|█████████▉| 16311/16400 [56:11<01:06,  1.35pipeline/s]Optimization Progress: 100%|█████████▉| 16391/16400 [56:15<00:04,  1.88pipeline/s]
Generation 163 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16402pipeline [56:16,  1.88pipeline/s]Optimization Progress: 16402pipeline [56:16,  2.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16402pipeline [56:17,  2.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16402pipeline [56:17,  2.36pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 16402pipeline [56:17,  2.36pipeline/s]Optimization Progress:  99%|█████████▉| 16405/16500 [56:18<00:45,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16405/16500 [56:18<00:45,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16406/16500 [56:18<00:44,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16407/16500 [56:18<00:44,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16408/16500 [56:18<00:43,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16409/16500 [56:18<00:43,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16410/16500 [56:18<00:43,  2.09pipeline/s]Optimization Progress:  99%|█████████▉| 16412/16500 [56:23<00:47,  1.85pipeline/s]Optimization Progress: 100%|█████████▉| 16492/16500 [56:24<00:03,  2.60pipeline/s]
Generation 164 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 16502pipeline [56:24,  2.60pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 16502pipeline [56:25,  2.60pipeline/s]Optimization Progress: 16502pipeline [56:25,  3.53pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 16502pipeline [56:26,  3.53pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16502pipeline [56:27,  3.53pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16502pipeline [56:27,  3.53pipeline/s]Optimization Progress:  99%|█████████▉| 16509/16600 [56:28<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16509/16600 [56:28<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16510/16600 [56:28<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16511/16600 [56:28<00:28,  3.18pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16512/16600 [56:28<00:27,  3.18pipeline/s]Optimization Progress:  99%|█████████▉| 16514/16600 [56:35<00:55,  1.54pipeline/s]Optimization Progress: 100%|█████████▉| 16594/16600 [56:35<00:02,  2.19pipeline/s]
Generation 165 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16602pipeline [56:37,  2.19pipeline/s]Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 16602pipeline [56:37,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 16602pipeline [56:38,  2.76pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16602pipeline [56:39,  2.76pipeline/s]Optimization Progress:  99%|█████████▉| 16608/16700 [56:39<00:36,  2.54pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16608/16700 [56:39<00:36,  2.54pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16609/16700 [56:39<00:35,  2.54pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16610/16700 [56:39<00:35,  2.54pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16611/16700 [56:39<00:35,  2.54pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16612/16700 [56:39<00:34,  2.54pipeline/s]Optimization Progress:  99%|█████████▉| 16613/16700 [56:50<00:34,  2.54pipeline/s]Optimization Progress:  99%|█████████▉| 16614/16700 [56:51<01:15,  1.14pipeline/s]Optimization Progress: 100%|█████████▉| 16694/16700 [56:54<00:03,  1.60pipeline/s]
Generation 166 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:55,  1.60pipeline/s]Optimization Progress: 16702pipeline [56:55,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:55,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:14:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:55,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:56,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 16702pipeline [56:57,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:57,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:15:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16702pipeline [56:57,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 16702pipeline [56:57,  2.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 16702pipeline [56:57,  2.13pipeline/s]Optimization Progress:  99%|█████████▉| 16707/16800 [56:58<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16707/16800 [56:58<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16708/16800 [56:58<00:46,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16709/16800 [56:58<00:45,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16710/16800 [56:58<00:45,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16711/16800 [56:58<00:44,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16712/16800 [56:58<00:44,  2.00pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16713/16800 [56:58<00:43,  2.00pipeline/s]Optimization Progress:  99%|█████████▉| 16715/16800 [57:02<00:43,  1.96pipeline/s]Optimization Progress: 100%|█████████▉| 16795/16800 [57:03<00:01,  2.77pipeline/s]
Generation 167 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16802pipeline [57:03,  2.77pipeline/s]Optimization Progress: 16802pipeline [57:03,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16802pipeline [57:04,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16802pipeline [57:04,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 16802pipeline [57:04,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16802pipeline [57:04,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16802pipeline [57:05,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 16802pipeline [57:06,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16802pipeline [57:06,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16802pipeline [57:06,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16802pipeline [57:06,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16802pipeline [57:07,  3.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16802pipeline [57:07,  3.89pipeline/s]Optimization Progress:  99%|█████████▉| 16811/16900 [57:07<00:27,  3.19pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16811/16900 [57:07<00:27,  3.19pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16812/16900 [57:07<00:27,  3.19pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16813/16900 [57:07<00:27,  3.19pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16814/16900 [57:08<00:26,  3.19pipeline/s]Optimization Progress: 100%|█████████▉| 16816/16900 [57:15<00:58,  1.44pipeline/s]Optimization Progress: 100%|█████████▉| 16896/16900 [57:16<00:01,  2.04pipeline/s]
Generation 168 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16902pipeline [57:16,  2.04pipeline/s]Optimization Progress: 16902pipeline [57:16,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 16902pipeline [57:16,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 16902pipeline [57:16,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16902pipeline [57:17,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 16902pipeline [57:19,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 16902pipeline [57:20,  2.87pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 16902pipeline [57:20,  2.87pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16906/17000 [57:20<00:32,  2.87pipeline/s]Optimization Progress:  99%|█████████▉| 16907/17000 [57:20<00:45,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16907/17000 [57:20<00:45,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16908/17000 [57:20<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16909/17000 [57:20<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16910/17000 [57:20<00:43,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16911/17000 [57:20<00:43,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16912/17000 [57:20<00:42,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16913/17000 [57:21<00:42,  2.06pipeline/s]Optimization Progress: 100%|█████████▉| 16915/17000 [57:26<00:47,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 16995/17000 [57:29<00:02,  2.48pipeline/s]
Generation 169 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 17002pipeline [57:30,  2.48pipeline/s]Optimization Progress: 17002pipeline [57:30,  3.28pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 17002pipeline [57:30,  3.28pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17002pipeline [57:30,  3.28pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17002pipeline [57:31,  3.28pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17002pipeline [57:33,  3.28pipeline/s]Optimization Progress:  99%|█████████▉| 17007/17100 [57:33<00:39,  2.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17007/17100 [57:33<00:39,  2.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17008/17100 [57:33<00:38,  2.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17009/17100 [57:33<00:38,  2.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17010/17100 [57:33<00:37,  2.37pipeline/s]Optimization Progress:  99%|█████████▉| 17012/17100 [57:37<00:45,  1.91pipeline/s]Optimization Progress: 100%|█████████▉| 17092/17100 [57:40<00:03,  2.66pipeline/s]
Generation 170 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 17102pipeline [57:40,  2.66pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 17102pipeline [57:40,  2.66pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17102pipeline [57:40,  2.66pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 17102pipeline [57:40,  2.66pipeline/s]Optimization Progress: 17102pipeline [57:40,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 17102pipeline [57:40,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17102pipeline [57:41,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 17102pipeline [57:41,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 17102pipeline [57:41,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17102pipeline [57:41,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17102pipeline [57:41,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17102pipeline [57:42,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 17102pipeline [57:43,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17102pipeline [57:43,  3.55pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 17102pipeline [57:43,  3.55pipeline/s]Optimization Progress:  99%|█████████▉| 17105/17200 [57:44<00:49,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17105/17200 [57:44<00:49,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17106/17200 [57:44<00:48,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17107/17200 [57:44<00:48,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17108/17200 [57:44<00:47,  1.92pipeline/s]Optimization Progress:  99%|█████████▉| 17110/17200 [57:48<00:54,  1.66pipeline/s]Optimization Progress: 100%|█████████▉| 17190/17200 [57:53<00:04,  2.26pipeline/s]
Generation 171 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 17202pipeline [57:53,  2.26pipeline/s]Optimization Progress: 17202pipeline [57:53,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 17202pipeline [57:54,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17202pipeline [57:54,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17202pipeline [57:55,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17202pipeline [57:55,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17202pipeline [57:55,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 17202pipeline [57:55,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17202pipeline [57:55,  3.13pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 17202pipeline [57:56,  3.13pipeline/s]Optimization Progress:  99%|█████████▉| 17208/17300 [57:58<00:39,  2.33pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17208/17300 [57:58<00:39,  2.33pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17209/17300 [57:58<00:39,  2.33pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17210/17300 [57:58<00:38,  2.33pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17211/17300 [57:58<00:38,  2.33pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17212/17300 [57:58<00:37,  2.33pipeline/s]Optimization Progress: 100%|█████████▉| 17214/17300 [58:02<00:46,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 17294/17300 [58:04<00:02,  2.62pipeline/s]
Generation 172 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 17302pipeline [58:04,  2.62pipeline/s]Optimization Progress: 17302pipeline [58:04,  3.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 17302pipeline [58:05,  3.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17302pipeline [58:05,  3.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17302pipeline [58:05,  3.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 17302pipeline [58:06,  3.57pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 17302pipeline [58:07,  3.57pipeline/s]Optimization Progress:  99%|█████████▉| 17305/17400 [58:08<00:50,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17305/17400 [58:08<00:50,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17306/17400 [58:08<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17307/17400 [58:08<00:49,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17308/17400 [58:08<00:48,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17309/17400 [58:08<00:48,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17310/17400 [58:08<00:47,  1.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17311/17400 [58:08<00:47,  1.89pipeline/s]Optimization Progress: 100%|█████████▉| 17313/17400 [58:11<00:44,  1.95pipeline/s]Optimization Progress: 100%|█████████▉| 17393/17400 [58:13<00:02,  2.75pipeline/s]
Generation 173 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17402pipeline [58:13,  2.75pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 17402pipeline [58:13,  2.75pipeline/s]Optimization Progress: 17402pipeline [58:13,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17402pipeline [58:13,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17402pipeline [58:13,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 17402pipeline [58:13,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17402pipeline [58:13,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 17402pipeline [58:14,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 17402pipeline [58:15,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17402pipeline [58:15,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 17402pipeline [58:16,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17402pipeline [58:16,  3.86pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17402pipeline [58:16,  3.86pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17402/17500 [58:16<00:25,  3.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17403/17500 [58:16<00:25,  3.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17404/17500 [58:16<00:24,  3.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17405/17500 [58:16<00:24,  3.86pipeline/s]Optimization Progress:  99%|█████████▉| 17406/17500 [58:16<00:41,  2.25pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17406/17500 [58:16<00:41,  2.25pipeline/s]Optimization Progress:  99%|█████████▉| 17409/17500 [58:21<01:13,  1.23pipeline/s]Optimization Progress: 100%|█████████▉| 17488/17500 [58:25<00:06,  1.72pipeline/s]
Generation 174 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17502pipeline [58:25,  1.72pipeline/s]Optimization Progress: 17502pipeline [58:25,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 [17:16:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17502pipeline [58:25,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17502pipeline [58:26,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 17502pipeline [58:26,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17502pipeline [58:26,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 17502pipeline [58:27,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17502pipeline [58:28,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 17502pipeline [58:28,  2.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17502pipeline [58:28,  2.44pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17505/17600 [58:28<00:38,  2.44pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 17506/17600 [58:28<00:38,  2.44pipeline/s]Optimization Progress:  99%|█████████▉| 17507/17600 [58:28<00:46,  2.02pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17507/17600 [58:28<00:46,  2.02pipeline/s]Optimization Progress:  99%|█████████▉| 17511/17600 [58:38<01:33,  1.05s/pipeline]Optimization Progress: 100%|█████████▉| 17589/17600 [58:41<00:08,  1.34pipeline/s]
Generation 175 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17602pipeline [58:41,  1.34pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17602pipeline [58:41,  1.34pipeline/s]Optimization Progress: 17602pipeline [58:41,  1.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 17602pipeline [58:41,  1.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 17602pipeline [58:41,  1.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17602pipeline [58:41,  1.89pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17602pipeline [58:44,  1.89pipeline/s]Optimization Progress:  99%|█████████▉| 17608/17700 [58:44<00:49,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17608/17700 [58:44<00:49,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17609/17700 [58:44<00:48,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17610/17700 [58:44<00:48,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17611/17700 [58:44<00:47,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17612/17700 [58:44<00:47,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17613/17700 [58:44<00:46,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 17615/17700 [58:51<00:56,  1.51pipeline/s]Optimization Progress: 100%|█████████▉| 17695/17700 [58:52<00:02,  2.14pipeline/s]
Generation 176 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 17702pipeline [58:53,  2.14pipeline/s]Optimization Progress: 17702pipeline [58:53,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17702pipeline [58:53,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 17702pipeline [58:54,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 17702pipeline [58:54,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17702pipeline [58:55,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17702pipeline [58:55,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 17702pipeline [58:55,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 17702pipeline [58:55,  2.84pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 17702pipeline [58:56,  2.84pipeline/s]Optimization Progress:  99%|█████████▉| 17705/17800 [58:56<00:58,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17705/17800 [58:56<00:58,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17706/17800 [58:56<00:57,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17707/17800 [58:56<00:57,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17708/17800 [58:56<00:56,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17709/17800 [58:56<00:55,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17710/17800 [58:56<00:55,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17711/17800 [58:56<00:54,  1.63pipeline/s]Optimization Progress: 100%|█████████▉| 17713/17800 [59:02<00:55,  1.58pipeline/s]Optimization Progress: 100%|█████████▉| 17793/17800 [59:04<00:03,  2.21pipeline/s]
Generation 177 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17802pipeline [59:04,  2.21pipeline/s]Optimization Progress: 17802pipeline [59:04,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17802pipeline [59:05,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17802pipeline [59:06,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17802pipeline [59:06,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17802pipeline [59:07,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17802pipeline [59:07,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17802pipeline [59:07,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 17802pipeline [59:07,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 17802pipeline [59:07,  3.07pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 17802pipeline [59:08,  3.07pipeline/s]Optimization Progress:  99%|█████████▉| 17805/17900 [59:08<00:57,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17805/17900 [59:08<00:57,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17806/17900 [59:08<00:56,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17807/17900 [59:08<00:55,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17808/17900 [59:08<00:55,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17809/17900 [59:08<00:54,  1.66pipeline/s]Optimization Progress:  99%|█████████▉| 17810/17900 [59:20<00:54,  1.66pipeline/s]Optimization Progress: 100%|█████████▉| 17811/17900 [59:20<01:31,  1.03s/pipeline]Optimization Progress: 100%|█████████▉| 17891/17900 [59:21<00:06,  1.38pipeline/s]
Generation 178 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 17902pipeline [59:22,  1.38pipeline/s]Optimization Progress: 17902pipeline [59:22,  1.94pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17902pipeline [59:22,  1.94pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17902pipeline [59:23,  1.94pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17902pipeline [59:24,  1.94pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17902pipeline [59:24,  1.94pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 17902pipeline [59:25,  1.94pipeline/s]Optimization Progress:  99%|█████████▉| 17908/18000 [59:26<00:52,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17908/18000 [59:26<00:52,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17909/18000 [59:26<00:51,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17910/18000 [59:26<00:51,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17911/18000 [59:26<00:50,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 17913/18000 [59:33<01:14,  1.16pipeline/s]Optimization Progress: 100%|█████████▉| 17993/18000 [59:37<00:04,  1.63pipeline/s]
Generation 179 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 18002pipeline [59:37,  1.63pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 18002pipeline [59:37,  1.63pipeline/s]Optimization Progress: 18002pipeline [59:37,  2.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 18002pipeline [59:38,  2.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 18002pipeline [59:38,  2.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 18002pipeline [59:38,  2.25pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 18002pipeline [59:40,  2.25pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18003/18100 [59:40<00:43,  2.25pipeline/s]Optimization Progress:  99%|█████████▉| 18004/18100 [59:40<01:13,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18004/18100 [59:40<01:13,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18005/18100 [59:40<01:12,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18006/18100 [59:40<01:11,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18007/18100 [59:40<01:10,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18008/18100 [59:40<01:10,  1.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18009/18100 [59:40<01:09,  1.31pipeline/s]Optimization Progress: 100%|█████████▉| 18011/18100 [59:44<01:02,  1.42pipeline/s]Optimization Progress: 100%|█████████▉| 18091/18100 [59:46<00:04,  1.99pipeline/s]
Generation 180 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205309908.4022672	LinearSVR(FastICA(input_matrix, FastICA__tol=0.5), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-167363961.75375396	LinearSVR(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.75, Nystroem__kernel=rbf, Nystroem__n_components=4), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-128109427.68306914	LinearSVR(SGDRegressor(Nystroem(RobustScaler(input_matrix), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=rbf, Nystroem__n_components=7), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=100.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-103128471.83086064	LinearSVR(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.8, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=1), SGDRegressor__alpha=0.001, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-96751716.88948992	LinearSVR(PolynomialFeatures(SGDRegressor(Nystroem(RandomForestRegressor(RobustScaler(input_matrix), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.3, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.65, Nystroem__kernel=rbf, Nystroem__n_components=4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 18102pipeline [59:46,  1.99pipeline/s]Optimization Progress: 18102pipeline [59:46,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 18102pipeline [59:46,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 18102pipeline [59:46,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 18102pipeline [59:46,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 18102pipeline [59:46,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 18102pipeline [59:47,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 18102pipeline [59:47,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 18102pipeline [59:47,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 18102pipeline [59:48,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 18102pipeline [59:49,  2.83pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [17:17:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7ff410636dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7ff410747669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7ff410754f8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7ff41073bcbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7ff410628f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fea1e3e39dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fea1e3e3067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fea1e3fb27e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fea1e3fbcb4]

.
Optimization Progress: 18102pipeline [59:49,  2.83pipeline/s]                                                             Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18105/18200 [59:50<00:33,  2.83pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18106/18200 [59:50<00:33,  2.83pipeline/s]Optimization Progress:  99%|█████████▉| 18107/18200 [59:50<00:43,  2.16pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18107/18200 [59:50<00:43,  2.16pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18108/18200 [59:50<00:42,  2.16pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18109/18200 [59:50<00:42,  2.16pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18110/18200 [59:50<00:41,  2.16pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18111/18200 [59:50<00:41,  2.16pipeline/s]Optimization Progress: 100%|█████████▉| 18113/18200 [59:56<00:57,  1.52pipeline/s]                                                                                  
Optimization Progress: 100%|█████████▉| 18192/18200 [59:56<00:05,  1.52pipeline/s]                                                                                  60.02 minutes have elapsed. TPOT will close down.
TPOT closed during evaluation in one generation.
WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.
Optimization Progress: 100%|█████████▉| 18192/18200 [59:56<00:05,  1.52pipeline/s]                                                                                  
Optimization Progress: 100%|█████████▉| 18192/18200 [59:56<00:05,  1.52pipeline/s]                                                                                  
TPOT closed prematurely. Will use the current best pipeline.
Optimization Progress: 100%|█████████▉| 18192/18200 [59:56<00:05,  1.52pipeline/s]                                                                                  Best pipeline:
0. RobustScaler()
1. StackingEstimator(estimator=RandomForestRegressor(bootstrap=False,
                                                  max_features=0.3,
                                                  min_samples_leaf=14,
                                                  min_samples_split=4))
2. Nystroem(gamma=0.65, n_components=4)
3. StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.1,
                                         fit_intercept=False, l1_ratio=0.5,
                                         learning_rate='constant', loss='huber',
                                         penalty='elasticnet', power_t=50.0))
4. PolynomialFeatures(include_bias=False)
5. LinearSVR(C=20.0, epsilon=0.1, tol=0.01)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
