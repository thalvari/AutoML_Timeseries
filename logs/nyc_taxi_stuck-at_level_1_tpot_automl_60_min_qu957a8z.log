30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:  10%|█         | 10/100 [00:06<00:54,  1.65pipeline/s]Optimization Progress:  90%|█████████ | 90/100 [00:08<00:04,  2.29pipeline/s]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  2.29pipeline/s]Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 100/100 [00:09<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:10<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:10<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 100/100 [00:10<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  3.23pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  3.23pipeline/s]Optimization Progress:  54%|█████▎    | 107/200 [00:13<00:37,  2.47pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  54%|█████▎    | 107/200 [00:13<00:37,  2.47pipeline/s]Optimization Progress:  55%|█████▍    | 109/200 [00:17<01:24,  1.08pipeline/s]Optimization Progress:  94%|█████████▍| 189/200 [00:20<00:07,  1.52pipeline/s]
Generation 1 - Current Pareto front scores:
-1	-370179417.4270652	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-362322198.9589519	XGBRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  1.52pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [00:21<00:00,  1.52pipeline/s]Optimization Progress: 100%|██████████| 200/200 [00:21<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 200/200 [00:21<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 200/200 [00:22<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [00:23<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:13:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 200/200 [00:24<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 200/200 [00:24<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [00:25<00:00,  2.06pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [00:25<00:00,  2.06pipeline/s]Optimization Progress:  68%|██████▊   | 204/300 [00:25<01:04,  1.50pipeline/s]Optimization Progress:  68%|██████▊   | 204/300 [00:39<01:04,  1.50pipeline/s]Optimization Progress:  68%|██████▊   | 205/300 [00:50<12:16,  7.75s/pipeline]Optimization Progress:  95%|█████████▌| 285/300 [00:52<01:21,  5.44s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-370179417.4270652	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-362322198.9589519	XGBRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-3	-362319909.9149116	XGBRegressor(SGDRegressor(FastICA(input_matrix, FastICA__tol=0.4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [00:53<00:00,  5.44s/pipeline]Optimization Progress: 100%|██████████| 300/300 [00:53<00:00,  3.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 300/300 [00:55<00:00,  3.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [00:55<00:00,  3.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 300/300 [00:58<00:00,  3.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 300/300 [00:58<00:00,  3.81s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  75%|███████▌  | 300/400 [00:58<06:21,  3.81s/pipeline]Optimization Progress:  76%|███████▌  | 303/400 [01:02<05:52,  3.64s/pipeline]Optimization Progress:  95%|█████████▌| 381/400 [01:19<01:09,  3.64s/pipeline]Optimization Progress:  96%|█████████▌| 382/400 [01:35<00:48,  2.67s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-370179417.4270652	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-362322198.9589519	XGBRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-3	-362319909.9149116	XGBRegressor(SGDRegressor(FastICA(input_matrix, FastICA__tol=0.4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [01:36<00:00,  2.67s/pipeline]Optimization Progress: 100%|██████████| 400/400 [01:36<00:00,  1.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 400/400 [01:36<00:00,  1.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 400/400 [01:36<00:00,  1.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [01:36<00:00,  1.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:14:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 400/400 [01:39<00:00,  1.88s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  80%|████████  | 401/500 [01:39<03:05,  1.88s/pipeline]Optimization Progress:  80%|████████  | 402/500 [01:39<02:58,  1.83s/pipeline]Optimization Progress:  80%|████████  | 402/500 [01:49<02:58,  1.83s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:14<19:07, 11.83s/pipeline]Optimization Progress:  97%|█████████▋| 483/500 [02:16<02:20,  8.29s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-370179417.4270652	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-362288753.7941898	XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-4	-361256569.88773996	XGBRegressor(SelectPercentile(SGDRegressor(FastICA(input_matrix, FastICA__tol=0.4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), SelectPercentile__percentile=63), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [02:17<00:00,  8.29s/pipeline]Optimization Progress: 100%|██████████| 500/500 [02:17<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [02:17<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [02:17<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [02:17<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:15:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 [17:15:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [02:18<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [02:19<00:00,  5.81s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [02:19<00:00,  5.81s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [02:19<07:15,  4.45s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [02:29<07:15,  4.45s/pipeline]Optimization Progress:  84%|████████▍ | 503/600 [02:51<20:14, 12.52s/pipeline]Optimization Progress:  97%|█████████▋| 583/600 [03:25<02:31,  8.89s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-362317689.7417933	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-2	-362288753.7941898	XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-3	-361166093.3744925	XGBRegressor(SelectPercentile(SGDRegressor(input_matrix, SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), SelectPercentile__percentile=63), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-5	-352013815.65798694	XGBRegressor(SelectPercentile(SGDRegressor(FastICA(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), FastICA__tol=0.4), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), SelectPercentile__percentile=80), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-6	-343699405.43721545	LinearSVR(OneHotEncoder(AdaBoostRegressor(Normalizer(Normalizer(ZeroCount(input_matrix), Normalizer__norm=l2), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _mate_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 600/600 [03:25<00:00,  8.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 600/600 [03:26<00:00,  8.89s/pipeline]Optimization Progress: 100%|██████████| 600/600 [03:26<00:00,  6.23s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 600/600 [03:26<00:00,  6.23s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 600/600 [03:28<00:00,  6.23s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [03:28<00:00,  6.23s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 600/600 [03:28<00:00,  6.23s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:16:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 600/600 [03:29<00:00,  6.23s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▌ | 601/700 [03:29<10:16,  6.23s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [03:29<07:57,  4.87s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▌ | 602/700 [03:29<07:57,  4.87s/pipeline]Optimization Progress:  86%|████████▌ | 603/700 [03:39<07:52,  4.87s/pipeline]Optimization Progress:  86%|████████▋ | 604/700 [04:04<13:54,  8.69s/pipeline]Optimization Progress:  98%|█████████▊| 684/700 [05:34<01:42,  6.42s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-362030487.3351959	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4)
-2	-357367295.6420322	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350244873.5011544	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [05:34<00:00,  6.42s/pipeline]Optimization Progress: 100%|██████████| 700/700 [05:34<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 700/700 [05:34<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 700/700 [05:35<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 700/700 [05:36<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 700/700 [05:36<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [05:37<00:00,  4.50s/pipeline]Optimization Progress:  88%|████████▊ | 704/800 [05:43<06:07,  3.83s/pipeline]Optimization Progress:  98%|█████████▊| 782/800 [05:46<00:48,  2.69s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-362030487.3351959	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4)
-2	-357367295.6420322	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350233117.80698895	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277921756.659874	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  2.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  2.69s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  2.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  2.69s/pipeline]Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 800/800 [05:46<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:47<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [05:47<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:18:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:47<00:00,  1.89s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 [17:18:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:48<00:00,  1.89s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  89%|████████▉ | 800/900 [05:48<03:08,  1.89s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  89%|████████▉ | 801/900 [05:48<03:06,  1.89s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [05:59<03:05,  1.89s/pipeline]Optimization Progress:  89%|████████▉ | 803/900 [06:01<04:40,  2.89s/pipeline]Optimization Progress:  98%|█████████▊| 883/900 [06:03<00:34,  2.03s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-357801244.28535324	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)
-2	-357247569.3822069	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350233117.80698895	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-4	-349917839.0248262	XGBRegressor(SGDRegressor(Normalizer(FastICA(input_matrix, FastICA__tol=0.4), Normalizer__norm=l1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277921756.659874	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  2.03s/pipeline]Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:04<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:05<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [06:05<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [06:05<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [06:06<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:06<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 900/900 [06:06<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [06:06<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 900/900 [06:06<00:00,  1.43s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  90%|█████████ | 902/1000 [06:06<02:19,  1.43s/pipeline]Optimization Progress:  90%|█████████ | 903/1000 [06:06<01:59,  1.23s/pipeline]Optimization Progress:  90%|█████████ | 903/1000 [06:19<01:59,  1.23s/pipeline]Optimization Progress:  90%|█████████ | 904/1000 [06:34<14:36,  9.13s/pipeline]Optimization Progress:  98%|█████████▊| 984/1000 [06:36<01:42,  6.40s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-357801244.28535324	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)
-2	-357247569.3822069	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350233117.80698895	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-4	-349917839.0248262	XGBRegressor(SGDRegressor(Normalizer(FastICA(input_matrix, FastICA__tol=0.4), Normalizer__norm=l1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277921756.659874	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [06:36<00:00,  6.40s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [06:36<00:00,  4.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [06:38<00:00,  4.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [17:19:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [06:38<00:00,  4.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:19:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [06:38<00:00,  4.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1000/1000 [06:38<00:00,  4.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  91%|█████████ | 1000/1100 [06:38<07:29,  4.49s/pipeline]Optimization Progress:  91%|█████████ | 1001/1100 [06:49<07:24,  4.49s/pipeline]Optimization Progress:  91%|█████████ | 1002/1100 [06:52<09:01,  5.52s/pipeline]Optimization Progress:  98%|█████████▊| 1082/1100 [06:55<01:09,  3.88s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-357801244.28535324	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)
-2	-357247569.3822069	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350233117.80698895	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-4	-349917839.0248262	XGBRegressor(SGDRegressor(Normalizer(FastICA(input_matrix, FastICA__tol=0.4), Normalizer__norm=l1), SGDRegressor__alpha=0.01, SGDRegressor__eta0=0.1, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277921756.659874	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1100/1100 [06:55<00:00,  3.88s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [06:55<00:00,  2.72s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:20:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [06:56<00:00,  2.72s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:20:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [06:56<00:00,  2.72s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 [17:20:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [06:56<00:00,  2.72s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:20:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [06:56<00:00,  2.72s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 1100/1100 [06:58<00:00,  2.72s/pipeline]Optimization Progress:  92%|█████████▏| 1104/1200 [07:08<04:35,  2.87s/pipeline]Optimization Progress:  98%|█████████▊| 1182/1200 [07:20<00:36,  2.05s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-357801244.28535324	XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)
-2	-356030977.0303303	XGBRegressor(CombineDFs(input_matrix, KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350217117.07480913	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.15000000000000002), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277921756.659874	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [07:20<00:00,  2.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 1200/1200 [07:21<00:00,  2.05s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [07:21<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:20:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1200/1200 [07:22<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [07:22<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [07:23<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:20:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 100%|██████████| 1200/1200 [07:23<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1200/1200 [07:23<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1200/1200 [07:23<00:00,  1.46s/pipeline]Optimization Progress:  92%|█████████▏| 1201/1300 [12:24<2:31:42, 91.94s/pipeline]                                                                                  Skipped pipeline #1202 due to time out. Continuing to the next pipeline.
Optimization Progress:  92%|█████████▏| 1202/1300 [12:24<2:30:10, 91.94s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress:  99%|█████████▊| 1282/1300 [12:27<19:18, 64.37s/pipeline]  
Generation 12 - Current Pareto front scores:
-1	-357619401.1258658	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003)
-2	-355934394.3715087	XGBRegressor(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-350204441.5938544	XGBRegressor(Binarizer(PCA(input_matrix, PCA__iterated_power=5, PCA__svd_solver=randomized), Binarizer__threshold=0.30000000000000004), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277299091.66549605	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-274706239.9726575	LinearSVR(MaxAbsScaler(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:25:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1301pipeline [12:28, 64.37s/pipeline]Optimization Progress: 1301pipeline [12:28, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:25:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 [17:25:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 1301pipeline [12:29, 45.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1301pipeline [12:30, 45.08s/pipeline]Optimization Progress:  93%|█████████▎| 1301/1400 [12:39<1:14:22, 45.08s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [16:00<2:35:09, 94.99s/pipeline]Optimization Progress:  99%|█████████▊| 1382/1400 [16:07<19:57, 66.52s/pipeline]  
Generation 13 - Current Pareto front scores:
-1	-357619401.1258658	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.35000000000000003)
-2	-354688793.7316941	XGBRegressor(FastICA(input_matrix, FastICA__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-344924938.73341364	XGBRegressor(Binarizer(StandardScaler(input_matrix), Binarizer__threshold=0.15000000000000002), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-302336122.7132608	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-277299091.66549605	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-274680996.0690163	LinearSVR(MaxAbsScaler(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:29:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1401pipeline [16:08, 66.52s/pipeline]Optimization Progress: 1401pipeline [16:08, 46.58s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [17:29:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1401pipeline [16:08, 46.58s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1401pipeline [16:08, 46.58s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1401pipeline [16:09, 46.58s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1401pipeline [16:11, 46.58s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1401pipeline [16:11, 46.58s/pipeline]Optimization Progress:  93%|█████████▎| 1402/1500 [16:16<57:15, 35.05s/pipeline]Optimization Progress:  99%|█████████▉| 1482/1500 [16:21<07:21, 24.55s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-356933886.02864647	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-354688793.7316941	XGBRegressor(FastICA(input_matrix, FastICA__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-302316060.73121333	LinearSVR(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-276674477.67853165	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=max), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-264799290.40160155	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(LinearSVR(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25), LinearSVR__C=10.0, LinearSVR__dual=False, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1501pipeline [16:21, 24.55s/pipeline]Optimization Progress: 1501pipeline [16:21, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1501pipeline [16:21, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 1501pipeline [16:21, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [17:29:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1501pipeline [16:21, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 1501pipeline [16:22, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:29:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1501pipeline [16:23, 17.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 1501pipeline [16:24, 17.20s/pipeline]Optimization Progress:  94%|█████████▍| 1502/1600 [16:39<28:05, 17.20s/pipeline]Optimization Progress:  94%|█████████▍| 1503/1600 [16:51<26:42, 16.52s/pipeline]Optimization Progress:  99%|█████████▉| 1583/1600 [17:20<03:18, 11.67s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-356933886.02864647	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-354678173.03350365	XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-294156735.0088565	LinearSVR(OneHotEncoder(DecisionTreeRegressor(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=11), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-274759348.14664	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-264799290.40160155	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(LinearSVR(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.25), LinearSVR__C=10.0, LinearSVR__dual=False, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1601pipeline [17:20, 11.67s/pipeline]Optimization Progress: 1601pipeline [17:20,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 1601pipeline [17:21,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1601pipeline [17:21,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 1601pipeline [17:21,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1601pipeline [17:21,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 1601pipeline [17:22,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:30:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1601pipeline [17:23,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 1601pipeline [17:23,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1601pipeline [17:23,  8.18s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1601pipeline [17:24,  8.18s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [17:25<10:15,  6.34s/pipeline]Optimization Progress:  94%|█████████▍| 1604/1700 [18:26<36:23, 22.74s/pipeline]Optimization Progress:  99%|█████████▉| 1684/1700 [18:33<04:15, 15.95s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-354623757.3930565	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-302988342.69432664	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-294156735.0088565	LinearSVR(OneHotEncoder(DecisionTreeRegressor(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=11), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-274624339.6687931	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l1), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-262621430.707326	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(AdaBoostRegressor(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 1701pipeline [18:33, 15.95s/pipeline]Optimization Progress: 1701pipeline [18:33, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1701pipeline [18:33, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1701pipeline [18:34, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1701pipeline [18:34, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1701pipeline [18:34, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:31:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1701pipeline [18:34, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1701pipeline [18:35, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 1701pipeline [18:35, 11.17s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 1701pipeline [18:35, 11.17s/pipeline]Optimization Progress:  95%|█████████▍| 1705/1800 [18:40<13:15,  8.37s/pipeline]Optimization Progress:  99%|█████████▉| 1785/1800 [18:44<01:28,  5.88s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-354623757.3930565	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-302934092.48720044	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=max), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-294156735.0088565	LinearSVR(OneHotEncoder(DecisionTreeRegressor(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), DecisionTreeRegressor__max_depth=6, DecisionTreeRegressor__min_samples_leaf=15, DecisionTreeRegressor__min_samples_split=11), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-274624339.6687931	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l1), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-262621430.707326	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(AdaBoostRegressor(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), AdaBoostRegressor__learning_rate=0.5, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1801pipeline [18:45,  5.88s/pipeline]Optimization Progress: 1801pipeline [18:45,  4.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1801pipeline [18:45,  4.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 1801pipeline [18:46,  4.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1801pipeline [18:47,  4.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..
Optimization Progress: 1801pipeline [18:47,  4.13s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 1801pipeline [18:48,  4.13s/pipeline]Optimization Progress:  95%|█████████▍| 1804/1900 [18:56<06:27,  4.04s/pipeline]Optimization Progress:  99%|█████████▉| 1883/1900 [19:06<00:48,  2.86s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-354623757.3930565	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-302385457.02189237	LinearSVR(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=max), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=chi2, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-280527418.0528916	LinearSVR(OneHotEncoder(ZeroCount(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-260188830.14454865	LinearSVR(OneHotEncoder(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-239990664.84886336	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Normalizer(PolynomialFeatures(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-232612079.84335676	LinearSVR(OneHotEncoder(OneHotEncoder(ZeroCount(RobustScaler(Normalizer(Normalizer(RandomForestRegressor(Nystroem(input_matrix, Nystroem__gamma=0.5, Nystroem__kernel=sigmoid, Nystroem__n_components=9), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.6000000000000001, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=9, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), Normalizer__norm=max))), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 1901pipeline [19:07,  2.86s/pipeline]Optimization Progress: 1901pipeline [19:07,  2.03s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:32:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 1901pipeline [19:10,  2.03s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 1901pipeline [19:10,  2.03s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 1901pipeline [19:10,  2.03s/pipeline]Optimization Progress:  95%|█████████▌| 1903/2000 [19:11<03:10,  1.96s/pipeline]Optimization Progress:  95%|█████████▌| 1904/2000 [19:16<04:29,  2.81s/pipeline]Optimization Progress:  99%|█████████▉| 1984/2000 [19:17<00:31,  1.97s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-354623757.3930565	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-280493516.06510097	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-231349268.5520529	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-10	-229738454.1580245	LinearSVR(OneHotEncoder(OneHotEncoder(KNeighborsRegressor(ZeroCount(MinMaxScaler(AdaBoostRegressor(Normalizer(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.5, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100))), KNeighborsRegressor__n_neighbors=2, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 2001pipeline [19:18,  1.97s/pipeline]Optimization Progress: 2001pipeline [19:18,  1.39s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 2001pipeline [19:18,  1.39s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2001pipeline [19:20,  1.39s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 2001pipeline [19:20,  1.39s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2001pipeline [19:20,  1.39s/pipeline]Optimization Progress:  95%|█████████▌| 2001/2100 [19:29<02:18,  1.39s/pipeline]Optimization Progress:  95%|█████████▌| 2002/2100 [19:37<10:34,  6.47s/pipeline]Optimization Progress:  99%|█████████▉| 2082/2100 [19:39<01:21,  4.54s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-354623757.3930565	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-280493516.06510097	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-231349268.5520529	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-225464484.39459753	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2101pipeline [19:40,  4.54s/pipeline]Optimization Progress: 2101pipeline [19:40,  3.19s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2101pipeline [19:40,  3.19s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2101pipeline [19:42,  3.19s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 2101pipeline [19:42,  3.19s/pipeline]Optimization Progress:  96%|█████████▌| 2103/2200 [19:48<05:45,  3.56s/pipeline]Optimization Progress:  99%|█████████▉| 2182/2200 [19:52<00:45,  2.51s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-354242772.9340952	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-335715940.08204097	XGBRegressor(FastICA(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=35, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance), FastICA__tol=0.2), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-280408642.7334149	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-280135514.49271727	LinearSVR(OneHotEncoder(ZeroCount(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-231349268.5520529	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-225464484.39459753	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:32:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 2201pipeline [19:52,  2.51s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2201pipeline [19:52,  2.51s/pipeline]Optimization Progress: 2201pipeline [19:52,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 2201pipeline [19:53,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 2201pipeline [19:56,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2201pipeline [19:56,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2201pipeline [19:57,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 2201pipeline [19:57,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2201pipeline [19:57,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 2201pipeline [19:57,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2201pipeline [19:57,  1.76s/pipeline]Optimization Progress:  96%|█████████▌| 2204/2300 [20:09<02:48,  1.76s/pipeline]Optimization Progress:  96%|█████████▌| 2205/2300 [20:17<04:57,  3.13s/pipeline]Optimization Progress:  99%|█████████▉| 2285/2300 [20:20<00:33,  2.20s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-354242772.9340952	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-346403691.91913307	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-3	-294163884.9488401	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280408642.7334149	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-280135514.49271727	LinearSVR(OneHotEncoder(ZeroCount(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-231349268.5520529	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-225464484.39459753	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-213620986.0332443	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(LassoLarsCV(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), LassoLarsCV__normalize=True), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 2301pipeline [20:25,  2.20s/pipeline]Optimization Progress: 2301pipeline [20:25,  1.63s/pipeline]Optimization Progress:  96%|█████████▌| 2303/2400 [20:27<02:13,  1.38s/pipeline]Optimization Progress:  96%|█████████▌| 2304/2400 [20:33<04:25,  2.76s/pipeline]Optimization Progress:  99%|█████████▉| 2384/2400 [20:35<00:31,  1.94s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-353751849.33210135	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-345181720.11360455	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-294163884.9488401	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280408642.7334149	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-280135514.49271727	LinearSVR(OneHotEncoder(ZeroCount(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-231349268.5520529	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-225464484.39459753	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-213429259.83960223	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(LassoLarsCV(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), LassoLarsCV__normalize=True), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2401pipeline [20:36,  1.94s/pipeline]Optimization Progress: 2401pipeline [20:36,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2401pipeline [20:37,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2401pipeline [20:37,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:33:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 2401pipeline [20:39,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2401pipeline [20:39,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2401pipeline [20:39,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2401pipeline [20:39,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2401pipeline [20:40,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2401pipeline [20:41,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2401pipeline [20:41,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..
Optimization Progress: 2401pipeline [20:41,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2401pipeline [20:42,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..
Optimization Progress: 2401pipeline [20:42,  1.38s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2401pipeline [20:42,  1.38s/pipeline]Optimization Progress:  96%|█████████▌| 2405/2500 [20:43<02:16,  1.44s/pipeline]Optimization Progress:  96%|█████████▌| 2406/2500 [20:47<03:30,  2.24s/pipeline]Optimization Progress:  99%|█████████▉| 2486/2500 [20:49<00:22,  1.57s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-353751849.33210135	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-294163884.9488401	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280175098.4729498	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-280135514.49271727	LinearSVR(OneHotEncoder(ZeroCount(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), Normalizer__norm=l2)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-230081554.6854145	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-223182104.4062767	LinearSVR(OneHotEncoder(RobustScaler(AdaBoostRegressor(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-213429259.83960223	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(LassoLarsCV(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), LassoLarsCV__normalize=True), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2501pipeline [20:51,  1.57s/pipeline]Optimization Progress: 2501pipeline [20:51,  1.14s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2501pipeline [20:51,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2501pipeline [20:51,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..
Optimization Progress: 2501pipeline [20:52,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..
Optimization Progress: 2501pipeline [20:53,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 X needs to contain only non-negative integers..
Optimization Progress: 2501pipeline [20:53,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2501pipeline [20:55,  1.14s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2501pipeline [20:57,  1.14s/pipeline]Optimization Progress:  96%|█████████▋| 2506/2600 [20:58<01:53,  1.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2506/2600 [20:58<01:53,  1.21s/pipeline]Optimization Progress:  96%|█████████▋| 2508/2600 [21:05<02:52,  1.88s/pipeline]Optimization Progress: 100%|█████████▉| 2588/2600 [21:08<00:15,  1.33s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-294079989.9325119	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280175098.4729498	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-274320833.02831864	LinearSVR(OneHotEncoder(ZeroCount(AdaBoostRegressor(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-6	-230081554.6854145	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-223182104.4062767	LinearSVR(OneHotEncoder(RobustScaler(AdaBoostRegressor(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-213429259.83960223	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(LassoLarsCV(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), LassoLarsCV__normalize=True), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2601pipeline [21:09,  1.33s/pipeline]Optimization Progress: 2601pipeline [21:09,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:34:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 2601pipeline [21:09,  1.07pipeline/s]                                                            _pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2601pipeline [21:10,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2601pipeline [21:10,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2601pipeline [21:11,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2601pipeline [21:12,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2601pipeline [21:12,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2601pipeline [21:12,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 2601pipeline [21:16,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 2601pipeline [21:17,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2601pipeline [21:17,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2601pipeline [21:17,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2601pipeline [21:19,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2601pipeline [21:19,  1.07pipeline/s]Optimization Progress: 2601pipeline [21:19,  1.07pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2601pipeline [21:20,  1.07pipeline/s]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress:  96%|█████████▋| 2602/2700 [26:53<2:49:33, 103.81s/pipeline]                                                                                   Skipped pipeline #2644 due to time out. Continuing to the next pipeline.
Optimization Progress:  98%|█████████▊| 2644/2700 [26:53<1:36:53, 103.81s/pipeline]Optimization Progress:  99%|█████████▉| 2683/2700 [27:02<20:35, 72.70s/pipeline]   
Generation 26 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-294079989.9325119	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280164401.51321137	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-230925857.5502003	LinearSVR(OneHotEncoder(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-230081554.6854145	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-223182104.4062767	LinearSVR(OneHotEncoder(RobustScaler(AdaBoostRegressor(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-213429259.83960223	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(LassoLarsCV(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), LassoLarsCV__normalize=True), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-10	-208545631.21774942	LinearSVR(OneHotEncoder(OneHotEncoder(KNeighborsRegressor(OneHotEncoder(MinMaxScaler(AdaBoostRegressor(Normalizer(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.5, Nystroem__kernel=sigmoid, Nystroem__n_components=9), Normalizer__norm=l2), Normalizer__norm=max), AdaBoostRegressor__learning_rate=0.001, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), KNeighborsRegressor__n_neighbors=2, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2702pipeline [27:02, 72.70s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2702pipeline [27:04, 72.70s/pipeline]Optimization Progress: 2702pipeline [27:04, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2702pipeline [27:07, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2702pipeline [27:08, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 2702pipeline [27:09, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2702pipeline [27:09, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2702pipeline [27:11, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 2702pipeline [27:11, 50.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 2702pipeline [27:11, 50.93s/pipeline]Optimization Progress:  97%|█████████▋| 2706/2800 [27:12<56:42, 36.20s/pipeline]Optimization Progress:  97%|█████████▋| 2707/2800 [27:21<43:31, 28.08s/pipeline]Optimization Progress: 100%|█████████▉| 2787/2800 [27:27<04:15, 19.68s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-294079989.9325119	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-280164401.51321137	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-230925857.5502003	LinearSVR(OneHotEncoder(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-230081554.6854145	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=9), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2802pipeline [27:29, 19.68s/pipeline]Optimization Progress: 2802pipeline [27:29, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 2802pipeline [27:29, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 [17:40:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 2802pipeline [27:29, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 2802pipeline [27:29, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2802pipeline [27:32, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 2802pipeline [27:33, 13.82s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2802pipeline [27:33, 13.82s/pipeline]Optimization Progress:  97%|█████████▋| 2803/2900 [27:36<18:47, 11.62s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2803/2900 [27:36<18:47, 11.62s/pipeline]Optimization Progress:  97%|█████████▋| 2805/2900 [27:42<14:22,  9.08s/pipeline]Optimization Progress:  99%|█████████▉| 2885/2900 [27:48<01:35,  6.38s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293874694.98968315	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-280164401.51321137	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-5	-230925857.5502003	LinearSVR(OneHotEncoder(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-209598160.69945118	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2902pipeline [27:48,  6.38s/pipeline]Optimization Progress: 2902pipeline [27:48,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 2902pipeline [27:48,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.01000.
Optimization Progress: 2902pipeline [27:48,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 2902pipeline [27:48,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 2902pipeline [27:49,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2902pipeline [27:50,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2902pipeline [27:50,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 2902pipeline [27:51,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 2902pipeline [27:51,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:41:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 2902pipeline [27:52,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 2902pipeline [27:53,  4.47s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 2902pipeline [27:53,  4.47s/pipeline]                                                            Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 2903/3000 [27:55<07:13,  4.47s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2904/3000 [27:55<07:08,  4.47s/pipeline]Optimization Progress:  97%|█████████▋| 2905/3000 [27:55<06:04,  3.84s/pipeline]Optimization Progress:  97%|█████████▋| 2908/3000 [28:04<05:25,  3.54s/pipeline]Optimization Progress: 100%|█████████▉| 2986/3000 [28:09<00:35,  2.50s/pipeline]
Generation 29 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293867935.2239855	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.8500000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-230925857.5502003	LinearSVR(OneHotEncoder(Normalizer(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=9), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-209598160.69945118	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(XGBRegressor(Normalizer(input_matrix, Normalizer__norm=l2), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=16, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3002pipeline [28:10,  2.50s/pipeline]Optimization Progress: 3002pipeline [28:10,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 3002pipeline [28:10,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3002pipeline [28:12,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 3002pipeline [28:12,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3002pipeline [28:12,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3002pipeline [28:12,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3002pipeline [28:13,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3002pipeline [28:14,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3002pipeline [28:14,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3002pipeline [28:14,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3002pipeline [28:15,  1.76s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3002pipeline [28:15,  1.76s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3003/3100 [28:17<02:50,  1.76s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3004/3100 [28:17<02:48,  1.76s/pipeline]Optimization Progress:  97%|█████████▋| 3005/3100 [28:17<03:03,  1.93s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3005/3100 [28:17<03:03,  1.93s/pipeline]Optimization Progress:  97%|█████████▋| 3008/3100 [28:27<03:42,  2.42s/pipeline]Optimization Progress: 100%|█████████▉| 3087/3100 [28:35<00:22,  1.72s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293830753.43358785	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-209485760.4692003	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3102pipeline [28:36,  1.72s/pipeline]Optimization Progress: 3102pipeline [28:36,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 3102pipeline [28:36,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:41:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 3102pipeline [28:37,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 3102pipeline [28:39,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3102pipeline [28:39,  1.23s/pipeline]Optimization Progress:  97%|█████████▋| 3103/3200 [28:47<06:29,  4.02s/pipeline]Optimization Progress:  99%|█████████▉| 3183/3200 [28:53<00:48,  2.84s/pipeline]
Generation 31 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293830753.43358785	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-209485760.4692003	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3202pipeline [28:54,  2.84s/pipeline]Optimization Progress: 3202pipeline [28:54,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 3202pipeline [28:55,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3202pipeline [28:57,  2.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 3202pipeline [28:57,  2.00s/pipeline]Optimization Progress:  97%|█████████▋| 3203/3300 [28:58<04:05,  2.53s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3203/3300 [28:58<04:05,  2.53s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3204/3300 [28:58<04:03,  2.53s/pipeline]Optimization Progress:  97%|█████████▋| 3206/3300 [29:07<04:12,  2.69s/pipeline]Optimization Progress: 100%|█████████▉| 3286/3300 [29:13<00:26,  1.90s/pipeline]
Generation 32 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293830753.43358785	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-209485760.4692003	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204935713.68363768	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [17:42:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 3302pipeline [29:13,  1.90s/pipeline]Optimization Progress: 3302pipeline [29:13,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 x and y arrays must have at least 2 entries.
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=2 x and y arrays must have at least 2 entries.
Optimization Progress: 3302pipeline [29:16,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3302pipeline [29:17,  1.34s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.
Optimization Progress: 3302pipeline [29:18,  1.34s/pipeline]                                                            Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3302/3400 [29:19<02:11,  1.34s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3303/3400 [29:19<02:09,  1.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3304/3400 [29:19<02:08,  1.34s/pipeline]Optimization Progress:  97%|█████████▋| 3305/3400 [29:19<02:27,  1.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3305/3400 [29:20<02:27,  1.55s/pipeline]Optimization Progress:  97%|█████████▋| 3306/3400 [29:30<02:25,  1.55s/pipeline]Optimization Progress:  97%|█████████▋| 3307/3400 [29:34<05:02,  3.25s/pipeline]Optimization Progress: 100%|█████████▉| 3387/3400 [29:39<00:29,  2.30s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293830299.90889585	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-209485760.4692003	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204935713.68363768	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=17, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3402pipeline [29:41,  2.30s/pipeline]Optimization Progress: 3402pipeline [29:41,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 3402pipeline [29:41,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3402pipeline [29:41,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 3402pipeline [29:42,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 3402pipeline [29:42,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3402pipeline [29:43,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3402pipeline [29:43,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 3402pipeline [29:43,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3402pipeline [29:43,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3402pipeline [29:44,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3402pipeline [29:44,  1.63s/pipeline]Optimization Progress:  97%|█████████▋| 3405/3500 [29:45<02:24,  1.52s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3405/3500 [29:45<02:24,  1.52s/pipeline]Optimization Progress:  97%|█████████▋| 3407/3500 [30:06<06:37,  4.27s/pipeline]Optimization Progress: 100%|█████████▉| 3487/3500 [30:08<00:38,  3.00s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293826181.0005393	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-280130006.7400915	LinearSVR(OneHotEncoder(ZeroCount(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-209485760.4692003	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204284533.4344028	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(DecisionTreeRegressor(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=3), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3502pipeline [30:08,  3.00s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 3502pipeline [30:08,  3.00s/pipeline]Optimization Progress: 3502pipeline [30:08,  2.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:43:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 3502pipeline [30:10,  2.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 3502pipeline [30:10,  2.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3502pipeline [30:11,  2.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3502pipeline [30:11,  2.10s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 3502pipeline [30:13,  2.10s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3503/3600 [30:13<03:23,  2.10s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3504/3600 [30:13<03:21,  2.10s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3505/3600 [30:13<03:19,  2.10s/pipeline]Optimization Progress:  97%|█████████▋| 3506/3600 [30:13<02:56,  1.87s/pipeline]Optimization Progress:  97%|█████████▋| 3506/3600 [30:30<02:56,  1.87s/pipeline]Optimization Progress:  97%|█████████▋| 3507/3600 [30:30<09:53,  6.39s/pipeline]Optimization Progress: 100%|█████████▉| 3587/3600 [30:33<00:58,  4.48s/pipeline]
Generation 35 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293826181.0005393	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-221830508.36336556	LinearSVR(OneHotEncoder(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-209198622.7228858	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204284533.4344028	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(DecisionTreeRegressor(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=3), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3602pipeline [30:34,  4.48s/pipeline]Optimization Progress: 3602pipeline [30:34,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 3602pipeline [30:35,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 3602pipeline [30:35,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 3602pipeline [30:36,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3602pipeline [30:36,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3602pipeline [30:37,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3602pipeline [30:38,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3602pipeline [30:38,  3.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [17:43:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f2f1ed80dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f2f1ee91669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f2f1ee9ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f2f1ee85cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f2f1ed72f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f252cb2d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f252cb2d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f252cb4527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f252cb45cb4]

.
Optimization Progress: 3602pipeline [30:39,  3.16s/pipeline]Optimization Progress:  97%|█████████▋| 3603/3700 [30:40<06:01,  3.73s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3603/3700 [30:40<06:01,  3.73s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  97%|█████████▋| 3604/3700 [30:40<05:58,  3.73s/pipeline]Optimization Progress:  97%|█████████▋| 3606/3700 [30:46<05:07,  3.27s/pipeline]Optimization Progress: 100%|█████████▉| 3686/3700 [30:52<00:32,  2.31s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293826181.0005393	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-221830508.36336556	LinearSVR(OneHotEncoder(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-209198622.7228858	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204284533.4344028	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(DecisionTreeRegressor(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=3), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 3702pipeline [30:52,  2.31s/pipeline]Optimization Progress: 3702pipeline [30:52,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 3702pipeline [30:54,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3702pipeline [30:54,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 3702pipeline [30:54,  1.63s/pipeline]                                                            _pre_test decorator: _mate_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3702pipeline [30:55,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 3702pipeline [30:56,  1.63s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 3702pipeline [30:56,  1.63s/pipeline]Optimization Progress:  98%|█████████▊| 3705/3800 [30:57<02:28,  1.57s/pipeline]Optimization Progress:  98%|█████████▊| 3707/3800 [31:04<03:30,  2.26s/pipeline]Optimization Progress: 100%|█████████▉| 3786/3800 [31:11<00:22,  1.61s/pipeline]
Generation 37 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-341445717.1975881	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=11, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-293826181.0005393	LinearSVR(OneHotEncoder(Nystroem(input_matrix, Nystroem__gamma=0.7000000000000001, Nystroem__kernel=sigmoid, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-4	-221754633.09069115	LinearSVR(OneHotEncoder(PolynomialFeatures(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=sigmoid, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-209198622.7228858	LinearSVR(PolynomialFeatures(OneHotEncoder(Nystroem(Normalizer(input_matrix, Normalizer__norm=l2), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=rbf, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-208310288.00395948	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(PCA(Normalizer(input_matrix, Normalizer__norm=l2), PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-204284533.4344028	LinearSVR(OneHotEncoder(OneHotEncoder(OneHotEncoder(Nystroem(DecisionTreeRegressor(RandomForestRegressor(Normalizer(input_matrix, Normalizer__norm=l2), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=2, RandomForestRegressor__min_samples_split=4, RandomForestRegressor__n_estimators=100), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=3), Nystroem__gamma=0.35000000000000003, Nystroem__kernel=linear, Nystroem__n_components=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=0.5, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 3802pipeline [31:12,  1.61s/pipeline]Optimization Progress: 3802pipeline [31:12,  1.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 3802pipeline [31:13,  1.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3802pipeline [31:14,  1.16s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 3802pipeline [31:15,  1.16s/pipeline]