30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   6%|▌         | 6/100 [00:06<01:42,  1.09s/pipeline]Optimization Progress:  86%|████████▌ | 86/100 [00:08<00:10,  1.30pipeline/s]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  1.30pipeline/s]Optimization Progress: 100%|██████████| 100/100 [00:11<00:00,  1.65pipeline/s]Optimization Progress:  51%|█████     | 102/200 [00:17<02:15,  1.38s/pipeline]Optimization Progress:  91%|█████████ | 182/200 [00:20<00:17,  1.02pipeline/s]
Generation 1 - Current Pareto front scores:
-1	-376605915.636139	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=3, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4)
-2	-302033794.3495634	LinearSVR(Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  1.02pipeline/s]Optimization Progress: 100%|██████████| 200/200 [00:20<00:00,  1.44pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [00:23<00:00,  1.44pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 200/200 [00:26<00:00,  1.44pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 200/200 [00:26<00:00,  1.44pipeline/s]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  67%|██████▋   | 202/300 [00:26<01:07,  1.44pipeline/s]Optimization Progress:  68%|██████▊   | 203/300 [00:26<01:42,  1.05s/pipeline]Optimization Progress:  68%|██████▊   | 205/300 [00:39<04:12,  2.66s/pipeline]Optimization Progress:  95%|█████████▍| 284/300 [00:41<00:29,  1.87s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-362665707.5747949	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3)
-2	-302033794.3495634	LinearSVR(Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:47:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 300/300 [00:41<00:00,  1.87s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:42<00:00,  1.87s/pipeline]Optimization Progress: 100%|██████████| 300/300 [00:42<00:00,  1.33s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [00:45<00:00,  1.33s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [00:45<00:00,  1.33s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 300/300 [00:45<00:00,  1.33s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 300/300 [00:46<00:00,  1.33s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  75%|███████▌  | 301/400 [00:47<02:11,  1.33s/pipeline]Optimization Progress:  76%|███████▌  | 302/400 [00:47<02:39,  1.63s/pipeline]Optimization Progress:  76%|███████▌  | 303/400 [00:53<04:36,  2.85s/pipeline]Optimization Progress:  96%|█████████▌| 383/400 [00:55<00:34,  2.00s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-362665707.5747949	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [00:58<00:00,  2.00s/pipeline]Optimization Progress: 100%|██████████| 400/400 [00:58<00:00,  1.46s/pipeline]Optimization Progress:  80%|████████  | 402/500 [01:09<04:20,  2.66s/pipeline]Optimization Progress:  96%|█████████▋| 482/500 [01:12<00:33,  1.87s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-362665707.5747949	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:47:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 500/500 [01:14<00:00,  1.87s/pipeline]Optimization Progress: 100%|██████████| 500/500 [01:14<00:00,  1.35s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [01:15<00:00,  1.35s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [01:15<00:00,  1.35s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:47:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 500/500 [01:15<00:00,  1.35s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [01:16<01:56,  1.18s/pipeline]Optimization Progress:  84%|████████▍ | 503/600 [01:20<03:14,  2.01s/pipeline]Optimization Progress:  97%|█████████▋| 583/600 [01:26<00:24,  1.43s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-360137813.2250828	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:48:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 600/600 [01:26<00:00,  1.43s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 600/600 [01:28<00:00,  1.43s/pipeline]Optimization Progress: 100%|██████████| 600/600 [01:28<00:00,  1.04s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:48:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 600/600 [01:29<00:00,  1.04s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [15:48:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 600/600 [01:29<00:00,  1.04s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 600/600 [01:29<00:00,  1.04s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [01:30<01:32,  1.06pipeline/s]Optimization Progress:  86%|████████▌ | 603/700 [01:48<10:03,  6.22s/pipeline]Optimization Progress:  98%|█████████▊| 683/700 [01:50<01:14,  4.36s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-360137813.2250828	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 100%|██████████| 700/700 [01:50<00:00,  4.36s/pipeline]Optimization Progress: 100%|██████████| 700/700 [01:50<00:00,  3.06s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [01:50<00:00,  3.06s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 700/700 [01:51<00:00,  3.06s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 100%|██████████| 700/700 [01:53<00:00,  3.06s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [01:53<00:00,  3.06s/pipeline]Optimization Progress:  88%|████████▊ | 703/800 [01:53<04:00,  2.48s/pipeline]Optimization Progress:  88%|████████▊ | 705/800 [02:08<06:17,  3.98s/pipeline]Optimization Progress:  98%|█████████▊| 784/800 [02:12<00:44,  2.79s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-360137813.2250828	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 800/800 [02:13<00:00,  2.79s/pipeline]Optimization Progress: 100%|██████████| 800/800 [02:13<00:00,  1.98s/pipeline]Optimization Progress:  89%|████████▉ | 801/900 [02:35<13:10,  7.99s/pipeline]Optimization Progress:  98%|█████████▊| 881/900 [02:37<01:46,  5.60s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [02:39<00:00,  5.60s/pipeline]Optimization Progress: 100%|██████████| 900/900 [02:39<00:00,  3.94s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 900/900 [02:39<00:00,  3.94s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 900/900 [02:40<00:00,  3.94s/pipeline]Optimization Progress:  90%|█████████ | 902/1000 [02:41<04:56,  3.03s/pipeline]Optimization Progress:  90%|█████████ | 903/1000 [02:49<07:26,  4.60s/pipeline]Optimization Progress:  98%|█████████▊| 983/1000 [02:51<00:54,  3.23s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-198262123.21563774	LinearSVR(MaxAbsScaler(Nystroem(VarianceThreshold(MaxAbsScaler(input_matrix), VarianceThreshold__threshold=0.0001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7)), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:51<00:00,  3.23s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [02:51<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 1000/1000 [02:51<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:52<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [02:52<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 1000/1000 [02:52<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:52<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [02:53<00:00,  2.27s/pipeline]Optimization Progress:  91%|█████████ | 1002/1100 [03:10<03:42,  2.27s/pipeline]Optimization Progress:  91%|█████████ | 1003/1100 [03:36<09:41,  6.00s/pipeline]Optimization Progress:  98%|█████████▊| 1083/1100 [03:38<01:11,  4.21s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-356340404.6576531	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-217484186.68430963	LinearSVR(FastICA(input_matrix, FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-198262123.21563774	LinearSVR(MaxAbsScaler(Nystroem(VarianceThreshold(MaxAbsScaler(input_matrix), VarianceThreshold__threshold=0.0001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7)), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  4.21s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [03:39<00:00,  2.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1100/1100 [03:40<00:00,  2.96s/pipeline]Optimization Progress:  92%|█████████▏| 1102/1200 [03:44<04:46,  2.92s/pipeline]Optimization Progress:  98%|█████████▊| 1181/1200 [03:48<00:39,  2.06s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217439644.57795802	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-198262123.21563774	LinearSVR(MaxAbsScaler(Nystroem(VarianceThreshold(MaxAbsScaler(input_matrix), VarianceThreshold__threshold=0.0001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7)), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 1200/1200 [03:48<00:00,  2.06s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [03:48<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [03:48<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [03:48<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 1200/1200 [03:49<00:00,  1.45s/pipeline]Optimization Progress:  92%|█████████▏| 1202/1300 [04:00<02:21,  1.45s/pipeline]Optimization Progress:  93%|█████████▎| 1203/1300 [04:46<10:57,  6.78s/pipeline]Optimization Progress:  99%|█████████▊| 1283/1300 [04:48<01:20,  4.75s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217439644.57795802	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-205071642.69535607	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-198262123.21563774	LinearSVR(MaxAbsScaler(Nystroem(VarianceThreshold(MaxAbsScaler(input_matrix), VarianceThreshold__threshold=0.0001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7)), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1300/1300 [04:48<00:00,  4.75s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [04:48<00:00,  3.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [04:48<00:00,  3.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [04:49<00:00,  3.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 1300/1300 [04:49<00:00,  3.33s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [05:00<05:26,  3.33s/pipeline]Optimization Progress:  93%|█████████▎| 1303/1400 [05:59<15:16,  9.45s/pipeline]Optimization Progress:  99%|█████████▉| 1383/1400 [06:02<01:52,  6.63s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217439644.57795802	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-217428140.0875515	LinearSVR(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=6, DecisionTreeRegressor__min_samples_split=4), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-205071642.69535607	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-198262123.21563774	LinearSVR(MaxAbsScaler(Nystroem(VarianceThreshold(MaxAbsScaler(input_matrix), VarianceThreshold__threshold=0.0001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=7)), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [06:02<00:00,  6.63s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1400/1400 [06:03<00:00,  6.63s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [06:03<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [06:03<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [06:03<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1400/1400 [06:04<00:00,  4.64s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▎| 1403/1500 [06:04<07:30,  4.64s/pipeline]Optimization Progress:  94%|█████████▎| 1404/1500 [06:04<05:25,  3.39s/pipeline]Optimization Progress:  94%|█████████▎| 1404/1500 [06:20<05:25,  3.39s/pipeline]Optimization Progress:  94%|█████████▎| 1405/1500 [06:56<28:08, 17.77s/pipeline]Optimization Progress:  99%|█████████▉| 1485/1500 [06:58<03:06, 12.45s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-217439644.57795802	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-217362067.71834618	LinearSVR(FastICA(FastICA(input_matrix, FastICA__tol=0.45), FastICA__tol=0.4), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 100%|██████████| 1500/1500 [06:59<00:00, 12.45s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [06:59<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 1500/1500 [06:59<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 1500/1500 [06:59<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1500/1500 [06:59<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1500/1500 [07:00<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:53:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [07:00<00:00,  8.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1500/1500 [07:00<00:00,  8.73s/pipeline]Optimization Progress:  94%|█████████▍| 1503/1600 [07:01<10:08,  6.27s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1503/1600 [07:01<10:08,  6.27s/pipeline]Optimization Progress:  94%|█████████▍| 1505/1600 [07:05<08:04,  5.10s/pipeline]Optimization Progress:  99%|█████████▉| 1585/1600 [07:08<00:53,  3.58s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208163696.8528871	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [07:08<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [07:08<00:00,  3.58s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [07:08<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1600/1600 [07:08<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1600/1600 [07:09<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 1600/1600 [07:09<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 1600/1600 [07:09<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [07:09<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1600/1600 [07:10<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1600/1600 [07:10<00:00,  2.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:53:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [07:10<00:00,  2.51s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [07:13<03:33,  2.20s/pipeline]Optimization Progress:  99%|█████████▉| 1681/1700 [07:17<00:29,  1.55s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208163696.8528871	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [07:17<00:00,  1.55s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [07:17<00:00,  1.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1700/1700 [07:17<00:00,  1.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 1700/1700 [07:17<00:00,  1.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [07:17<00:00,  1.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [07:18<00:00,  1.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:53:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [07:18<00:00,  1.09s/pipeline]Optimization Progress:  95%|█████████▍| 1704/1800 [07:21<01:43,  1.08s/pipeline]Optimization Progress:  99%|█████████▉| 1782/1800 [07:23<00:13,  1.31pipeline/s]
Generation 17 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208163696.8528871	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.31pipeline/s]Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:54:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1800/1800 [07:24<00:00,  1.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 1800/1800 [07:25<00:00,  1.83pipeline/s]Optimization Progress:  95%|█████████▍| 1804/1900 [07:29<01:15,  1.27pipeline/s]Optimization Progress:  99%|█████████▉| 1882/1900 [07:31<00:10,  1.80pipeline/s]
Generation 18 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208118385.2130614	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  1.80pipeline/s]Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1900/1900 [07:31<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [07:32<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 1900/1900 [07:32<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [07:32<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [07:33<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1900/1900 [07:33<00:00,  2.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 1904/2000 [07:33<00:37,  2.55pipeline/s]Optimization Progress:  95%|█████████▌| 1906/2000 [07:40<01:10,  1.33pipeline/s]Optimization Progress:  99%|█████████▉| 1986/2000 [07:42<00:07,  1.87pipeline/s]
Generation 19 - Current Pareto front scores:
-1	-353502860.6995801	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=19, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208118385.2130614	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  1.87pipeline/s]Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2000/2000 [07:43<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [07:44<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [07:44<00:00,  2.63pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 2000/2000 [07:44<00:00,  2.63pipeline/s]Optimization Progress:  95%|█████████▌| 2005/2100 [07:48<00:57,  1.66pipeline/s]Optimization Progress:  99%|█████████▉| 2082/2100 [07:51<00:07,  2.31pipeline/s]
Generation 20 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208118385.2130614	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 2100/2100 [07:51<00:00,  2.31pipeline/s]Optimization Progress: 100%|██████████| 2100/2100 [07:51<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2100/2100 [07:52<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2100/2100 [07:52<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [07:53<00:00,  3.25pipeline/s]Optimization Progress:  96%|█████████▌| 2105/2200 [07:56<00:46,  2.05pipeline/s]Optimization Progress:  99%|█████████▉| 2181/2200 [07:59<00:06,  2.83pipeline/s]
Generation 21 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208111094.60887223	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-179834666.08588013	LinearSVR(Normalizer(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [07:59<00:00,  2.83pipeline/s]Optimization Progress: 100%|██████████| 2200/2200 [07:59<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [08:00<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [08:01<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 2200/2200 [08:01<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 2200/2200 [08:01<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:54:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 2200/2200 [08:01<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2200/2200 [08:01<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [08:02<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2200/2200 [08:02<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [08:02<00:00,  3.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2200/2200 [08:02<00:00,  3.98pipeline/s]Optimization Progress:  96%|█████████▌| 2205/2300 [08:07<01:03,  1.50pipeline/s]Optimization Progress:  99%|█████████▉| 2283/2300 [08:15<00:08,  2.02pipeline/s]
Generation 22 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208111094.60887223	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2300/2300 [08:15<00:00,  2.02pipeline/s]Optimization Progress: 100%|██████████| 2300/2300 [08:15<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 2300/2300 [08:15<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [08:15<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2300/2300 [08:15<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:54:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 2300/2300 [08:16<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 2300/2300 [08:17<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2300/2300 [08:17<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2300/2300 [08:18<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [08:18<00:00,  2.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2300/2300 [08:18<00:00,  2.85pipeline/s]Optimization Progress:  96%|█████████▌| 2304/2400 [08:22<01:12,  1.32pipeline/s]Optimization Progress:  99%|█████████▉| 2384/2400 [08:26<00:08,  1.84pipeline/s]
Generation 23 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208111094.60887223	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [08:26<00:00,  1.84pipeline/s]Optimization Progress: 100%|██████████| 2400/2400 [08:26<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 2400/2400 [08:27<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 2400/2400 [08:27<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [08:27<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [08:27<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2400/2400 [08:28<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 2400/2400 [08:28<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2400/2400 [08:29<00:00,  2.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 2400/2400 [08:29<00:00,  2.57pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2401/2500 [08:30<00:38,  2.57pipeline/s]Optimization Progress:  96%|█████████▌| 2403/2500 [08:35<01:49,  1.13s/pipeline]Optimization Progress:  99%|█████████▉| 2483/2500 [08:40<00:13,  1.24pipeline/s]
Generation 24 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208111094.60887223	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.
Optimization Progress: 100%|██████████| 2500/2500 [08:40<00:00,  1.24pipeline/s]Optimization Progress: 100%|██████████| 2500/2500 [08:40<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2500/2500 [08:41<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2500/2500 [08:41<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2500/2500 [08:41<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2500/2500 [08:42<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2500/2500 [08:42<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [08:42<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [08:42<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2500/2500 [08:43<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2500/2500 [08:43<00:00,  1.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2500/2500 [08:43<00:00,  1.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2502/2600 [08:43<00:55,  1.76pipeline/s]Optimization Progress:  96%|█████████▋| 2506/2600 [08:49<01:15,  1.25pipeline/s]Optimization Progress:  99%|█████████▉| 2584/2600 [08:51<00:09,  1.76pipeline/s]
Generation 25 - Current Pareto front scores:
-1	-353493362.6773792	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [08:52<00:00,  1.76pipeline/s]Optimization Progress: 100%|██████████| 2600/2600 [08:52<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2600/2600 [08:53<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2600/2600 [08:54<00:00,  2.40pipeline/s]Optimization Progress:  96%|█████████▋| 2604/2700 [08:58<01:14,  1.29pipeline/s]Optimization Progress:  99%|█████████▉| 2684/2700 [09:01<00:08,  1.81pipeline/s]
Generation 26 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [09:01<00:00,  1.81pipeline/s]Optimization Progress: 100%|██████████| 2700/2700 [09:01<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [09:01<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2700/2700 [09:05<00:00,  2.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [09:05<00:00,  2.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2701/2800 [09:05<00:38,  2.55pipeline/s]Optimization Progress:  97%|█████████▋| 2704/2800 [09:09<01:19,  1.21pipeline/s]Optimization Progress:  99%|█████████▉| 2783/2800 [09:11<00:09,  1.70pipeline/s]
Generation 27 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-183988269.39773375	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2800/2800 [09:11<00:00,  1.70pipeline/s]Optimization Progress: 100%|██████████| 2800/2800 [09:11<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [09:12<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [09:12<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [09:12<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [09:13<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [09:14<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [09:14<00:00,  2.35pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [09:14<00:00,  2.35pipeline/s]Optimization Progress:  97%|█████████▋| 2803/2900 [09:21<01:59,  1.23s/pipeline]Optimization Progress:  99%|█████████▉| 2883/2900 [09:24<00:14,  1.14pipeline/s]
Generation 28 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 2900/2900 [09:24<00:00,  1.14pipeline/s]Optimization Progress: 100%|██████████| 2900/2900 [09:24<00:00,  1.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 2900/2900 [09:25<00:00,  1.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 2900/2900 [09:28<00:00,  1.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2901/3000 [09:28<01:01,  1.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2902/3000 [09:28<01:00,  1.62pipeline/s]Optimization Progress:  97%|█████████▋| 2903/3000 [09:40<00:59,  1.62pipeline/s]Optimization Progress:  97%|█████████▋| 2904/3000 [10:56<11:44,  7.34s/pipeline]Optimization Progress:  99%|█████████▉| 2984/3000 [10:59<01:22,  5.15s/pipeline]
Generation 29 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [11:00<00:00,  5.15s/pipeline]Optimization Progress: 100%|██████████| 3000/3000 [11:00<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3000/3000 [11:00<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 3000/3000 [11:01<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3000/3000 [11:01<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 3000/3000 [11:01<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 3000/3000 [11:01<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 3000/3000 [11:02<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3000/3000 [11:02<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [11:02<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [11:02<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [11:03<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 3000/3000 [11:03<00:00,  3.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 3000/3000 [11:03<00:00,  3.61s/pipeline]Optimization Progress:  97%|█████████▋| 3002/3100 [11:03<04:52,  2.98s/pipeline]Optimization Progress:  97%|█████████▋| 3004/3100 [11:07<04:16,  2.67s/pipeline]Optimization Progress:  99%|█████████▉| 3083/3100 [11:12<00:32,  1.89s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-207508786.98563296	LinearSVR(FastICA(CombineDFs(input_matrix, input_matrix), FastICA__tol=0.7000000000000001), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172831831.39233205	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3100/3100 [11:12<00:00,  1.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [11:12<00:00,  1.89s/pipeline]Optimization Progress: 100%|██████████| 3100/3100 [11:12<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3100/3100 [11:13<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [11:13<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [11:14<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:57:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3100/3100 [11:14<00:00,  1.33s/pipeline]Optimization Progress:  97%|█████████▋| 3102/3200 [11:15<02:11,  1.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3102/3200 [11:15<02:11,  1.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3103/3200 [11:15<02:10,  1.34s/pipeline]Optimization Progress:  97%|█████████▋| 3105/3200 [11:19<02:11,  1.38s/pipeline]Optimization Progress: 100%|█████████▉| 3185/3200 [11:22<00:14,  1.02pipeline/s]
Generation 31 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205486076.63874108	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [11:22<00:00,  1.02pipeline/s]Optimization Progress: 100%|██████████| 3200/3200 [11:22<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [11:23<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 3200/3200 [11:23<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [11:26<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3200/3200 [11:26<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:58:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3200/3200 [11:26<00:00,  1.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3201/3300 [11:26<01:08,  1.45pipeline/s]Optimization Progress:  97%|█████████▋| 3203/3300 [11:31<02:06,  1.31s/pipeline]Optimization Progress:  99%|█████████▉| 3283/3300 [11:35<00:15,  1.07pipeline/s]
Generation 32 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205486076.63874108	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [11:35<00:00,  1.07pipeline/s]Optimization Progress: 100%|██████████| 3300/3300 [11:35<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3300/3300 [11:35<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3300/3300 [11:36<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3300/3300 [11:37<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3300/3300 [11:37<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3300/3300 [11:37<00:00,  1.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:58:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3300/3300 [11:37<00:00,  1.50pipeline/s]Optimization Progress:  97%|█████████▋| 3302/3400 [11:45<03:08,  1.92s/pipeline]Optimization Progress:  99%|█████████▉| 3381/3400 [11:48<00:25,  1.36s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205486076.63874108	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3400/3400 [11:49<00:00,  1.36s/pipeline]Optimization Progress: 100%|██████████| 3400/3400 [11:49<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [11:50<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3400/3400 [11:50<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:58:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3400/3400 [11:50<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [11:50<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [11:50<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [11:51<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [11:51<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3400/3400 [11:52<00:00,  1.04pipeline/s]Optimization Progress:  97%|█████████▋| 3404/3500 [11:52<01:23,  1.16pipeline/s]Optimization Progress:  97%|█████████▋| 3406/3500 [11:56<01:59,  1.27s/pipeline]Optimization Progress: 100%|█████████▉| 3485/3500 [12:01<00:13,  1.10pipeline/s]
Generation 34 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205486076.63874108	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 3500/3500 [12:01<00:00,  1.10pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [12:01<00:00,  1.10pipeline/s]Optimization Progress: 100%|██████████| 3500/3500 [12:01<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:58:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3500/3500 [12:01<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3500/3500 [12:02<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [12:03<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [12:03<00:00,  1.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3502/3600 [12:04<01:02,  1.56pipeline/s]Optimization Progress:  97%|█████████▋| 3503/3600 [12:04<01:14,  1.30pipeline/s]Optimization Progress:  97%|█████████▋| 3506/3600 [12:10<01:47,  1.15s/pipeline]Optimization Progress: 100%|█████████▉| 3584/3600 [12:13<00:12,  1.23pipeline/s]
Generation 35 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-177260329.28258958	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=6), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 3600/3600 [12:14<00:00,  1.23pipeline/s]Optimization Progress: 100%|██████████| 3600/3600 [12:14<00:00,  1.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [12:15<00:00,  1.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:58:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3600/3600 [12:15<00:00,  1.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [12:16<00:00,  1.68pipeline/s]Optimization Progress:  97%|█████████▋| 3602/3700 [12:16<01:10,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3602/3700 [12:16<01:10,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3603/3700 [12:16<01:09,  1.40pipeline/s]Optimization Progress:  97%|█████████▋| 3604/3700 [12:30<01:08,  1.40pipeline/s]Optimization Progress:  97%|█████████▋| 3605/3700 [13:30<12:31,  7.91s/pipeline]Optimization Progress: 100%|█████████▉| 3685/3700 [13:50<01:24,  5.61s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-173778715.66656527	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-165552927.99440724	LinearSVR(Normalizer(Nystroem(OneHotEncoder(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [13:50<00:00,  5.61s/pipeline]Optimization Progress: 100%|██████████| 3700/3700 [13:50<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [13:50<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [13:50<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [13:51<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [13:51<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [13:52<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [13:52<00:00,  3.93s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:00:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3700/3700 [13:52<00:00,  3.93s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3702/3800 [13:53<06:25,  3.93s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3703/3800 [13:53<06:21,  3.93s/pipeline]Optimization Progress:  97%|█████████▋| 3704/3800 [13:53<04:46,  2.98s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3704/3800 [13:53<04:46,  2.98s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3705/3800 [13:53<04:43,  2.98s/pipeline]Optimization Progress:  98%|█████████▊| 3707/3800 [13:56<03:43,  2.41s/pipeline]Optimization Progress: 100%|█████████▉| 3787/3800 [14:00<00:22,  1.70s/pipeline]
Generation 37 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-173778715.66656527	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-164362711.55832887	LinearSVR(Normalizer(Nystroem(OneHotEncoder(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=10), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:00:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3800/3800 [14:00<00:00,  1.70s/pipeline]Optimization Progress: 100%|██████████| 3800/3800 [14:00<00:00,  1.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 3800/3800 [14:00<00:00,  1.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3800/3800 [14:01<00:00,  1.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3800/3800 [14:01<00:00,  1.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3800/3800 [14:03<00:00,  1.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 3800/3800 [14:03<00:00,  1.19s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3800/3900 [14:03<01:59,  1.19s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3801/3900 [14:03<01:58,  1.19s/pipeline]Optimization Progress:  98%|█████████▊| 3804/3900 [14:06<02:04,  1.29s/pipeline]Optimization Progress: 100%|█████████▉| 3883/3900 [14:09<00:15,  1.09pipeline/s]
Generation 38 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-173778715.66656527	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-164362711.55832887	LinearSVR(Normalizer(Nystroem(OneHotEncoder(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=10), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3900/3900 [14:09<00:00,  1.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:00:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3900/3900 [14:10<00:00,  1.09pipeline/s]Optimization Progress: 100%|██████████| 3900/3900 [14:10<00:00,  1.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3900/3900 [14:11<00:00,  1.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:00:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 3900/3900 [14:12<00:00,  1.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3900/3900 [14:12<00:00,  1.54pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3901/4000 [14:12<01:04,  1.54pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3902/4000 [14:12<01:03,  1.54pipeline/s]Optimization Progress:  98%|█████████▊| 3903/4000 [14:12<01:08,  1.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3903/4000 [14:12<01:08,  1.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3904/4000 [14:12<01:08,  1.41pipeline/s]Optimization Progress:  98%|█████████▊| 3906/4000 [14:16<01:22,  1.14pipeline/s]Optimization Progress: 100%|█████████▉| 3986/4000 [14:18<00:08,  1.61pipeline/s]
Generation 39 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-173778715.66656527	LinearSVR(MinMaxScaler(FastICA(input_matrix, FastICA__tol=0.4)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-172595761.13505632	LinearSVR(MinMaxScaler(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.4), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-6	-168634108.49273258	LinearSVR(Binarizer(Nystroem(OneHotEncoder(DecisionTreeRegressor(MaxAbsScaler(input_matrix), DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=15), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.0, Nystroem__kernel=cosine, Nystroem__n_components=6), Binarizer__threshold=0.25), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-7	-164362711.55832887	LinearSVR(Normalizer(Nystroem(OneHotEncoder(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=10), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-163664482.28279203	LinearSVR(Normalizer(Nystroem(OneHotEncoder(MaxAbsScaler(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=5)), OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:00:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4000/4000 [14:18<00:00,  1.61pipeline/s]Optimization Progress: 100%|██████████| 4000/4000 [14:18<00:00,  2.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4000/4000 [14:19<00:00,  2.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4000/4000 [14:20<00:00,  2.21pipeline/s]Optimization Progress:  98%|█████████▊| 4003/4100 [14:25<01:39,  1.02s/pipeline]Optimization Progress: 100%|█████████▉| 4082/4100 [14:29<00:13,  1.37pipeline/s]
Generation 40 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4100/4100 [14:29<00:00,  1.37pipeline/s]Optimization Progress: 100%|██████████| 4100/4100 [14:29<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:01:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4100/4100 [14:29<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [14:29<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [14:30<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [14:30<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [14:30<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [14:31<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 4100/4100 [14:31<00:00,  1.95pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [14:31<00:00,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4101/4200 [14:32<00:50,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4102/4200 [14:32<00:50,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4103/4200 [14:32<00:49,  1.95pipeline/s]Optimization Progress:  98%|█████████▊| 4104/4200 [14:32<00:56,  1.71pipeline/s]Optimization Progress:  98%|█████████▊| 4107/4200 [14:38<01:39,  1.07s/pipeline]Optimization Progress: 100%|█████████▉| 4185/4200 [14:41<00:11,  1.32pipeline/s]
Generation 41 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4200/4200 [14:41<00:00,  1.32pipeline/s]Optimization Progress: 100%|██████████| 4200/4200 [14:41<00:00,  1.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [14:43<00:00,  1.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4200/4200 [14:43<00:00,  1.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [14:43<00:00,  1.85pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:01:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4200/4200 [14:43<00:00,  1.85pipeline/s]Optimization Progress:  98%|█████████▊| 4203/4300 [14:49<01:54,  1.18s/pipeline]Optimization Progress: 100%|█████████▉| 4281/4300 [14:53<00:15,  1.19pipeline/s]
Generation 42 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 4300/4300 [14:53<00:00,  1.19pipeline/s]Optimization Progress: 100%|██████████| 4300/4300 [14:53<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:53<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4300/4300 [14:53<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4300/4300 [14:54<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [14:55<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [14:55<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [14:55<00:00,  1.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4302/4400 [14:55<00:58,  1.69pipeline/s]Optimization Progress:  98%|█████████▊| 4303/4400 [14:55<01:00,  1.60pipeline/s]Optimization Progress:  98%|█████████▊| 4306/4400 [15:01<01:37,  1.04s/pipeline]Optimization Progress: 100%|█████████▉| 4384/4400 [15:05<00:11,  1.35pipeline/s]
Generation 43 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4400/4400 [15:05<00:00,  1.35pipeline/s]Optimization Progress: 100%|██████████| 4400/4400 [15:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4400/4400 [15:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4400/4400 [15:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4400/4400 [15:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [15:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [15:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4400/4400 [15:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4400/4400 [15:07<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 4400/4400 [15:07<00:00,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4400/4500 [15:08<00:52,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4401/4500 [15:08<00:51,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4402/4500 [15:08<00:51,  1.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4403/4500 [15:08<00:50,  1.91pipeline/s]Optimization Progress:  98%|█████████▊| 4404/4500 [15:08<00:52,  1.82pipeline/s]Optimization Progress:  98%|█████████▊| 4407/4500 [15:13<01:21,  1.14pipeline/s]Optimization Progress: 100%|█████████▉| 4485/4500 [15:15<00:09,  1.60pipeline/s]
Generation 44 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [15:15<00:00,  1.60pipeline/s]Optimization Progress: 100%|██████████| 4500/4500 [15:15<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:01:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4500/4500 [15:15<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 4500/4500 [15:16<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:01:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4500/4500 [15:16<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4500/4500 [15:17<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:01:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4500/4500 [15:17<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4500/4500 [15:17<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [15:17<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [15:18<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4500/4500 [15:18<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 4500/4500 [15:18<00:00,  2.26pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [15:18<00:00,  2.26pipeline/s]Optimization Progress:  98%|█████████▊| 4501/4600 [15:30<00:43,  2.26pipeline/s]Optimization Progress:  98%|█████████▊| 4502/4600 [15:49<08:47,  5.38s/pipeline]Optimization Progress: 100%|█████████▉| 4582/4600 [15:51<01:07,  3.77s/pipeline]
Generation 45 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:02:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4600/4600 [15:51<00:00,  3.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4600/4600 [15:52<00:00,  3.77s/pipeline]Optimization Progress: 100%|██████████| 4600/4600 [15:52<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [15:52<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [15:53<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [15:53<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4600/4600 [15:54<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [15:54<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [15:54<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4600/4600 [15:54<00:00,  2.65s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4600/4600 [15:54<00:00,  2.65s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4600/4700 [15:54<04:24,  2.65s/pipeline]Optimization Progress:  98%|█████████▊| 4603/4700 [15:59<04:11,  2.59s/pipeline]Optimization Progress: 100%|█████████▉| 4682/4700 [16:00<00:32,  1.82s/pipeline]
Generation 46 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4700/4700 [16:00<00:00,  1.82s/pipeline]Optimization Progress: 100%|██████████| 4700/4700 [16:00<00:00,  1.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4700/4700 [16:03<00:00,  1.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:02:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4700/4700 [16:03<00:00,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4702/4800 [16:03<02:04,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4703/4800 [16:03<02:03,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4704/4800 [16:03<02:02,  1.28s/pipeline]Optimization Progress:  98%|█████████▊| 4706/4800 [16:08<01:58,  1.27s/pipeline]Optimization Progress: 100%|█████████▉| 4786/4800 [16:09<00:12,  1.13pipeline/s]
Generation 47 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4800/4800 [16:09<00:00,  1.13pipeline/s]Optimization Progress: 100%|██████████| 4800/4800 [16:09<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4800/4800 [16:09<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4800/4800 [16:09<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:02:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4800/4800 [16:10<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 4800/4800 [16:10<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4800/4800 [16:10<00:00,  1.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4800/4800 [16:11<00:00,  1.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4802/4900 [16:11<01:02,  1.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4803/4900 [16:11<01:02,  1.56pipeline/s]Optimization Progress:  98%|█████████▊| 4804/4900 [16:20<01:01,  1.56pipeline/s]Optimization Progress:  98%|█████████▊| 4805/4900 [16:20<01:42,  1.08s/pipeline]Optimization Progress: 100%|█████████▉| 4885/4900 [16:23<00:11,  1.30pipeline/s]
Generation 48 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.30pipeline/s]Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [16:24<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4900/4900 [16:25<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:03:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 4900/4900 [16:25<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [16:26<00:00,  1.84pipeline/s]Optimization Progress:  98%|█████████▊| 4904/5000 [16:27<01:00,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4904/5000 [16:27<01:00,  1.60pipeline/s]Optimization Progress:  98%|█████████▊| 4906/5000 [16:34<02:21,  1.51s/pipeline]Optimization Progress: 100%|█████████▉| 4986/5000 [16:39<00:15,  1.07s/pipeline]
Generation 49 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5000/5000 [16:41<00:00,  1.07s/pipeline]Optimization Progress: 100%|██████████| 5000/5000 [16:41<00:00,  1.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 5000/5000 [16:42<00:00,  1.25pipeline/s]Optimization Progress:  98%|█████████▊| 5003/5100 [16:42<01:02,  1.55pipeline/s]Optimization Progress:  98%|█████████▊| 5004/5100 [16:48<03:39,  2.28s/pipeline]Optimization Progress: 100%|█████████▉| 5084/5100 [16:52<00:25,  1.61s/pipeline]
Generation 50 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5100/5100 [16:52<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [16:52<00:00,  1.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.61s/pipeline]Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [16:53<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 5100/5100 [16:54<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 5100/5100 [16:54<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 5100/5100 [16:54<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [16:55<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5100/5100 [16:55<00:00,  1.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [16:55<00:00,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5100/5200 [16:55<01:54,  1.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5101/5200 [16:55<01:53,  1.15s/pipeline]Optimization Progress:  98%|█████████▊| 5102/5200 [16:55<02:00,  1.23s/pipeline]Optimization Progress:  98%|█████████▊| 5103/5200 [17:00<03:40,  2.28s/pipeline]Optimization Progress: 100%|█████████▉| 5183/5200 [17:06<00:27,  1.61s/pipeline]
Generation 51 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-146668258.97668254	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(PCA(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), PCA__iterated_power=3, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.61s/pipeline]Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:03:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 5200/5200 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5200/5200 [17:09<00:00,  1.14s/pipeline]Optimization Progress:  98%|█████████▊| 5202/5300 [17:09<01:54,  1.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5202/5300 [17:09<01:54,  1.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5203/5300 [17:09<01:53,  1.17s/pipeline]Optimization Progress:  98%|█████████▊| 5205/5300 [17:45<06:57,  4.39s/pipeline]Optimization Progress: 100%|█████████▉| 5285/5300 [17:48<00:46,  3.08s/pipeline]
Generation 52 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-146668258.97668254	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(PCA(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), PCA__iterated_power=3, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 5300/5300 [17:48<00:00,  3.08s/pipeline]Optimization Progress: 100%|██████████| 5300/5300 [17:48<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5300/5300 [17:50<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 5300/5300 [17:51<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5300/5300 [17:52<00:00,  2.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:04:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 5300/5300 [17:52<00:00,  2.17s/pipeline]Optimization Progress:  98%|█████████▊| 5303/5400 [17:52<03:03,  1.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5303/5400 [17:52<03:03,  1.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5304/5400 [17:52<03:01,  1.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5305/5400 [17:52<02:59,  1.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5306/5400 [17:52<02:57,  1.89s/pipeline]Optimization Progress:  98%|█████████▊| 5308/5400 [18:02<02:59,  1.95s/pipeline]Optimization Progress: 100%|█████████▉| 5388/5400 [18:05<00:16,  1.38s/pipeline]
Generation 53 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-146668258.97668254	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(PCA(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), PCA__iterated_power=3, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5400/5400 [18:05<00:00,  1.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5400/5400 [18:06<00:00,  1.38s/pipeline]Optimization Progress: 100%|██████████| 5400/5400 [18:06<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5400/5400 [18:06<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5400/5400 [18:07<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5400/5400 [18:07<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5400/5400 [18:07<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:04:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 5400/5400 [18:07<00:00,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5400/5500 [18:08<01:38,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5401/5500 [18:08<01:37,  1.01pipeline/s]Optimization Progress:  98%|█████████▊| 5402/5500 [18:08<01:38,  1.00s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5402/5500 [18:08<01:38,  1.00s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5403/5500 [18:08<01:37,  1.00s/pipeline]Optimization Progress:  98%|█████████▊| 5405/5500 [18:14<02:00,  1.27s/pipeline]Optimization Progress: 100%|█████████▉| 5485/5500 [18:16<00:13,  1.11pipeline/s]
Generation 54 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5500/5500 [18:16<00:00,  1.11pipeline/s]Optimization Progress: 100%|██████████| 5500/5500 [18:16<00:00,  1.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5500/5500 [18:17<00:00,  1.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 5500/5500 [18:19<00:00,  1.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5500/5500 [18:19<00:00,  1.58pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5501/5600 [18:19<01:02,  1.58pipeline/s]Optimization Progress:  98%|█████████▊| 5502/5600 [18:30<01:02,  1.58pipeline/s]Optimization Progress:  98%|█████████▊| 5503/5600 [18:37<03:58,  2.46s/pipeline]Optimization Progress: 100%|█████████▉| 5583/5600 [18:40<00:29,  1.74s/pipeline]
Generation 55 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5600/5600 [18:41<00:00,  1.74s/pipeline]Optimization Progress: 100%|██████████| 5600/5600 [18:41<00:00,  1.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5600/5600 [18:41<00:00,  1.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 5600/5600 [18:41<00:00,  1.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 5600/5600 [18:42<00:00,  1.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 5600/5600 [18:42<00:00,  1.22s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5600/5600 [18:42<00:00,  1.22s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5600/5700 [18:43<02:02,  1.22s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5601/5700 [18:43<02:01,  1.22s/pipeline]Optimization Progress:  98%|█████████▊| 5602/5700 [18:43<01:58,  1.21s/pipeline]Optimization Progress:  98%|█████████▊| 5604/5700 [18:52<03:28,  2.17s/pipeline]Optimization Progress: 100%|█████████▉| 5683/5700 [18:55<00:26,  1.53s/pipeline]
Generation 56 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 5700/5700 [18:55<00:00,  1.53s/pipeline]Optimization Progress: 100%|██████████| 5700/5700 [18:55<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress: 100%|██████████| 5700/5700 [18:55<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 5700/5700 [18:55<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5700/5700 [18:56<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5700/5700 [18:56<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5700/5700 [18:57<00:00,  1.08s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5700/5700 [18:57<00:00,  1.08s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5700/5800 [18:57<01:47,  1.08s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5701/5800 [18:57<01:46,  1.08s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5702/5800 [18:57<01:45,  1.08s/pipeline]Optimization Progress:  98%|█████████▊| 5703/5800 [18:57<01:35,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5703/5800 [18:57<01:35,  1.01pipeline/s]Optimization Progress:  98%|█████████▊| 5705/5800 [19:02<02:16,  1.44s/pipeline]Optimization Progress: 100%|█████████▉| 5785/5800 [19:04<00:15,  1.01s/pipeline]
Generation 57 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-116239874.14981277	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5800/5800 [19:05<00:00,  1.01s/pipeline]Optimization Progress: 100%|██████████| 5800/5800 [19:05<00:00,  1.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5800/5800 [19:05<00:00,  1.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5800/5800 [19:06<00:00,  1.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5800/5800 [19:07<00:00,  1.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5800/5800 [19:07<00:00,  1.38pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5802/5900 [19:07<01:11,  1.38pipeline/s]Optimization Progress:  98%|█████████▊| 5803/5900 [19:07<01:12,  1.33pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5803/5900 [19:07<01:12,  1.33pipeline/s]Optimization Progress:  98%|█████████▊| 5805/5900 [19:14<02:21,  1.49s/pipeline]Optimization Progress: 100%|█████████▉| 5885/5900 [19:18<00:15,  1.06s/pipeline]
Generation 58 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-116239874.14981277	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 5900/5900 [19:18<00:00,  1.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [19:18<00:00,  1.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:05:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 5900/5900 [19:18<00:00,  1.06s/pipeline]Optimization Progress: 100%|██████████| 5900/5900 [19:18<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [19:18<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:05:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 5900/5900 [19:19<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5900/5900 [19:19<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [19:20<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 5900/5900 [19:20<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5900/5900 [19:20<00:00,  1.33pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 5900/5900 [19:21<00:00,  1.33pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5900/6000 [19:21<01:14,  1.33pipeline/s]Optimization Progress:  98%|█████████▊| 5902/6000 [19:26<02:47,  1.71s/pipeline]Optimization Progress: 100%|█████████▉| 5982/6000 [19:33<00:22,  1.22s/pipeline]
Generation 59 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-116239874.14981277	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.22s/pipeline]Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:06:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6000/6000 [19:35<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 6000/6000 [19:36<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6000/6000 [19:36<00:00,  1.14pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:06:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6000/6000 [19:37<00:00,  1.14pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 6001/6100 [19:37<01:27,  1.14pipeline/s]Optimization Progress:  98%|█████████▊| 6002/6100 [19:37<01:36,  1.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6002/6100 [19:37<01:36,  1.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6003/6100 [19:37<01:35,  1.02pipeline/s]Optimization Progress:  98%|█████████▊| 6005/6100 [19:41<01:44,  1.10s/pipeline]Optimization Progress: 100%|█████████▉| 6085/6100 [19:43<00:11,  1.29pipeline/s]
Generation 60 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205213035.72999576	LinearSVR(FastICA(input_matrix, FastICA__tol=0.45), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6100/6100 [19:44<00:00,  1.29pipeline/s]Optimization Progress: 100%|██████████| 6100/6100 [19:44<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6100/6100 [19:45<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 6100/6100 [19:46<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6100/6100 [19:46<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 6100/6100 [19:46<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6100/6100 [19:46<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:06:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6100/6100 [19:47<00:00,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6100/6200 [19:47<00:56,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6101/6200 [19:47<00:56,  1.77pipeline/s]Optimization Progress:  98%|█████████▊| 6102/6200 [19:47<01:17,  1.26pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6102/6200 [19:47<01:17,  1.26pipeline/s]Optimization Progress:  98%|█████████▊| 6103/6200 [20:00<01:16,  1.26pipeline/s]Optimization Progress:  98%|█████████▊| 6104/6200 [20:12<06:51,  4.29s/pipeline]Optimization Progress: 100%|█████████▉| 6184/6200 [20:19<00:48,  3.03s/pipeline]
Generation 61 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6200/6200 [20:20<00:00,  3.03s/pipeline]Optimization Progress: 100%|██████████| 6200/6200 [20:20<00:00,  2.14s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6200/6300 [20:22<03:34,  2.14s/pipeline]Optimization Progress:  98%|█████████▊| 6201/6300 [20:22<03:08,  1.91s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6201/6300 [20:22<03:08,  1.91s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6202/6300 [20:22<03:06,  1.91s/pipeline]Optimization Progress:  98%|█████████▊| 6204/6300 [20:28<03:12,  2.01s/pipeline]Optimization Progress: 100%|█████████▉| 6284/6300 [20:32<00:22,  1.42s/pipeline]
Generation 62 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:07:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6300/6300 [20:32<00:00,  1.42s/pipeline]Optimization Progress: 100%|██████████| 6300/6300 [20:32<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6300/6300 [20:33<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6300/6300 [20:33<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:07:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6300/6300 [20:33<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6300/6300 [20:33<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6300/6300 [20:35<00:00,  1.00pipeline/s]Optimization Progress:  98%|█████████▊| 6303/6400 [20:35<01:34,  1.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6303/6400 [20:35<01:34,  1.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6304/6400 [20:35<01:33,  1.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6305/6400 [20:35<01:32,  1.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6306/6400 [20:35<01:31,  1.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6307/6400 [20:35<01:30,  1.03pipeline/s]Optimization Progress:  99%|█████████▊| 6309/6400 [20:40<01:24,  1.08pipeline/s]Optimization Progress: 100%|█████████▉| 6389/6400 [20:41<00:07,  1.52pipeline/s]
Generation 63 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 6400/6400 [20:41<00:00,  1.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 6400/6400 [20:41<00:00,  1.52pipeline/s]Optimization Progress: 100%|██████████| 6400/6400 [20:41<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [20:42<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [20:42<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 6400/6400 [20:42<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [20:42<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6400/6400 [20:42<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [20:43<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [20:44<00:00,  2.16pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6403/6500 [20:44<00:44,  2.16pipeline/s]Optimization Progress:  99%|█████████▊| 6404/6500 [20:44<00:47,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6404/6500 [20:44<00:47,  2.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6405/6500 [20:44<00:47,  2.01pipeline/s]Optimization Progress:  99%|█████████▊| 6407/6500 [20:47<01:07,  1.37pipeline/s]Optimization Progress: 100%|█████████▉| 6487/6500 [20:51<00:06,  1.91pipeline/s]
Generation 64 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-141510491.67982006	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6500/6500 [20:51<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 6500/6500 [20:51<00:00,  1.91pipeline/s]Optimization Progress: 100%|██████████| 6500/6500 [20:51<00:00,  2.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6500/6500 [20:52<00:00,  2.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:07:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6500/6500 [20:53<00:00,  2.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6500/6500 [20:53<00:00,  2.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6500/6500 [20:53<00:00,  2.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 6500/6500 [20:53<00:00,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6501/6600 [20:54<00:36,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6502/6600 [20:54<00:36,  2.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6503/6600 [20:54<00:35,  2.71pipeline/s]Optimization Progress:  99%|█████████▊| 6505/6600 [20:58<01:03,  1.51pipeline/s]Optimization Progress: 100%|█████████▉| 6585/6600 [21:03<00:07,  2.07pipeline/s]
Generation 65 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-141510491.67982006	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-108979350.57874283	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6600/6600 [21:03<00:00,  2.07pipeline/s]Optimization Progress: 100%|██████████| 6600/6600 [21:03<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [21:03<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6600/6600 [21:03<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [21:04<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [21:04<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 6600/6600 [21:05<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6600/6600 [21:05<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [21:05<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6600/6600 [21:06<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6600/6600 [21:06<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 6600/6600 [21:06<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [21:06<00:00,  2.91pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6602/6700 [21:06<00:33,  2.91pipeline/s]Optimization Progress:  99%|█████████▊| 6604/6700 [21:10<01:11,  1.34pipeline/s]Optimization Progress: 100%|█████████▉| 6684/6700 [21:10<00:08,  1.91pipeline/s]
Generation 66 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-141510491.67982006	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-108828749.28052059	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [21:11<00:00,  1.91pipeline/s]Optimization Progress: 100%|██████████| 6700/6700 [21:11<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6700/6700 [21:11<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:07:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6700/6700 [21:12<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [21:12<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6700/6700 [21:13<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6700/6700 [21:13<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [21:13<00:00,  2.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6702/6800 [21:14<00:36,  2.70pipeline/s]Optimization Progress:  99%|█████████▊| 6707/6800 [21:18<00:53,  1.74pipeline/s]Optimization Progress: 100%|█████████▉| 6784/6800 [21:21<00:06,  2.41pipeline/s]
Generation 67 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-141510491.67982006	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6800/6800 [21:21<00:00,  2.41pipeline/s]Optimization Progress: 100%|██████████| 6800/6800 [21:21<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6800/6800 [21:22<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:07:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6800/6800 [21:23<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6800/6800 [21:23<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6800/6800 [21:23<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6800/6800 [21:24<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6800/6800 [21:24<00:00,  3.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 6800/6800 [21:24<00:00,  3.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6801/6900 [21:24<00:29,  3.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6802/6900 [21:24<00:28,  3.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6803/6900 [21:24<00:28,  3.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6804/6900 [21:24<00:28,  3.40pipeline/s]Optimization Progress:  99%|█████████▊| 6805/6900 [21:24<00:34,  2.75pipeline/s]Optimization Progress:  99%|█████████▊| 6809/6900 [21:27<00:43,  2.10pipeline/s]Optimization Progress: 100%|█████████▉| 6886/6900 [21:29<00:04,  2.93pipeline/s]
Generation 68 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-146891363.29453754	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-143945730.43313953	LinearSVR(Normalizer(Nystroem(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=17), Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-141510491.67982006	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(MaxAbsScaler(input_matrix), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6900/6900 [21:31<00:00,  2.93pipeline/s]Optimization Progress: 100%|██████████| 6900/6900 [21:31<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6900/6900 [21:32<00:00,  3.44pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6901/7000 [21:33<00:28,  3.44pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6902/7000 [21:33<00:28,  3.44pipeline/s]Optimization Progress:  99%|█████████▊| 6903/7000 [21:33<00:34,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6903/7000 [21:33<00:34,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6904/7000 [21:33<00:33,  2.84pipeline/s]Optimization Progress:  99%|█████████▊| 6906/7000 [21:42<01:49,  1.16s/pipeline]Optimization Progress: 100%|█████████▉| 6986/7000 [21:46<00:11,  1.21pipeline/s]
Generation 69 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 7000/7000 [21:46<00:00,  1.21pipeline/s]Optimization Progress: 100%|██████████| 7000/7000 [21:46<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 7000/7000 [21:47<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7000/7000 [21:47<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7000/7000 [21:47<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7000/7000 [21:48<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7000/7000 [21:48<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7000/7000 [21:49<00:00,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7002/7100 [21:49<00:57,  1.71pipeline/s]Optimization Progress:  99%|█████████▊| 7003/7100 [21:49<01:08,  1.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7003/7100 [21:49<01:08,  1.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7004/7100 [21:49<01:07,  1.42pipeline/s]Optimization Progress:  99%|█████████▊| 7006/7100 [21:54<01:32,  1.02pipeline/s]Optimization Progress: 100%|█████████▉| 7086/7100 [21:56<00:09,  1.44pipeline/s]
Generation 70 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7100/7100 [21:57<00:00,  1.44pipeline/s]Optimization Progress: 100%|██████████| 7100/7100 [21:57<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [21:57<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7100/7100 [21:58<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [21:59<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [21:59<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7100/7100 [21:59<00:00,  2.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [21:59<00:00,  2.00pipeline/s]Optimization Progress:  99%|█████████▊| 7102/7200 [22:00<01:12,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7102/7200 [22:00<01:12,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7103/7200 [22:00<01:11,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7104/7200 [22:00<01:10,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7105/7200 [22:00<01:09,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7106/7200 [22:00<01:09,  1.36pipeline/s]Optimization Progress:  99%|█████████▊| 7108/7200 [22:03<01:04,  1.44pipeline/s]Optimization Progress: 100%|█████████▉| 7188/7200 [22:05<00:05,  2.02pipeline/s]
Generation 71 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7200/7200 [22:06<00:00,  2.02pipeline/s]Optimization Progress: 100%|██████████| 7200/7200 [22:06<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:08:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7200/7200 [22:07<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7200/7200 [22:07<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7200/7200 [22:08<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7200/7200 [22:08<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 7200/7200 [22:08<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7200/7200 [22:08<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 7200/7200 [22:09<00:00,  2.77pipeline/s]Optimization Progress:  99%|█████████▊| 7204/7300 [22:09<00:44,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7204/7300 [22:09<00:44,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7205/7300 [22:09<00:44,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7206/7300 [22:09<00:43,  2.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7207/7300 [22:09<00:43,  2.15pipeline/s]Optimization Progress:  99%|█████████▉| 7209/7300 [22:12<00:46,  1.94pipeline/s]Optimization Progress: 100%|█████████▉| 7289/7300 [22:15<00:04,  2.69pipeline/s]
Generation 72 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-107259969.81172939	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=10, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7300/7300 [22:16<00:00,  2.69pipeline/s]Optimization Progress: 100%|██████████| 7300/7300 [22:16<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 7300/7300 [22:16<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7300/7300 [22:16<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 7300/7300 [22:16<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 7300/7300 [22:17<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [16:08:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7300/7300 [22:17<00:00,  3.49pipeline/s]Optimization Progress:  99%|█████████▊| 7302/7400 [22:18<00:53,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7302/7400 [22:18<00:53,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7303/7400 [22:18<00:52,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7304/7400 [22:18<00:52,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7305/7400 [22:18<00:51,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7306/7400 [22:18<00:51,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7307/7400 [22:18<00:50,  1.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7308/7400 [22:18<00:50,  1.84pipeline/s]Optimization Progress:  99%|█████████▉| 7310/7400 [22:22<00:47,  1.90pipeline/s]Optimization Progress: 100%|█████████▉| 7390/7400 [22:24<00:03,  2.66pipeline/s]
Generation 73 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7400/7400 [22:24<00:00,  2.66pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7400/7400 [22:24<00:00,  2.66pipeline/s]Optimization Progress: 100%|██████████| 7400/7400 [22:24<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 7400/7400 [22:24<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7400/7400 [22:25<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 7400/7400 [22:26<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7400/7400 [22:26<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7400/7400 [22:26<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7400/7400 [22:26<00:00,  3.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7400/7400 [22:26<00:00,  3.59pipeline/s]Optimization Progress:  99%|█████████▊| 7403/7500 [22:27<00:40,  2.39pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7403/7500 [22:27<00:40,  2.39pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7404/7500 [22:27<00:40,  2.39pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7405/7500 [22:27<00:39,  2.39pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7406/7500 [22:27<00:39,  2.39pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7407/7500 [22:27<00:38,  2.39pipeline/s]Optimization Progress:  99%|█████████▉| 7409/7500 [22:30<00:44,  2.06pipeline/s]Optimization Progress: 100%|█████████▉| 7489/7500 [22:31<00:03,  2.92pipeline/s]
Generation 74 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7500/7500 [22:32<00:00,  2.92pipeline/s]Optimization Progress: 100%|██████████| 7500/7500 [22:32<00:00,  3.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 7500/7500 [22:32<00:00,  3.83pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7500/7500 [22:34<00:00,  3.83pipeline/s]Optimization Progress:  99%|█████████▊| 7503/7600 [22:34<00:36,  2.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7503/7600 [22:34<00:36,  2.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7504/7600 [22:34<00:35,  2.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7505/7600 [22:34<00:35,  2.69pipeline/s]Optimization Progress:  99%|█████████▉| 7507/7600 [22:41<01:11,  1.30pipeline/s]Optimization Progress: 100%|█████████▉| 7587/7600 [22:43<00:07,  1.83pipeline/s]
Generation 75 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 7600/7600 [22:43<00:00,  1.83pipeline/s]Optimization Progress: 100%|██████████| 7600/7600 [22:43<00:00,  2.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7600/7600 [22:44<00:00,  2.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7600/7600 [22:45<00:00,  2.59pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 7600/7600 [22:46<00:00,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7603/7700 [22:46<00:37,  2.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7604/7700 [22:46<00:37,  2.59pipeline/s]Optimization Progress:  99%|█████████▉| 7605/7700 [22:46<00:41,  2.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7605/7700 [22:46<00:41,  2.30pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7606/7700 [22:46<00:40,  2.30pipeline/s]Optimization Progress:  99%|█████████▉| 7608/7700 [22:50<01:04,  1.42pipeline/s]Optimization Progress: 100%|█████████▉| 7688/7700 [22:53<00:06,  1.98pipeline/s]
Generation 76 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [22:53<00:00,  1.98pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7700/7700 [22:55<00:00,  1.98pipeline/s]Optimization Progress: 100%|██████████| 7700/7700 [22:55<00:00,  2.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [22:55<00:00,  2.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [22:55<00:00,  2.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [22:55<00:00,  2.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7700/7700 [22:56<00:00,  2.50pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [22:57<00:00,  2.50pipeline/s]Optimization Progress:  99%|█████████▊| 7702/7800 [22:57<00:52,  1.86pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7702/7800 [22:57<00:52,  1.86pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7703/7800 [22:57<00:52,  1.86pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7704/7800 [22:57<00:51,  1.86pipeline/s]Optimization Progress:  99%|█████████▉| 7706/7800 [23:02<01:11,  1.32pipeline/s]Optimization Progress: 100%|█████████▉| 7786/7800 [23:05<00:07,  1.84pipeline/s]
Generation 77 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7800/7800 [23:05<00:00,  1.84pipeline/s]Optimization Progress: 100%|██████████| 7800/7800 [23:05<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7800/7800 [23:06<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7800/7800 [23:09<00:00,  2.58pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7801/7900 [23:09<00:38,  2.58pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7802/7900 [23:09<00:37,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 7803/7900 [23:09<00:56,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7803/7900 [23:09<00:56,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7804/7900 [23:09<00:56,  1.71pipeline/s]Optimization Progress:  99%|█████████▉| 7806/7900 [23:14<01:25,  1.10pipeline/s]Optimization Progress: 100%|█████████▉| 7886/7900 [23:19<00:09,  1.53pipeline/s]
Generation 78 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-131252753.68420318	LinearSVR(Binarizer(Normalizer(MaxAbsScaler(Nystroem(RBFSampler(MaxAbsScaler(input_matrix), RBFSampler__gamma=0.7000000000000001), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=polynomial, Nystroem__n_components=7)), Normalizer__norm=max), Binarizer__threshold=0.4), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7900/7900 [23:19<00:00,  1.53pipeline/s]Optimization Progress: 100%|██████████| 7900/7900 [23:19<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7900/7900 [23:20<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 7900/7900 [23:20<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:09:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 7900/7900 [23:22<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 7900/7900 [23:22<00:00,  2.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7900/7900 [23:22<00:00,  2.16pipeline/s]Optimization Progress:  99%|█████████▉| 7906/8000 [23:22<00:46,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7906/8000 [23:22<00:46,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7907/8000 [23:22<00:45,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7908/8000 [23:22<00:45,  2.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7909/8000 [23:22<00:44,  2.02pipeline/s]Optimization Progress:  99%|█████████▉| 7911/8000 [23:29<01:06,  1.34pipeline/s]Optimization Progress: 100%|█████████▉| 7991/8000 [23:30<00:04,  1.91pipeline/s]
Generation 79 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8000/8000 [23:30<00:00,  1.91pipeline/s]Optimization Progress: 100%|██████████| 8000/8000 [23:30<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 8000/8000 [23:30<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8000/8000 [23:31<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8000/8000 [23:32<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 8000/8000 [23:32<00:00,  2.67pipeline/s]Optimization Progress:  99%|█████████▉| 8004/8100 [23:34<00:49,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8004/8100 [23:34<00:49,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8005/8100 [23:34<00:49,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8006/8100 [23:34<00:48,  1.94pipeline/s]Optimization Progress:  99%|█████████▉| 8008/8100 [23:38<01:06,  1.38pipeline/s]Optimization Progress: 100%|█████████▉| 8088/8100 [23:42<00:06,  1.93pipeline/s]
Generation 80 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8100/8100 [23:43<00:00,  1.93pipeline/s]Optimization Progress: 100%|██████████| 8100/8100 [23:43<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8100/8100 [23:43<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8100/8100 [23:43<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8100/8100 [23:44<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [23:44<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [23:44<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8100/8100 [23:44<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [23:44<00:00,  2.53pipeline/s]Optimization Progress:  99%|█████████▉| 8104/8200 [23:45<00:43,  2.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8104/8200 [23:45<00:43,  2.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8105/8200 [23:45<00:43,  2.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8106/8200 [23:45<00:42,  2.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8107/8200 [23:45<00:42,  2.20pipeline/s]Optimization Progress:  99%|█████████▉| 8109/8200 [23:49<00:49,  1.85pipeline/s]Optimization Progress: 100%|█████████▉| 8189/8200 [23:51<00:04,  2.58pipeline/s]
Generation 81 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8200/8200 [23:51<00:00,  2.58pipeline/s]Optimization Progress: 100%|██████████| 8200/8200 [23:51<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [23:52<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [23:52<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 8200/8200 [23:53<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [23:53<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [23:54<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8200/8200 [23:54<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 8200/8200 [23:54<00:00,  3.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8200/8200 [23:54<00:00,  3.62pipeline/s]Optimization Progress:  99%|█████████▉| 8204/8300 [23:55<00:43,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8204/8300 [23:55<00:43,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8205/8300 [23:55<00:42,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8206/8300 [23:55<00:42,  2.23pipeline/s]Optimization Progress:  99%|█████████▉| 8208/8300 [24:06<01:46,  1.16s/pipeline]Optimization Progress: 100%|█████████▉| 8288/8300 [24:08<00:09,  1.22pipeline/s]
Generation 82 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8300/8300 [24:08<00:00,  1.22pipeline/s]Optimization Progress: 100%|██████████| 8300/8300 [24:08<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8300/8300 [24:09<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [24:10<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [24:11<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [24:11<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [24:12<00:00,  1.72pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [24:12<00:00,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8302/8400 [24:12<00:56,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 8303/8400 [24:12<01:14,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8303/8400 [24:12<01:14,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8304/8400 [24:12<01:13,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8305/8400 [24:12<01:12,  1.31pipeline/s]Optimization Progress:  99%|█████████▉| 8307/8400 [24:16<01:17,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 8387/8400 [24:18<00:07,  1.68pipeline/s]
Generation 83 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [24:18<00:00,  1.68pipeline/s]Optimization Progress: 100%|██████████| 8400/8400 [24:18<00:00,  2.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [24:20<00:00,  2.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8400/8400 [24:20<00:00,  2.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:10:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8400/8400 [24:21<00:00,  2.38pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8401/8500 [24:21<00:41,  2.38pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8402/8500 [24:21<00:41,  2.38pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8403/8500 [24:21<00:40,  2.38pipeline/s]Optimization Progress:  99%|█████████▉| 8404/8500 [24:21<00:49,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8404/8500 [24:21<00:49,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8405/8500 [24:21<00:48,  1.95pipeline/s]Optimization Progress:  99%|█████████▉| 8407/8500 [24:24<01:00,  1.54pipeline/s]Optimization Progress: 100%|█████████▉| 8487/8500 [24:27<00:06,  2.14pipeline/s]
Generation 84 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:11:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8500/8500 [24:27<00:00,  2.14pipeline/s]Optimization Progress: 100%|██████████| 8500/8500 [24:27<00:00,  3.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [24:28<00:00,  3.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8500/8500 [24:28<00:00,  3.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 8500/8500 [24:30<00:00,  3.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8500/8500 [24:31<00:00,  3.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 8500/8500 [24:31<00:00,  3.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8502/8600 [24:31<00:32,  3.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8503/8600 [24:31<00:31,  3.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8504/8600 [24:31<00:31,  3.04pipeline/s]Optimization Progress:  99%|█████████▉| 8506/8600 [24:35<00:57,  1.64pipeline/s]Optimization Progress: 100%|█████████▉| 8586/8600 [24:38<00:06,  2.28pipeline/s]
Generation 85 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8600/8600 [24:40<00:00,  2.28pipeline/s]Optimization Progress: 100%|██████████| 8600/8600 [24:40<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8600/8600 [24:40<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8600/8600 [24:40<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8600/8600 [24:41<00:00,  2.84pipeline/s]Optimization Progress:  99%|█████████▉| 8608/8700 [24:41<00:26,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8608/8700 [24:41<00:26,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8609/8700 [24:41<00:25,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8610/8700 [24:41<00:25,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8611/8700 [24:41<00:25,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8612/8700 [24:41<00:25,  3.52pipeline/s]Optimization Progress:  99%|█████████▉| 8614/8700 [24:45<00:32,  2.65pipeline/s]Optimization Progress: 100%|█████████▉| 8694/8700 [24:45<00:01,  3.75pipeline/s]
Generation 86 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8700/8700 [24:45<00:00,  3.75pipeline/s]Optimization Progress: 100%|██████████| 8700/8700 [24:45<00:00,  5.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [24:46<00:00,  5.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [24:48<00:00,  5.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8704/8800 [24:49<00:18,  5.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8705/8800 [24:49<00:18,  5.20pipeline/s]Optimization Progress:  99%|█████████▉| 8706/8800 [24:49<00:27,  3.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8706/8800 [24:49<00:27,  3.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8707/8800 [24:49<00:27,  3.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8708/8800 [24:49<00:26,  3.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8709/8800 [24:49<00:26,  3.42pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8710/8800 [24:49<00:26,  3.42pipeline/s]Optimization Progress:  99%|█████████▉| 8712/8800 [24:53<00:37,  2.37pipeline/s]Optimization Progress: 100%|█████████▉| 8792/8800 [24:55<00:02,  3.32pipeline/s]
Generation 87 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 8800/8800 [24:55<00:00,  3.32pipeline/s]Optimization Progress: 100%|██████████| 8800/8800 [24:55<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:11:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8800/8800 [24:55<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 8800/8800 [24:56<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8800/8800 [24:56<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8800/8800 [24:57<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:11:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8800/8800 [24:57<00:00,  4.62pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8800/8800 [24:57<00:00,  4.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8801/8900 [24:58<00:21,  4.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8802/8900 [24:58<00:21,  4.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8803/8900 [24:58<00:20,  4.62pipeline/s]Optimization Progress:  99%|█████████▉| 8804/8900 [24:58<00:36,  2.65pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 8804/8900 [24:58<00:36,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8805/8900 [24:58<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8806/8900 [24:58<00:35,  2.65pipeline/s]Optimization Progress:  99%|█████████▉| 8808/8900 [25:03<00:59,  1.55pipeline/s]Optimization Progress: 100%|█████████▉| 8888/8900 [25:05<00:05,  2.17pipeline/s]
Generation 88 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:11:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8900/8900 [25:06<00:00,  2.17pipeline/s]Optimization Progress: 100%|██████████| 8900/8900 [25:06<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [25:06<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 8900/8900 [25:06<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:11:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 8900/8900 [25:07<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8900/8900 [25:07<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 8900/8900 [25:07<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 8900/8900 [25:07<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 8900/8900 [25:08<00:00,  3.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 8900/8900 [25:08<00:00,  3.00pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8902/9000 [25:09<00:32,  3.00pipeline/s]Optimization Progress:  99%|█████████▉| 8903/9000 [25:09<00:50,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8903/9000 [25:09<00:50,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8904/9000 [25:09<00:50,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8905/9000 [25:09<00:49,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8906/9000 [25:09<00:49,  1.90pipeline/s]Optimization Progress:  99%|█████████▉| 8908/9000 [25:13<00:54,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 8988/9000 [25:14<00:05,  2.36pipeline/s]
Generation 89 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9000/9000 [25:14<00:00,  2.36pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 9000/9000 [25:14<00:00,  2.36pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9000/9000 [25:16<00:00,  2.36pipeline/s]Optimization Progress: 100%|██████████| 9000/9000 [25:16<00:00,  2.92pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 9000/9000 [25:17<00:00,  2.92pipeline/s]Optimization Progress:  99%|█████████▉| 9002/9100 [25:17<00:41,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9002/9100 [25:17<00:41,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9003/9100 [25:17<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9004/9100 [25:17<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9005/9100 [25:17<00:40,  2.37pipeline/s]Optimization Progress:  99%|█████████▉| 9007/9100 [25:21<00:51,  1.81pipeline/s]Optimization Progress: 100%|█████████▉| 9087/9100 [25:24<00:05,  2.53pipeline/s]
Generation 90 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:12:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9100/9100 [25:24<00:00,  2.53pipeline/s]Optimization Progress: 100%|██████████| 9100/9100 [25:24<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9100/9100 [25:24<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9100/9100 [25:24<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:12:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9100/9100 [25:25<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9100/9100 [25:25<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9100/9100 [25:25<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 9100/9100 [25:25<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9100/9100 [25:25<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9100/9100 [25:26<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 9100/9100 [25:27<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 9100/9100 [25:27<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 9100/9100 [25:27<00:00,  3.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:12:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9100/9100 [25:28<00:00,  3.53pipeline/s]Optimization Progress:  99%|█████████▉| 9108/9200 [25:29<00:35,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9108/9200 [25:29<00:35,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9109/9200 [25:29<00:35,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9110/9200 [25:29<00:35,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9111/9200 [25:29<00:34,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9112/9200 [25:29<00:34,  2.56pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9113/9200 [25:29<00:33,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 9114/9200 [25:40<00:33,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 9115/9200 [25:48<01:31,  1.08s/pipeline]Optimization Progress: 100%|█████████▉| 9195/9200 [25:51<00:03,  1.30pipeline/s]
Generation 91 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9200/9200 [25:51<00:00,  1.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9200/9200 [25:51<00:00,  1.30pipeline/s]Optimization Progress: 100%|██████████| 9200/9200 [25:51<00:00,  1.81pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9200/9200 [25:51<00:00,  1.81pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9200/9200 [25:51<00:00,  1.81pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:12:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9200/9200 [25:52<00:00,  1.81pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 9200/9200 [25:52<00:00,  1.81pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9200/9200 [25:54<00:00,  1.81pipeline/s]Optimization Progress:  99%|█████████▉| 9203/9300 [25:55<01:14,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9203/9300 [25:55<01:14,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9204/9300 [25:55<01:13,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9205/9300 [25:55<01:12,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9206/9300 [25:55<01:11,  1.31pipeline/s]Optimization Progress:  99%|█████████▉| 9208/9300 [25:58<01:08,  1.34pipeline/s]Optimization Progress: 100%|█████████▉| 9288/9300 [26:00<00:06,  1.89pipeline/s]
Generation 92 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-123653697.89706388	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9300/9300 [26:00<00:00,  1.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9300/9300 [26:01<00:00,  1.89pipeline/s]Optimization Progress: 100%|██████████| 9300/9300 [26:01<00:00,  2.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:12:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9300/9300 [26:02<00:00,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 9307/9400 [26:04<00:38,  2.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9307/9400 [26:04<00:38,  2.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9308/9400 [26:04<00:37,  2.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9309/9400 [26:04<00:37,  2.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9310/9400 [26:04<00:37,  2.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9311/9400 [26:04<00:36,  2.43pipeline/s]Optimization Progress:  99%|█████████▉| 9312/9400 [26:20<00:36,  2.43pipeline/s]Optimization Progress:  99%|█████████▉| 9313/9400 [26:21<01:37,  1.12s/pipeline]Optimization Progress: 100%|█████████▉| 9393/9400 [26:23<00:05,  1.26pipeline/s]
Generation 93 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [26:24<00:00,  1.26pipeline/s]Optimization Progress: 100%|██████████| 9400/9400 [26:24<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9400/9400 [26:26<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [26:27<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [26:28<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [26:28<00:00,  1.77pipeline/s]Optimization Progress:  99%|█████████▉| 9402/9500 [26:28<01:40,  1.03s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9402/9500 [26:28<01:40,  1.03s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9403/9500 [26:28<01:39,  1.03s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9404/9500 [26:28<01:38,  1.03s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9405/9500 [26:28<01:37,  1.03s/pipeline]Optimization Progress:  99%|█████████▉| 9407/9500 [26:32<01:31,  1.01pipeline/s]Optimization Progress: 100%|█████████▉| 9487/9500 [26:33<00:09,  1.44pipeline/s]
Generation 94 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9500/9500 [26:34<00:00,  1.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9500/9500 [26:34<00:00,  1.44pipeline/s]Optimization Progress: 100%|██████████| 9500/9500 [26:34<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9500/9500 [26:34<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:35<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9500/9500 [26:36<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:36<00:00,  2.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [26:37<00:00,  2.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9503/9600 [26:37<00:47,  2.03pipeline/s]Optimization Progress:  99%|█████████▉| 9504/9600 [26:37<00:59,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9504/9600 [26:37<00:59,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9505/9600 [26:37<00:58,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9506/9600 [26:37<00:58,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9507/9600 [26:37<00:57,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9508/9600 [26:37<00:57,  1.61pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9509/9600 [26:37<00:56,  1.61pipeline/s]Optimization Progress:  99%|█████████▉| 9511/9600 [26:41<00:54,  1.65pipeline/s]Optimization Progress: 100%|█████████▉| 9591/9600 [26:43<00:03,  2.31pipeline/s]
Generation 95 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9600/9600 [26:44<00:00,  2.31pipeline/s]Optimization Progress: 100%|██████████| 9600/9600 [26:44<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9600/9600 [26:44<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 9600/9600 [26:44<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9600/9600 [26:45<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 9600/9600 [26:46<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 9600/9600 [26:46<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9600/9600 [26:46<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9600/9600 [26:46<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9600/9600 [26:47<00:00,  3.03pipeline/s]Optimization Progress:  99%|█████████▉| 9606/9700 [26:47<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9606/9700 [26:47<00:35,  2.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9607/9700 [26:47<00:35,  2.65pipeline/s]Optimization Progress:  99%|█████████▉| 9609/9700 [26:52<01:12,  1.26pipeline/s]Optimization Progress: 100%|█████████▉| 9689/9700 [26:54<00:06,  1.77pipeline/s]
Generation 96 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [26:54<00:00,  1.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [26:56<00:00,  1.77pipeline/s]Optimization Progress: 100%|██████████| 9700/9700 [26:56<00:00,  2.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [26:56<00:00,  2.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [26:58<00:00,  2.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [26:58<00:00,  2.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9700/9700 [26:58<00:00,  2.30pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [26:58<00:00,  2.30pipeline/s]Optimization Progress:  99%|█████████▉| 9704/9800 [26:59<00:49,  1.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9704/9800 [26:59<00:49,  1.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9705/9800 [26:59<00:48,  1.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9706/9800 [26:59<00:48,  1.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9707/9800 [26:59<00:47,  1.96pipeline/s]Optimization Progress:  99%|█████████▉| 9709/9800 [27:03<00:58,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 9789/9800 [27:05<00:04,  2.21pipeline/s]
Generation 97 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9800/9800 [27:05<00:00,  2.21pipeline/s]Optimization Progress: 100%|██████████| 9800/9800 [27:05<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9800/9800 [27:07<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9800/9800 [27:08<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 9800/9800 [27:08<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9800/9800 [27:08<00:00,  3.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9801/9900 [27:08<00:32,  3.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9802/9900 [27:08<00:31,  3.09pipeline/s]Optimization Progress:  99%|█████████▉| 9803/9900 [27:08<00:53,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9803/9900 [27:08<00:53,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9804/9900 [27:08<00:53,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9805/9900 [27:08<00:52,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9806/9900 [27:08<00:51,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9807/9900 [27:08<00:51,  1.81pipeline/s]Optimization Progress:  99%|█████████▉| 9809/9900 [27:14<01:00,  1.49pipeline/s]Optimization Progress: 100%|█████████▉| 9889/9900 [27:18<00:05,  2.06pipeline/s]
Generation 98 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [27:19<00:00,  2.06pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [27:20<00:00,  2.06pipeline/s]Optimization Progress: 100%|██████████| 9900/9900 [27:20<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9900/9900 [27:20<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [27:20<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [16:13:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [27:20<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [27:21<00:00,  2.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 9900/9900 [27:21<00:00,  2.67pipeline/s]Optimization Progress:  99%|█████████▉| 9905/10000 [27:22<00:39,  2.41pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9905/10000 [27:22<00:39,  2.41pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9906/10000 [27:22<00:39,  2.41pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9907/10000 [27:22<00:38,  2.41pipeline/s]Optimization Progress:  99%|█████████▉| 9909/10000 [27:26<00:50,  1.82pipeline/s]Optimization Progress: 100%|█████████▉| 9989/10000 [27:29<00:04,  2.51pipeline/s]
Generation 99 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [16:14:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 10000/10000 [27:31<00:00,  2.51pipeline/s]Optimization Progress: 100%|██████████| 10000/10000 [27:31<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10000/10000 [27:31<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10000/10000 [27:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [16:14:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7fc20d830dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7fc20d941669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7fc20d94ef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7fc20d935cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7fc20d822f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fb81b5dd9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7fb81b5dd067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fb81b5f527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7fb81b5f5cb4]

.
Optimization Progress: 100%|██████████| 10000/10000 [27:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10000/10000 [27:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10000/10000 [27:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 10000/10000 [27:33<00:00,  3.07pipeline/s]Optimization Progress:  99%|█████████▉| 10003/10100 [27:34<00:50,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10003/10100 [27:34<00:50,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10004/10100 [27:34<00:50,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10005/10100 [27:34<00:49,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10006/10100 [27:34<00:49,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10007/10100 [27:34<00:48,  1.92pipeline/s]Optimization Progress:  99%|█████████▉| 10009/10100 [27:43<01:15,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 10089/10100 [27:45<00:06,  1.69pipeline/s]
Generation 100 - Current Pareto front scores:
-1	-353353219.2427436	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205195579.150488	LinearSVR(FastICA(input_matrix, FastICA__tol=0.1), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-132179816.47761555	LinearSVR(Normalizer(Nystroem(input_matrix, Nystroem__gamma=0.9, Nystroem__kernel=laplacian, Nystroem__n_components=1), Normalizer__norm=max), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-119363425.7900027	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)
-8	-102232346.29274675	LinearSVR(Normalizer(MaxAbsScaler(PolynomialFeatures(Nystroem(MaxAbsScaler(SGDRegressor(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=polynomial, Nystroem__n_components=7), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.001)                                                                                  Best pipeline:
0. StackingEstimator(estimator=XGBRegressor(base_score=0.5, booster='gbtree',
                                         colsample_bylevel=1,
                                         colsample_bynode=1, colsample_bytree=1,
                                         gamma=0, gpu_id=-1,
                                         importance_type='gain',
                                         interaction_constraints='',
                                         learning_rate=0.001, max_delta_step=0,
                                         max_depth=10, min_child_weight=18,
                                         missing=nan, monotone_constraints='()',
                                         n_estimators=100, n_jobs=1, nthread=1,
                                         num_parallel_tree=1, random_state=0,
                                         reg_alpha=0, reg_lambda=1,
                                         scale_pos_weight=1, subsample=0.05,
                                         tree_method='exact',
                                         validate_parameters=1,
                                         verbosity=None))
1. StackingEstimator(estimator=SGDRegressor(alpha=0.0, eta0=1.0, l1_ratio=1.0,
                                         loss='huber', penalty='elasticnet',
                                         power_t=0.0))
2. MaxAbsScaler()
3. Nystroem(gamma=0.30000000000000004, kernel='polynomial', n_components=7)
4. PolynomialFeatures(include_bias=False)
5. MaxAbsScaler()
6. Normalizer(norm='max')
7. LinearSVR(C=5.0, epsilon=1.0, tol=0.001)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
