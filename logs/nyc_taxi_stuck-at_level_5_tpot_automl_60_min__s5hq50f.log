30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   5%|▌         | 5/100 [00:07<02:21,  1.49s/pipeline]Optimization Progress:  85%|████████▌ | 85/100 [00:11<00:15,  1.06s/pipeline]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 100/100 [00:16<00:00,  1.06s/pipeline]Optimization Progress: 100%|██████████| 100/100 [00:16<00:00,  1.19pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 100/100 [00:16<00:00,  1.19pipeline/s]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:17<00:00,  1.19pipeline/s]Optimization Progress:  52%|█████▎    | 105/200 [00:18<01:04,  1.46pipeline/s]                                                                              Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  52%|█████▎    | 105/200 [00:18<01:04,  1.46pipeline/s]Optimization Progress:  54%|█████▎    | 107/200 [00:28<03:16,  2.11s/pipeline]Optimization Progress:  94%|█████████▎| 187/200 [01:09<00:21,  1.63s/pipeline]
Generation 1 - Current Pareto front scores:
-1	-217796018.1985159	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=15, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:09<00:00,  1.63s/pipeline]Optimization Progress: 100%|██████████| 200/200 [01:09<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 200/200 [01:10<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:10<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [01:12<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:12<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 200/200 [01:13<00:00,  1.15s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 200/200 [01:15<00:00,  1.15s/pipeline]Optimization Progress:  68%|██████▊   | 203/300 [01:17<02:38,  1.63s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  68%|██████▊   | 203/300 [01:17<02:38,  1.63s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  68%|██████▊   | 204/300 [01:17<02:36,  1.63s/pipeline]Optimization Progress:  69%|██████▊   | 206/300 [01:58<08:09,  5.20s/pipeline]Optimization Progress:  95%|█████████▌| 286/300 [02:02<00:51,  3.66s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [02:03<00:00,  3.66s/pipeline]Optimization Progress: 100%|██████████| 300/300 [02:03<00:00,  2.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 300/300 [02:07<00:00,  2.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [02:07<00:00,  2.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 300/300 [02:09<00:00,  2.58s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 300/300 [02:10<00:00,  2.58s/pipeline]Optimization Progress:  76%|███████▌  | 303/400 [02:11<04:06,  2.54s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▌  | 303/400 [02:11<04:06,  2.54s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▌  | 304/400 [02:11<04:04,  2.54s/pipeline]Optimization Progress:  76%|███████▋  | 306/400 [02:19<04:04,  2.60s/pipeline]Optimization Progress:  96%|█████████▋| 386/400 [02:23<00:25,  1.84s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 400/400 [02:24<00:00,  1.84s/pipeline]Optimization Progress: 100%|██████████| 400/400 [02:24<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:24<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 400/400 [02:26<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:27<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:27<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [20:22:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 400/400 [02:30<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:30<00:00,  1.30s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 400/400 [02:31<00:00,  1.30s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:32<02:43,  1.69s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:50<02:43,  1.69s/pipeline]Optimization Progress:  81%|████████  | 404/500 [03:03<16:56, 10.59s/pipeline]Optimization Progress:  97%|█████████▋| 484/500 [03:05<01:58,  7.42s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [03:13<00:00,  7.42s/pipeline]Optimization Progress: 100%|██████████| 500/500 [03:13<00:00,  5.33s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [20:23:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 500/500 [03:13<00:00,  5.33s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [03:14<06:21,  3.89s/pipeline]Optimization Progress:  84%|████████▍ | 503/600 [03:32<13:08,  8.13s/pipeline]Optimization Progress:  97%|█████████▋| 583/600 [03:37<01:37,  5.71s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 600/600 [03:38<00:00,  5.71s/pipeline]Optimization Progress: 100%|██████████| 600/600 [03:38<00:00,  4.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [03:43<00:00,  4.01s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [03:44<00:00,  4.01s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [03:45<06:13,  3.81s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▌ | 602/700 [03:45<06:13,  3.81s/pipeline]Optimization Progress:  86%|████████▋ | 604/700 [04:31<15:32,  9.71s/pipeline]Optimization Progress:  98%|█████████▊| 684/700 [04:50<01:49,  6.86s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 700/700 [04:51<00:00,  6.86s/pipeline]Optimization Progress: 100%|██████████| 700/700 [04:51<00:00,  4.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [04:52<00:00,  4.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 700/700 [04:53<00:00,  4.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 700/700 [04:56<00:00,  4.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [04:56<00:00,  4.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [04:58<00:00,  4.82s/pipeline]Optimization Progress:  88%|████████▊ | 701/800 [05:11<15:51,  9.61s/pipeline]Optimization Progress:  98%|█████████▊| 781/800 [05:18<02:08,  6.75s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-207432682.12920508	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-190683650.43690383	GradientBoostingRegressor(PolynomialFeatures(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=16, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingRegressor__alpha=0.8, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=15, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.5)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [05:21<00:00,  6.75s/pipeline]Optimization Progress: 100%|██████████| 800/800 [05:21<00:00,  4.78s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [05:21<00:00,  4.78s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [05:23<00:00,  4.78s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 800/800 [05:23<00:00,  4.78s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [05:24<00:00,  4.78s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [05:25<06:24,  3.93s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  89%|████████▉ | 802/900 [05:25<06:24,  3.93s/pipeline]Optimization Progress:  89%|████████▉ | 804/900 [05:36<07:06,  4.45s/pipeline]Optimization Progress:  98%|█████████▊| 884/900 [05:41<00:50,  3.13s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-202700384.26076278	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [05:42<00:00,  3.13s/pipeline]Optimization Progress: 100%|██████████| 900/900 [05:42<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [05:42<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [05:44<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 900/900 [05:45<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 900/900 [05:49<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 900/900 [05:49<00:00,  2.22s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 900/900 [05:50<00:00,  2.22s/pipeline]Optimization Progress:  90%|█████████ | 902/1000 [05:50<04:23,  2.69s/pipeline]Optimization Progress:  90%|█████████ | 903/1000 [06:14<14:45,  9.13s/pipeline]Optimization Progress:  98%|█████████▊| 983/1000 [06:25<01:49,  6.43s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-202700384.26076278	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [06:26<00:00,  6.43s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [06:26<00:00,  4.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 1000/1000 [06:31<00:00,  4.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:26:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [06:34<00:00,  4.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [20:26:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [06:34<00:00,  4.52s/pipeline]Optimization Progress:  91%|█████████ | 1000/1100 [06:40<07:31,  4.52s/pipeline]Optimization Progress:  91%|█████████ | 1001/1100 [07:26<35:11, 21.33s/pipeline]Optimization Progress:  98%|█████████▊| 1081/1100 [07:35<04:44, 14.96s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 1100/1100 [07:35<00:00, 14.96s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [07:35<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1100/1100 [07:37<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [07:37<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 1100/1100 [07:38<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [07:40<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [07:40<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [07:40<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [07:41<00:00, 10.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [07:43<00:00, 10.48s/pipeline]Optimization Progress:  92%|█████████▏| 1100/1200 [07:50<17:27, 10.48s/pipeline]Optimization Progress:  92%|█████████▏| 1101/1200 [07:50<19:27, 11.79s/pipeline]Optimization Progress:  98%|█████████▊| 1181/1200 [07:55<02:37,  8.27s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [07:55<00:00,  8.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [07:55<00:00,  8.27s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [07:55<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [07:58<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 1200/1200 [08:02<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [08:02<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 1200/1200 [08:03<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [08:03<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [08:04<00:00,  5.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [08:05<00:00,  5.79s/pipeline]Optimization Progress:  92%|█████████▏| 1200/1300 [08:10<09:39,  5.79s/pipeline]Optimization Progress:  92%|█████████▏| 1201/1300 [08:21<19:28, 11.80s/pipeline]Optimization Progress:  99%|█████████▊| 1281/1300 [08:32<02:37,  8.30s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1300/1300 [08:32<00:00,  8.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:28:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1300/1300 [08:34<00:00,  8.30s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [08:34<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1300/1300 [08:36<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [20:28:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1300/1300 [08:36<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1300/1300 [08:38<00:00,  5.86s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [08:42<08:27,  5.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1302/1400 [08:42<08:27,  5.17s/pipeline]Optimization Progress:  93%|█████████▎| 1304/1400 [08:57<09:31,  5.96s/pipeline]Optimization Progress:  99%|█████████▉| 1384/1400 [09:08<01:07,  4.21s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 1400/1400 [09:10<00:00,  4.21s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [09:10<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1400/1400 [09:12<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1400/1400 [09:12<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [09:13<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [09:14<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:29:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1400/1400 [09:18<00:00,  2.98s/pipeline]Optimization Progress:  93%|█████████▎| 1401/1500 [09:47<22:00, 13.33s/pipeline]Optimization Progress:  99%|█████████▊| 1481/1500 [09:54<02:57,  9.36s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:30:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [09:56<00:00,  9.36s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [09:56<00:00,  6.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1500/1500 [09:56<00:00,  6.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1500/1500 [09:57<00:00,  6.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:30:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [10:04<00:00,  6.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [10:04<00:00,  6.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1500/1500 [10:05<00:00,  6.58s/pipeline]Optimization Progress:  94%|█████████▍| 1501/1600 [10:20<19:22, 11.74s/pipeline]Optimization Progress:  99%|█████████▉| 1581/1600 [10:26<02:36,  8.24s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 100%|██████████| 1600/1600 [10:27<00:00,  8.24s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [10:27<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:30:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:28<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [10:30<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1600/1600 [10:31<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:30:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:36<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [20:30:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:36<00:00,  5.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 [20:30:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [10:36<00:00,  5.78s/pipeline]Optimization Progress:  94%|█████████▍| 1600/1700 [10:40<09:37,  5.78s/pipeline]Optimization Progress:  94%|█████████▍| 1601/1700 [10:58<22:18, 13.52s/pipeline]Optimization Progress:  99%|█████████▉| 1681/1700 [11:08<03:00,  9.50s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:31:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:09<00:00,  9.50s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [11:09<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:31:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:11<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:31:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:11<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 1700/1700 [11:11<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [11:11<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 1700/1700 [11:12<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [11:12<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1700/1700 [11:15<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [11:16<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1700/1700 [11:16<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:31:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:17<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 1700/1700 [11:17<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..
Optimization Progress: 100%|██████████| 1700/1700 [11:17<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [11:18<00:00,  6.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [11:19<00:00,  6.66s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [11:20<00:00,  6.66s/pipeline]Optimization Progress:  94%|█████████▍| 1701/1800 [11:20<13:12,  8.00s/pipeline]Optimization Progress:  95%|█████████▍| 1702/1800 [11:33<15:47,  9.67s/pipeline]Optimization Progress:  99%|█████████▉| 1782/1800 [11:44<02:02,  6.81s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:32:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [11:44<00:00,  6.81s/pipeline]Optimization Progress: 100%|██████████| 1800/1800 [11:44<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:32:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [11:44<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 1800/1800 [11:45<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1800/1800 [11:46<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [11:46<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [11:48<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [11:48<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [11:50<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 1800/1800 [11:51<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1800/1800 [11:52<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [11:53<00:00,  4.77s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [11:54<00:00,  4.77s/pipeline]Optimization Progress:  95%|█████████▍| 1801/1900 [12:00<07:52,  4.77s/pipeline]Optimization Progress:  95%|█████████▍| 1802/1900 [12:06<10:40,  6.53s/pipeline]Optimization Progress:  99%|█████████▉| 1882/1900 [12:12<01:22,  4.60s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [12:14<00:00,  4.60s/pipeline]Optimization Progress: 100%|██████████| 1900/1900 [12:14<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [12:14<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:32:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1900/1900 [12:14<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [12:16<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [12:16<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [12:16<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [12:17<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [12:18<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [12:19<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [12:19<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:32:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 1900/1900 [12:20<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1900/1900 [12:22<00:00,  3.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [12:22<00:00,  3.24s/pipeline]Optimization Progress:  95%|█████████▌| 1901/2000 [12:23<08:22,  5.08s/pipeline]Optimization Progress:  95%|█████████▌| 1902/2000 [13:12<29:47, 18.24s/pipeline]Optimization Progress:  99%|█████████▉| 1982/2000 [13:23<03:50, 12.81s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2000/2000 [13:24<00:00, 12.81s/pipeline]Optimization Progress: 100%|██████████| 2000/2000 [13:24<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:33:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 2000/2000 [13:26<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [13:27<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 2000/2000 [13:27<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 2000/2000 [13:29<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 2000/2000 [13:31<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2000/2000 [13:31<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2000/2000 [13:32<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2000/2000 [13:32<00:00,  8.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 2000/2000 [13:33<00:00,  8.99s/pipeline]Optimization Progress:  95%|█████████▌| 2002/2100 [13:33<12:24,  7.60s/pipeline]Optimization Progress:  95%|█████████▌| 2003/2100 [13:42<13:06,  8.10s/pipeline]Optimization Progress:  99%|█████████▉| 2083/2100 [14:01<01:37,  5.74s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [14:02<00:00,  5.74s/pipeline]Optimization Progress: 100%|██████████| 2100/2100 [14:02<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2100/2100 [14:05<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 2100/2100 [14:08<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:34:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 2100/2100 [14:09<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2100/2100 [14:09<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2100/2100 [14:09<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:34:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 2100/2100 [14:09<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [14:10<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [14:10<00:00,  4.04s/pipeline]Optimization Progress:  96%|█████████▌| 2101/2200 [14:10<08:30,  5.16s/pipeline]Optimization Progress:  96%|█████████▌| 2102/2200 [14:17<09:26,  5.78s/pipeline]Optimization Progress:  99%|█████████▉| 2182/2200 [14:20<01:13,  4.06s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [14:21<00:00,  4.06s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [14:21<00:00,  2.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [14:21<00:00,  2.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [14:22<00:00,  2.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [14:22<00:00,  2.86s/pipeline]Optimization Progress:  96%|█████████▌| 2202/2300 [14:39<07:40,  4.70s/pipeline]Optimization Progress:  99%|█████████▉| 2282/2300 [15:01<01:00,  3.37s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [15:01<00:00,  3.37s/pipeline]Optimization Progress: 100%|██████████| 2300/2300 [15:01<00:00,  2.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [15:02<00:00,  2.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [15:03<00:00,  2.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2300/2300 [15:07<00:00,  2.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [15:10<00:00,  2.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 2300/2300 [15:10<00:00,  2.36s/pipeline]Optimization Progress:  96%|█████████▌| 2302/2400 [15:20<03:51,  2.36s/pipeline]Optimization Progress:  96%|█████████▌| 2303/2400 [15:23<06:11,  3.83s/pipeline]Optimization Progress:  99%|█████████▉| 2383/2400 [15:31<00:46,  2.71s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  2.71s/pipeline]Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:32<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:32<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 2400/2400 [15:32<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:34<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:35<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.
Optimization Progress: 100%|██████████| 2400/2400 [15:36<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:37<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:37<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:38<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:40<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [15:40<00:00,  1.90s/pipeline]Optimization Progress:  96%|█████████▌| 2401/2500 [15:50<03:08,  1.90s/pipeline]Optimization Progress:  96%|█████████▌| 2402/2500 [15:50<06:45,  4.14s/pipeline]Optimization Progress:  99%|█████████▉| 2482/2500 [15:56<00:52,  2.92s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-191544606.28213397	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.92s/pipeline]Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:57<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 2500/2500 [15:58<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 2500/2500 [15:59<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [16:03<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [16:03<00:00,  2.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [16:04<00:00,  2.06s/pipeline]Optimization Progress:  96%|█████████▌| 2502/2600 [16:05<04:11,  2.57s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2502/2600 [16:05<04:11,  2.57s/pipeline]Optimization Progress:  96%|█████████▋| 2504/2600 [16:12<04:43,  2.95s/pipeline]Optimization Progress:  99%|█████████▉| 2584/2600 [16:15<00:33,  2.08s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-191303228.2713902	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:36:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [16:16<00:00,  2.08s/pipeline]Optimization Progress: 100%|██████████| 2600/2600 [16:16<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [16:19<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 2600/2600 [16:20<00:00,  1.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [16:21<00:00,  1.46s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2604/2700 [16:24<02:19,  1.46s/pipeline]Optimization Progress:  96%|█████████▋| 2605/2700 [16:24<02:27,  1.55s/pipeline]Optimization Progress:  97%|█████████▋| 2608/2700 [16:33<03:02,  1.98s/pipeline]Optimization Progress:  99%|█████████▉| 2686/2700 [16:37<00:19,  1.40s/pipeline]
Generation 26 - Current Pareto front scores:
-1	-191303228.2713902	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:38<00:00,  1.40s/pipeline]Optimization Progress: 100%|██████████| 2700/2700 [16:38<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [16:40<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [16:41<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:41<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [16:41<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2700/2700 [16:41<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:41<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2700/2700 [16:43<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 2700/2700 [16:44<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:44<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:44<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:45<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:47<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:47<00:00,  1.00pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:47<00:00,  1.00pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2702/2800 [16:47<01:37,  1.00pipeline/s]Optimization Progress:  97%|█████████▋| 2703/2800 [16:47<02:36,  1.61s/pipeline]Optimization Progress:  97%|█████████▋| 2705/2800 [16:54<03:31,  2.23s/pipeline]Optimization Progress:  99%|█████████▉| 2784/2800 [16:58<00:25,  1.57s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-191303228.2713902	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:00<00:00,  1.57s/pipeline]Optimization Progress: 100%|██████████| 2800/2800 [17:00<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:03<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:03<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [17:03<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 2800/2800 [17:05<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:05<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:07<00:00,  1.14s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:07<00:00,  1.14s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2801/2900 [17:07<01:53,  1.14s/pipeline]Optimization Progress:  97%|█████████▋| 2802/2900 [17:07<03:00,  1.85s/pipeline]Optimization Progress:  97%|█████████▋| 2803/2900 [17:18<07:25,  4.59s/pipeline]Optimization Progress:  99%|█████████▉| 2883/2900 [17:21<00:54,  3.23s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-191303228.2713902	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 2900/2900 [17:22<00:00,  3.23s/pipeline]Optimization Progress: 100%|██████████| 2900/2900 [17:22<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2900/2900 [17:22<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:22<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:25<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [17:27<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2900/2900 [17:29<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:30<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:30<00:00,  2.27s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2900/3000 [17:31<03:47,  2.27s/pipeline]Optimization Progress:  97%|█████████▋| 2901/3000 [17:40<03:44,  2.27s/pipeline]Optimization Progress:  97%|█████████▋| 2902/3000 [17:41<07:25,  4.55s/pipeline]Optimization Progress:  99%|█████████▉| 2982/3000 [17:51<00:57,  3.22s/pipeline]
Generation 29 - Current Pareto front scores:
-1	-191303228.2713902	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 3000/3000 [17:52<00:00,  3.22s/pipeline]Optimization Progress: 100%|██████████| 3000/3000 [17:52<00:00,  2.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3000/3000 [17:56<00:00,  2.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [17:59<00:00,  2.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [17:59<00:00,  2.26s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3000/3100 [18:01<03:46,  2.26s/pipeline]Optimization Progress:  97%|█████████▋| 3001/3100 [18:10<03:43,  2.26s/pipeline]Optimization Progress:  97%|█████████▋| 3002/3100 [18:23<10:16,  6.29s/pipeline]Optimization Progress:  99%|█████████▉| 3082/3100 [18:26<01:19,  4.41s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [18:28<00:00,  4.41s/pipeline]Optimization Progress: 100%|██████████| 3100/3100 [18:28<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [18:28<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [18:29<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 3100/3100 [18:29<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [18:31<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [18:31<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [18:31<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [18:31<00:00,  3.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [18:31<00:00,  3.11s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3100/3200 [18:35<05:11,  3.11s/pipeline]Optimization Progress:  97%|█████████▋| 3101/3200 [18:35<07:14,  4.39s/pipeline]Optimization Progress:  97%|█████████▋| 3102/3200 [18:45<09:54,  6.06s/pipeline]Optimization Progress:  99%|█████████▉| 3182/3200 [18:48<01:16,  4.25s/pipeline]
Generation 31 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [18:48<00:00,  4.25s/pipeline]Optimization Progress: 100%|██████████| 3200/3200 [18:48<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 3200/3200 [18:48<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 3200/3200 [18:51<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [18:51<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [18:52<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3200/3200 [18:52<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 3200/3200 [18:52<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3200/3200 [18:53<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 3200/3200 [18:53<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 3200/3200 [18:53<00:00,  2.98s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 3200/3200 [18:56<00:00,  2.98s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3202/3300 [18:57<04:52,  2.98s/pipeline]Optimization Progress:  97%|█████████▋| 3203/3300 [18:57<04:46,  2.95s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3203/3300 [18:57<04:46,  2.95s/pipeline]Optimization Progress:  97%|█████████▋| 3204/3300 [19:10<04:43,  2.95s/pipeline]Optimization Progress:  97%|█████████▋| 3205/3300 [19:13<07:17,  4.60s/pipeline]Optimization Progress: 100%|█████████▉| 3285/3300 [19:19<00:48,  3.24s/pipeline]
Generation 32 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [19:19<00:00,  3.24s/pipeline]Optimization Progress: 100%|██████████| 3300/3300 [19:19<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [19:20<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [19:20<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 3300/3300 [19:22<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [19:23<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3300/3300 [19:23<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 3300/3300 [19:23<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [19:23<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [19:28<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [19:28<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [19:28<00:00,  2.27s/pipeline]Optimization Progress:  97%|█████████▋| 3301/3400 [19:30<03:44,  2.27s/pipeline]Optimization Progress:  97%|█████████▋| 3302/3400 [19:41<07:59,  4.89s/pipeline]Optimization Progress:  99%|█████████▉| 3382/3400 [19:44<01:01,  3.44s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [19:45<00:00,  3.44s/pipeline]Optimization Progress: 100%|██████████| 3400/3400 [19:45<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3400/3400 [19:45<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [19:50<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [19:51<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [19:51<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [19:52<00:00,  2.42s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [19:52<00:00,  2.42s/pipeline]Optimization Progress:  97%|█████████▋| 3403/3500 [19:53<03:59,  2.47s/pipeline]Optimization Progress:  97%|█████████▋| 3404/3500 [20:07<09:44,  6.09s/pipeline]Optimization Progress: 100%|█████████▉| 3484/3500 [20:09<01:08,  4.27s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [20:12<00:00,  4.27s/pipeline]Optimization Progress: 100%|██████████| 3500/3500 [20:12<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:40:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 3500/3500 [20:13<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [20:13<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [20:15<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [20:17<00:00,  3.04s/pipeline]Optimization Progress:  97%|█████████▋| 3502/3600 [20:24<06:19,  3.88s/pipeline]Optimization Progress: 100%|█████████▉| 3582/3600 [20:33<00:49,  2.75s/pipeline]
Generation 35 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-5	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [20:34<00:00,  2.75s/pipeline]Optimization Progress: 100%|██████████| 3600/3600 [20:34<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 3600/3600 [20:39<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3600/3600 [20:41<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3600/3600 [20:42<00:00,  1.95s/pipeline]Optimization Progress:  97%|█████████▋| 3601/3700 [20:43<06:33,  3.98s/pipeline]Optimization Progress:  97%|█████████▋| 3602/3700 [20:54<09:58,  6.11s/pipeline]Optimization Progress: 100%|█████████▉| 3682/3700 [20:58<01:17,  4.29s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.
Optimization Progress: 100%|██████████| 3700/3700 [21:01<00:00,  4.29s/pipeline]Optimization Progress: 100%|██████████| 3700/3700 [21:01<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 3700/3700 [21:03<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 3700/3700 [21:05<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 3700/3700 [21:05<00:00,  3.05s/pipeline]Optimization Progress:  97%|█████████▋| 3702/3800 [21:08<05:04,  3.11s/pipeline]Optimization Progress:  97%|█████████▋| 3703/3800 [21:18<08:29,  5.26s/pipeline]Optimization Progress: 100%|█████████▉| 3783/3800 [21:22<01:02,  3.69s/pipeline]
Generation 37 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3800/3800 [21:22<00:00,  3.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3800/3800 [21:22<00:00,  3.69s/pipeline]Optimization Progress: 100%|██████████| 3800/3800 [21:22<00:00,  2.59s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3800/3800 [21:23<00:00,  2.59s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3800/3800 [21:25<00:00,  2.59s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3800/3800 [21:31<00:00,  2.59s/pipeline]Optimization Progress:  97%|█████████▋| 3802/3900 [21:32<05:19,  3.26s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3802/3900 [21:32<05:19,  3.26s/pipeline]Optimization Progress:  98%|█████████▊| 3804/3900 [21:44<06:29,  4.06s/pipeline]Optimization Progress: 100%|█████████▉| 3884/3900 [21:50<00:45,  2.87s/pipeline]
Generation 38 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.87s/pipeline]Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress: 100%|██████████| 3900/3900 [21:51<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 3900/3900 [21:52<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [21:55<00:00,  2.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 3900/3900 [21:56<00:00,  2.02s/pipeline]Optimization Progress:  98%|█████████▊| 3903/4000 [21:59<03:33,  2.20s/pipeline]Optimization Progress:  98%|█████████▊| 3904/4000 [22:09<07:14,  4.53s/pipeline]Optimization Progress: 100%|█████████▉| 3984/4000 [22:13<00:50,  3.18s/pipeline]
Generation 39 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4000/4000 [22:13<00:00,  3.18s/pipeline]Optimization Progress: 100%|██████████| 4000/4000 [22:13<00:00,  2.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 4000/4000 [22:16<00:00,  2.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4000/4000 [22:16<00:00,  2.24s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4000/4100 [22:23<03:43,  2.24s/pipeline]Optimization Progress:  98%|█████████▊| 4003/4100 [22:30<05:09,  3.19s/pipeline]Optimization Progress: 100%|█████████▉| 4082/4100 [22:35<00:40,  2.25s/pipeline]
Generation 40 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [22:35<00:00,  2.25s/pipeline]Optimization Progress: 100%|██████████| 4100/4100 [22:35<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [22:35<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [22:36<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [22:37<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 4100/4100 [22:39<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4100/4100 [22:43<00:00,  1.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [22:45<00:00,  1.58s/pipeline]Optimization Progress:  98%|█████████▊| 4102/4200 [22:50<02:34,  1.58s/pipeline]Optimization Progress:  98%|█████████▊| 4103/4200 [22:59<05:41,  3.52s/pipeline]Optimization Progress: 100%|█████████▉| 4183/4200 [23:04<00:42,  2.48s/pipeline]
Generation 41 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [23:05<00:00,  2.48s/pipeline]Optimization Progress: 100%|██████████| 4200/4200 [23:05<00:00,  1.75s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4200/4200 [23:07<00:00,  1.75s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [23:07<00:00,  1.75s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 4200/4200 [23:07<00:00,  1.75s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [23:10<00:00,  1.75s/pipeline]Optimization Progress:  98%|█████████▊| 4201/4300 [23:30<14:41,  8.90s/pipeline]Optimization Progress: 100%|█████████▉| 4281/4300 [23:35<01:58,  6.25s/pipeline]
Generation 42 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 4300/4300 [23:36<00:00,  6.25s/pipeline]Optimization Progress: 100%|██████████| 4300/4300 [23:36<00:00,  4.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [23:38<00:00,  4.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4300/4300 [23:39<00:00,  4.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 4300/4300 [23:39<00:00,  4.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4300/4300 [23:43<00:00,  4.40s/pipeline]Optimization Progress:  98%|█████████▊| 4301/4400 [23:45<09:24,  5.70s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4301/4400 [23:45<09:24,  5.70s/pipeline]Optimization Progress:  98%|█████████▊| 4303/4400 [23:58<09:42,  6.00s/pipeline]Optimization Progress: 100%|█████████▉| 4383/4400 [24:02<01:11,  4.22s/pipeline]
Generation 43 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 4400/4400 [24:02<00:00,  4.22s/pipeline]Optimization Progress: 100%|██████████| 4400/4400 [24:02<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [24:04<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 4400/4400 [24:05<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4400/4400 [24:05<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 4400/4400 [24:06<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [24:06<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 4400/4400 [24:07<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4400/4400 [24:11<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4400/4400 [24:12<00:00,  2.95s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4402/4500 [24:12<04:49,  2.95s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4403/4500 [24:12<04:46,  2.95s/pipeline]Optimization Progress:  98%|█████████▊| 4404/4500 [24:20<04:43,  2.95s/pipeline]Optimization Progress:  98%|█████████▊| 4405/4500 [25:02<08:55,  5.64s/pipeline]Optimization Progress: 100%|█████████▉| 4485/4500 [25:11<00:59,  3.98s/pipeline]
Generation 44 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 4500/4500 [25:12<00:00,  3.98s/pipeline]Optimization Progress: 100%|██████████| 4500/4500 [25:12<00:00,  2.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4500/4500 [25:12<00:00,  2.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4500/4500 [25:13<00:00,  2.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 4500/4500 [25:16<00:00,  2.79s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4502/4600 [25:23<04:33,  2.79s/pipeline]Optimization Progress:  98%|█████████▊| 4503/4600 [25:23<05:02,  3.11s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4503/4600 [25:23<05:02,  3.11s/pipeline]Optimization Progress:  98%|█████████▊| 4505/4600 [25:35<06:15,  3.95s/pipeline]Optimization Progress: 100%|█████████▉| 4585/4600 [25:40<00:41,  2.78s/pipeline]
Generation 45 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [25:40<00:00,  2.78s/pipeline]Optimization Progress: 100%|██████████| 4600/4600 [25:40<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4600/4600 [25:41<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4600/4600 [25:41<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:46:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 4600/4600 [25:44<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 4600/4600 [25:45<00:00,  1.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [25:46<00:00,  1.95s/pipeline]Optimization Progress:  98%|█████████▊| 4605/4700 [25:56<03:45,  2.37s/pipeline]Optimization Progress: 100%|█████████▉| 4683/4700 [26:01<00:28,  1.68s/pipeline]
Generation 46 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-153846300.72077543	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4700/4700 [26:02<00:00,  1.68s/pipeline]Optimization Progress: 100%|██████████| 4700/4700 [26:02<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4700/4700 [26:02<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 4700/4700 [26:04<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4700/4700 [26:08<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [26:08<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4700/4700 [26:09<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [26:09<00:00,  1.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4700/4700 [26:10<00:00,  1.18s/pipeline]Optimization Progress:  98%|█████████▊| 4701/4800 [26:20<01:56,  1.18s/pipeline]Optimization Progress:  98%|█████████▊| 4702/4800 [26:21<06:07,  3.75s/pipeline]Optimization Progress: 100%|█████████▉| 4782/4800 [26:25<00:47,  2.64s/pipeline]
Generation 47 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4800/4800 [26:35<00:00,  2.64s/pipeline]Optimization Progress: 100%|██████████| 4800/4800 [26:35<00:00,  2.00s/pipeline]Optimization Progress:  98%|█████████▊| 4801/4900 [26:36<02:51,  1.73s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4801/4900 [26:36<02:51,  1.73s/pipeline]Optimization Progress:  98%|█████████▊| 4803/4900 [26:47<04:37,  2.86s/pipeline]Optimization Progress: 100%|█████████▉| 4883/4900 [26:50<00:34,  2.02s/pipeline]
Generation 48 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 4900/4900 [26:54<00:00,  2.02s/pipeline]Optimization Progress: 100%|██████████| 4900/4900 [26:54<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [26:55<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [26:56<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4900/4900 [26:56<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [26:58<00:00,  1.47s/pipeline]Optimization Progress:  98%|█████████▊| 4901/5000 [27:00<04:52,  2.96s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4901/5000 [27:00<04:52,  2.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4902/5000 [27:00<04:49,  2.96s/pipeline]Optimization Progress:  98%|█████████▊| 4904/5000 [27:08<04:32,  2.84s/pipeline]Optimization Progress: 100%|█████████▉| 4984/5000 [27:18<00:32,  2.03s/pipeline]
Generation 49 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5000/5000 [27:18<00:00,  2.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:47:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 5000/5000 [27:19<00:00,  2.03s/pipeline]Optimization Progress: 100%|██████████| 5000/5000 [27:19<00:00,  1.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 5000/5000 [27:21<00:00,  1.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5000/5000 [27:22<00:00,  1.44s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 5000/5000 [27:22<00:00,  1.44s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5000/5100 [27:29<02:24,  1.44s/pipeline]Optimization Progress:  98%|█████████▊| 5001/5100 [27:29<06:17,  3.81s/pipeline]Optimization Progress:  98%|█████████▊| 5002/5100 [27:37<08:22,  5.13s/pipeline]Optimization Progress: 100%|█████████▉| 5082/5100 [27:53<01:05,  3.65s/pipeline]
Generation 50 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 5100/5100 [27:54<00:00,  3.65s/pipeline]Optimization Progress: 100%|██████████| 5100/5100 [27:54<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:48:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 5100/5100 [27:55<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [27:56<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5100/5100 [27:58<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5100/5100 [28:00<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 5100/5100 [28:00<00:00,  2.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [28:00<00:00,  2.58s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5100/5200 [28:03<04:18,  2.58s/pipeline]Optimization Progress:  98%|█████████▊| 5101/5200 [28:03<07:04,  4.29s/pipeline]Optimization Progress:  98%|█████████▊| 5102/5200 [28:11<09:02,  5.53s/pipeline]Optimization Progress: 100%|█████████▉| 5182/5200 [28:14<01:09,  3.89s/pipeline]
Generation 51 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5200/5200 [28:20<00:00,  3.89s/pipeline]Optimization Progress: 100%|██████████| 5200/5200 [28:20<00:00,  2.81s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5200/5200 [28:22<00:00,  2.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5200/5200 [28:22<00:00,  2.81s/pipeline]Optimization Progress:  98%|█████████▊| 5202/5300 [28:23<04:05,  2.50s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5202/5300 [28:23<04:05,  2.50s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5203/5300 [28:23<04:02,  2.50s/pipeline]Optimization Progress:  98%|█████████▊| 5205/5300 [28:35<04:35,  2.90s/pipeline]Optimization Progress: 100%|█████████▉| 5285/5300 [28:40<00:30,  2.05s/pipeline]
Generation 52 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5300/5300 [28:41<00:00,  2.05s/pipeline]Optimization Progress: 100%|██████████| 5300/5300 [28:41<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:49:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 100%|██████████| 5300/5300 [28:47<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 5300/5300 [28:47<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5300/5300 [28:47<00:00,  1.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5300/5300 [28:47<00:00,  1.45s/pipeline]Optimization Progress:  98%|█████████▊| 5301/5400 [28:50<05:46,  3.50s/pipeline]Optimization Progress:  98%|█████████▊| 5302/5400 [28:56<07:16,  4.45s/pipeline]Optimization Progress: 100%|█████████▉| 5382/5400 [29:03<00:56,  3.14s/pipeline]
Generation 53 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5400/5400 [29:07<00:00,  3.14s/pipeline]Optimization Progress: 100%|██████████| 5400/5400 [29:07<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5400/5400 [29:08<00:00,  2.27s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5400/5500 [29:12<03:47,  2.27s/pipeline]Optimization Progress:  98%|█████████▊| 5401/5500 [29:12<05:04,  3.08s/pipeline]Optimization Progress:  98%|█████████▊| 5402/5500 [29:24<09:05,  5.56s/pipeline]Optimization Progress: 100%|█████████▉| 5482/5500 [29:29<01:10,  3.92s/pipeline]
Generation 54 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5500/5500 [29:31<00:00,  3.92s/pipeline]Optimization Progress: 100%|██████████| 5500/5500 [29:31<00:00,  2.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 5500/5500 [29:31<00:00,  2.77s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5500/5600 [29:39<04:36,  2.77s/pipeline]Optimization Progress:  98%|█████████▊| 5501/5600 [29:39<07:09,  4.34s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5501/5600 [29:39<07:09,  4.34s/pipeline]Optimization Progress:  98%|█████████▊| 5503/5600 [29:46<06:40,  4.13s/pipeline]Optimization Progress: 100%|█████████▉| 5583/5600 [29:50<00:49,  2.90s/pipeline]
Generation 55 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 5600/5600 [29:56<00:00,  2.90s/pipeline]Optimization Progress: 100%|██████████| 5600/5600 [29:56<00:00,  2.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5600/5600 [29:59<00:00,  2.13s/pipeline]Optimization Progress:  98%|█████████▊| 5601/5700 [30:00<04:40,  2.84s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5601/5700 [30:00<04:40,  2.84s/pipeline]Optimization Progress:  98%|█████████▊| 5603/5700 [30:08<05:08,  3.19s/pipeline]Optimization Progress: 100%|█████████▉| 5683/5700 [30:13<00:38,  2.25s/pipeline]
Generation 56 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5700/5700 [30:13<00:00,  2.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5700/5700 [30:17<00:00,  2.25s/pipeline]Optimization Progress: 100%|██████████| 5700/5700 [30:17<00:00,  1.63s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 5700/5700 [30:18<00:00,  1.63s/pipeline]Optimization Progress:  98%|█████████▊| 5707/5800 [30:23<02:10,  1.40s/pipeline]Optimization Progress:  98%|█████████▊| 5708/5800 [30:34<06:33,  4.28s/pipeline]Optimization Progress: 100%|█████████▉| 5788/5800 [30:38<00:36,  3.01s/pipeline]
Generation 57 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 5800/5800 [30:44<00:00,  3.01s/pipeline]Optimization Progress: 100%|██████████| 5800/5800 [30:44<00:00,  2.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5800/5800 [30:47<00:00,  2.26s/pipeline]Optimization Progress:  98%|█████████▊| 5802/5900 [30:47<03:21,  2.06s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5802/5900 [30:47<03:21,  2.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5803/5900 [30:47<03:19,  2.06s/pipeline]Optimization Progress:  98%|█████████▊| 5805/5900 [31:00<04:14,  2.68s/pipeline]Optimization Progress: 100%|█████████▉| 5885/5900 [31:07<00:28,  1.90s/pipeline]
Generation 58 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [31:11<00:00,  1.90s/pipeline]Optimization Progress: 100%|██████████| 5900/5900 [31:11<00:00,  1.41s/pipeline]Optimization Progress:  98%|█████████▊| 5901/6000 [31:17<04:48,  2.91s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5901/6000 [31:17<04:48,  2.91s/pipeline]Optimization Progress:  98%|█████████▊| 5903/6000 [36:20<1:16:45, 47.48s/pipeline]                                                                                  Skipped pipeline #5966 due to time out. Continuing to the next pipeline.
Optimization Progress:  99%|█████████▉| 5966/6000 [36:20<26:54, 47.48s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress: 100%|█████████▉| 5984/6000 [36:26<08:52, 33.26s/pipeline]
Generation 59 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6001pipeline [36:30, 33.26s/pipeline]Optimization Progress: 6001pipeline [36:30, 23.36s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [20:56:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 6001pipeline [36:31, 23.36s/pipeline]Optimization Progress:  98%|█████████▊| 6002/6100 [36:35<29:02, 17.78s/pipeline]Optimization Progress:  98%|█████████▊| 6003/6100 [36:49<26:54, 16.64s/pipeline]Optimization Progress: 100%|█████████▉| 6083/6100 [36:57<03:18, 11.68s/pipeline]
Generation 60 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6101pipeline [36:58, 11.68s/pipeline]Optimization Progress: 6101pipeline [36:58,  8.19s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6101pipeline [37:05,  8.19s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6101pipeline [37:05,  8.19s/pipeline]Optimization Progress:  98%|█████████▊| 6104/6200 [37:07<10:34,  6.61s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6104/6200 [37:07<10:34,  6.61s/pipeline]Optimization Progress:  98%|█████████▊| 6106/6200 [37:17<09:44,  6.22s/pipeline]Optimization Progress: 100%|█████████▉| 6186/6200 [37:21<01:01,  4.37s/pipeline]
Generation 61 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6201pipeline [37:24,  4.37s/pipeline]Optimization Progress: 6201pipeline [37:24,  3.12s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6201pipeline [37:26,  3.12s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 6201pipeline [37:27,  3.12s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 6201pipeline [37:28,  3.12s/pipeline]Optimization Progress:  98%|█████████▊| 6202/6300 [37:29<05:56,  3.64s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6202/6300 [37:29<05:56,  3.64s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6203/6300 [37:29<05:52,  3.64s/pipeline]Optimization Progress:  98%|█████████▊| 6205/6300 [37:41<05:50,  3.69s/pipeline]Optimization Progress: 100%|█████████▉| 6285/6300 [37:48<00:39,  2.61s/pipeline]
Generation 62 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [20:58:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 6301pipeline [37:50,  2.61s/pipeline]Optimization Progress: 6301pipeline [37:50,  1.86s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6301pipeline [37:54,  1.86s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6301pipeline [37:55,  1.86s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 6301pipeline [37:57,  1.86s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6301/6400 [37:58<03:04,  1.86s/pipeline]Optimization Progress:  98%|█████████▊| 6302/6400 [37:58<06:09,  3.77s/pipeline]Optimization Progress:  98%|█████████▊| 6303/6400 [38:10<10:04,  6.23s/pipeline]Optimization Progress: 100%|█████████▉| 6383/6400 [38:21<01:14,  4.40s/pipeline]
Generation 63 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6401pipeline [38:21,  4.40s/pipeline]Optimization Progress: 6401pipeline [38:21,  3.09s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 6401pipeline [38:27,  3.09s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6402/6500 [38:30<05:02,  3.09s/pipeline]Optimization Progress:  99%|█████████▊| 6403/6500 [38:40<04:59,  3.09s/pipeline]Optimization Progress:  99%|█████████▊| 6404/6500 [38:41<06:38,  4.15s/pipeline]Optimization Progress: 100%|█████████▉| 6484/6500 [38:45<00:46,  2.92s/pipeline]
Generation 64 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6501pipeline [38:47,  2.92s/pipeline]Optimization Progress: 6501pipeline [38:47,  2.07s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6501pipeline [38:47,  2.07s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 6501pipeline [38:54,  2.07s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6501/6600 [38:55<03:25,  2.07s/pipeline]Optimization Progress:  99%|█████████▊| 6502/6600 [38:55<06:33,  4.02s/pipeline]Optimization Progress:  99%|█████████▊| 6503/6600 [39:07<10:28,  6.47s/pipeline]Optimization Progress: 100%|█████████▉| 6583/6600 [39:12<01:17,  4.55s/pipeline]
Generation 65 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 6601pipeline [39:13,  4.55s/pipeline]Optimization Progress: 6601pipeline [39:13,  3.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6601pipeline [39:15,  3.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6601pipeline [39:16,  3.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 6601pipeline [39:19,  3.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6601pipeline [39:20,  3.20s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6601pipeline [39:22,  3.20s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6602/6700 [39:22<05:13,  3.20s/pipeline]Optimization Progress:  99%|█████████▊| 6603/6700 [39:22<05:50,  3.62s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6603/6700 [39:22<05:50,  3.62s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▊| 6604/6700 [39:22<05:47,  3.62s/pipeline]Optimization Progress:  99%|█████████▊| 6606/6700 [39:36<06:08,  3.93s/pipeline]Optimization Progress: 100%|█████████▉| 6686/6700 [39:41<00:38,  2.77s/pipeline]
Generation 66 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [21:00:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 6701pipeline [39:42,  2.77s/pipeline]Optimization Progress: 6701pipeline [39:42,  1.96s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6701pipeline [39:43,  1.96s/pipeline]Optimization Progress:  99%|█████████▊| 6702/6800 [40:02<12:13,  7.48s/pipeline]Optimization Progress: 100%|█████████▉| 6782/6800 [40:06<01:34,  5.25s/pipeline]
Generation 67 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 6801pipeline [40:08,  5.25s/pipeline]Optimization Progress: 6801pipeline [40:08,  3.70s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 6801pipeline [40:16,  3.70s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6801/6900 [40:16<06:06,  3.70s/pipeline]Optimization Progress:  99%|█████████▊| 6802/6900 [40:16<08:14,  5.05s/pipeline]Optimization Progress:  99%|█████████▊| 6803/6900 [40:26<10:39,  6.59s/pipeline]Optimization Progress: 100%|█████████▉| 6883/6900 [40:36<01:19,  4.65s/pipeline]
Generation 68 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 6901pipeline [40:39,  4.65s/pipeline]Optimization Progress: 6901pipeline [40:39,  3.30s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 6901pipeline [40:39,  3.30s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 6901pipeline [40:40,  3.30s/pipeline]Optimization Progress:  99%|█████████▊| 6902/7000 [40:46<07:28,  4.58s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▊| 6902/7000 [40:46<07:28,  4.58s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6903/7000 [40:46<07:24,  4.58s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6904/7000 [40:47<07:19,  4.58s/pipeline]Optimization Progress:  99%|█████████▊| 6906/7000 [40:59<06:28,  4.14s/pipeline]Optimization Progress: 100%|█████████▉| 6986/7000 [41:04<00:40,  2.91s/pipeline]
Generation 69 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7001pipeline [41:04,  2.91s/pipeline]Optimization Progress: 7001pipeline [41:04,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7001pipeline [41:05,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 7001pipeline [41:05,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 7001pipeline [41:05,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7001pipeline [41:06,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7001pipeline [41:08,  2.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7001pipeline [41:08,  2.05s/pipeline]Optimization Progress:  99%|█████████▊| 7005/7100 [41:13<03:17,  2.07s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7005/7100 [41:13<03:17,  2.07s/pipeline]Optimization Progress:  99%|█████████▊| 7007/7100 [41:20<03:51,  2.49s/pipeline]Optimization Progress: 100%|█████████▉| 7087/7100 [41:24<00:22,  1.76s/pipeline]
Generation 70 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7101pipeline [41:25,  1.76s/pipeline]Optimization Progress: 7101pipeline [41:25,  1.27s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 7101pipeline [41:28,  1.27s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7101pipeline [41:30,  1.27s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7101pipeline [41:31,  1.27s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:01:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 7101pipeline [41:32,  1.27s/pipeline]Optimization Progress:  99%|█████████▊| 7102/7200 [41:33<05:14,  3.21s/pipeline]Optimization Progress:  99%|█████████▊| 7103/7200 [41:46<09:40,  5.99s/pipeline]Optimization Progress: 100%|█████████▉| 7183/7200 [41:58<01:12,  4.24s/pipeline]
Generation 71 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7201pipeline [41:59,  4.24s/pipeline]Optimization Progress: 7201pipeline [41:59,  2.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7201pipeline [42:00,  2.98s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 7201pipeline [42:03,  2.98s/pipeline]Optimization Progress:  99%|█████████▊| 7203/7300 [42:07<05:15,  3.25s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7203/7300 [42:07<05:15,  3.25s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7204/7300 [42:07<05:12,  3.25s/pipeline]Optimization Progress:  99%|█████████▊| 7206/7300 [42:16<05:06,  3.26s/pipeline]Optimization Progress: 100%|█████████▉| 7286/7300 [42:28<00:32,  2.33s/pipeline]
Generation 72 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-148012500.4403528	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.001, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-6	-142688405.26526332	XGBRegressor(ZeroCount(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 7301pipeline [42:31,  2.33s/pipeline]Optimization Progress: 7301pipeline [42:31,  1.69s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 7301pipeline [42:33,  1.69s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 7301pipeline [42:33,  1.69s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7301/7400 [42:37<02:47,  1.69s/pipeline]Optimization Progress:  99%|█████████▊| 7302/7400 [42:37<04:49,  2.95s/pipeline]Optimization Progress:  99%|█████████▊| 7303/7400 [42:50<09:34,  5.92s/pipeline]Optimization Progress: 100%|█████████▉| 7383/7400 [42:59<01:11,  4.18s/pipeline]
Generation 73 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [21:03:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 7401pipeline [43:00,  4.18s/pipeline]Optimization Progress: 7401pipeline [43:00,  2.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 7401pipeline [43:02,  2.93s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7401pipeline [43:06,  2.93s/pipeline]Optimization Progress:  99%|█████████▊| 7403/7500 [43:09<05:28,  3.38s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7403/7500 [43:09<05:28,  3.38s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7404/7500 [43:09<05:24,  3.38s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7405/7500 [43:09<05:21,  3.38s/pipeline]Optimization Progress:  99%|█████████▊| 7406/7500 [43:20<05:17,  3.38s/pipeline]Optimization Progress:  99%|█████████▉| 7407/7500 [43:20<05:01,  3.24s/pipeline]Optimization Progress: 100%|█████████▉| 7487/7500 [43:31<00:30,  2.31s/pipeline]
Generation 74 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 7501pipeline [43:33,  2.31s/pipeline]Optimization Progress: 7501pipeline [43:33,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7501pipeline [43:33,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7501pipeline [43:34,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 7501pipeline [43:35,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7501pipeline [43:35,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7501pipeline [43:38,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 7501pipeline [43:39,  1.66s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 7501pipeline [43:39,  1.66s/pipeline]Optimization Progress:  99%|█████████▊| 7503/7600 [43:40<03:24,  2.11s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7503/7600 [43:40<03:24,  2.11s/pipeline]Optimization Progress:  99%|█████████▉| 7505/7600 [43:52<05:18,  3.35s/pipeline]Optimization Progress: 100%|█████████▉| 7585/7600 [44:10<00:36,  2.42s/pipeline]
Generation 75 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7601pipeline [44:12,  2.42s/pipeline]Optimization Progress: 7601pipeline [44:12,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7601pipeline [44:13,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 7601pipeline [44:13,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 7601pipeline [44:14,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7601pipeline [44:15,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7601pipeline [44:15,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 7601pipeline [44:17,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7601pipeline [44:17,  1.72s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:04:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 7601pipeline [44:18,  1.72s/pipeline]Optimization Progress:  99%|█████████▉| 7604/7700 [44:19<03:06,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7604/7700 [44:19<03:06,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7605/7700 [44:19<03:04,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7606/7700 [44:19<03:02,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7607/7700 [44:19<03:00,  1.94s/pipeline]Optimization Progress:  99%|█████████▉| 7609/7700 [44:30<03:03,  2.01s/pipeline]Optimization Progress: 100%|█████████▉| 7689/7700 [44:36<00:15,  1.43s/pipeline]
Generation 76 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7701pipeline [44:37,  1.43s/pipeline]Optimization Progress: 7701pipeline [44:37,  1.02s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7701pipeline [44:37,  1.02s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7701pipeline [44:38,  1.02s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7701pipeline [44:40,  1.02s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 7701pipeline [44:40,  1.02s/pipeline]Optimization Progress:  99%|█████████▉| 7703/7800 [44:44<02:57,  1.83s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7703/7800 [44:44<02:57,  1.83s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7704/7800 [44:44<02:55,  1.83s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7705/7800 [44:44<02:53,  1.83s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7706/7800 [44:44<02:51,  1.83s/pipeline]Optimization Progress:  99%|█████████▉| 7708/7800 [44:49<02:26,  1.60s/pipeline]Optimization Progress: 100%|█████████▉| 7788/7800 [44:55<00:13,  1.14s/pipeline]
Generation 77 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 7801pipeline [44:56,  1.14s/pipeline]Optimization Progress: 7801pipeline [44:56,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7801pipeline [44:58,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7801pipeline [44:58,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 7801pipeline [44:58,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7801pipeline [45:00,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 7801pipeline [45:01,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 7801pipeline [45:02,  1.21pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 7801pipeline [45:03,  1.21pipeline/s]Optimization Progress:  99%|█████████▉| 7804/7900 [45:03<02:03,  1.29s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7804/7900 [45:03<02:03,  1.29s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7805/7900 [45:03<02:02,  1.29s/pipeline]Optimization Progress:  99%|█████████▉| 7807/7900 [45:15<03:15,  2.10s/pipeline]Optimization Progress: 100%|█████████▉| 7887/7900 [45:18<00:19,  1.48s/pipeline]
Generation 78 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-151832380.9939387	XGBRegressor(ElasticNetCV(XGBRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7901pipeline [45:18,  1.48s/pipeline]Optimization Progress: 7901pipeline [45:18,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 7901pipeline [45:19,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 7901pipeline [45:19,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 7901pipeline [45:24,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 7901pipeline [45:25,  1.05s/pipeline]Optimization Progress:  99%|█████████▉| 7904/8000 [45:27<02:28,  1.54s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7904/8000 [45:27<02:28,  1.54s/pipeline]Optimization Progress:  99%|█████████▉| 7906/8000 [45:38<04:22,  2.80s/pipeline]Optimization Progress: 100%|█████████▉| 7986/8000 [45:48<00:27,  1.99s/pipeline]
Generation 79 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8001pipeline [45:51,  1.99s/pipeline]Optimization Progress: 8001pipeline [45:51,  1.46s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 8001pipeline [45:53,  1.46s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 8001pipeline [45:53,  1.46s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 8001pipeline [45:56,  1.46s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8001pipeline [45:57,  1.46s/pipeline]                                                            Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8001/8100 [45:58<02:24,  1.46s/pipeline]Optimization Progress:  99%|█████████▉| 8002/8100 [45:58<05:02,  3.09s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8002/8100 [45:58<05:02,  3.09s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8003/8100 [45:58<04:59,  3.09s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8004/8100 [45:58<04:56,  3.09s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8005/8100 [45:58<04:53,  3.09s/pipeline]Optimization Progress:  99%|█████████▉| 8007/8100 [46:10<04:30,  2.91s/pipeline]Optimization Progress: 100%|█████████▉| 8087/8100 [46:13<00:26,  2.05s/pipeline]
Generation 80 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 8101pipeline [46:13,  2.05s/pipeline]Optimization Progress: 8101pipeline [46:13,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 8101pipeline [46:14,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8101pipeline [46:14,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 8101pipeline [46:16,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8101pipeline [46:18,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 8101pipeline [46:19,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8101pipeline [46:20,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8101pipeline [46:21,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 8101pipeline [46:22,  1.45s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8101pipeline [46:22,  1.45s/pipeline]Optimization Progress:  99%|█████████▉| 8105/8200 [46:22<02:36,  1.65s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8105/8200 [46:22<02:36,  1.65s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8106/8200 [46:22<02:34,  1.65s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8107/8200 [46:22<02:33,  1.65s/pipeline]Optimization Progress:  99%|█████████▉| 8109/8200 [46:39<03:42,  2.44s/pipeline]Optimization Progress: 100%|█████████▉| 8189/8200 [46:44<00:18,  1.72s/pipeline]
Generation 81 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8201pipeline [46:45,  1.72s/pipeline]Optimization Progress: 8201pipeline [46:45,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8201pipeline [46:46,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8201pipeline [46:48,  1.23s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8201pipeline [46:49,  1.23s/pipeline]Optimization Progress:  99%|█████████▉| 8210/8300 [46:53<01:41,  1.13s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8210/8300 [46:53<01:41,  1.13s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8211/8300 [46:53<01:40,  1.13s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8212/8300 [46:53<01:39,  1.13s/pipeline]Optimization Progress:  99%|█████████▉| 8214/8300 [47:05<02:26,  1.70s/pipeline]Optimization Progress: 100%|█████████▉| 8294/8300 [47:09<00:07,  1.20s/pipeline]
Generation 82 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8301pipeline [47:09,  1.20s/pipeline]Optimization Progress: 8301pipeline [47:09,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 8301pipeline [47:10,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8301pipeline [47:10,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 8301pipeline [47:11,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8301pipeline [47:11,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8301pipeline [47:11,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8301pipeline [47:14,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8301pipeline [47:16,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8301pipeline [47:16,  1.15pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8301pipeline [47:16,  1.15pipeline/s]Optimization Progress:  99%|█████████▉| 8306/8400 [47:17<01:38,  1.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8306/8400 [47:17<01:38,  1.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8307/8400 [47:17<01:37,  1.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8308/8400 [47:17<01:36,  1.05s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 8309/8400 [47:17<01:35,  1.05s/pipeline]Optimization Progress:  99%|█████████▉| 8311/8400 [47:28<02:03,  1.39s/pipeline]Optimization Progress: 100%|█████████▉| 8391/8400 [47:31<00:08,  1.02pipeline/s]
Generation 83 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-142688405.26526332	XGBRegressor(ElasticNetCV(SelectFwe(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), SelectFwe__alpha=0.037), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8401pipeline [47:31,  1.02pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 8401pipeline [47:31,  1.02pipeline/s]Optimization Progress: 8401pipeline [47:31,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8401pipeline [47:33,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8401pipeline [47:33,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 8401pipeline [47:35,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:07:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 8401pipeline [47:35,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8401pipeline [47:37,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8401pipeline [47:37,  1.43pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8401pipeline [47:39,  1.43pipeline/s]Optimization Progress:  99%|█████████▉| 8407/8500 [47:39<01:21,  1.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8407/8500 [47:39<01:21,  1.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8408/8500 [47:39<01:20,  1.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8409/8500 [47:39<01:19,  1.14pipeline/s]Optimization Progress:  99%|█████████▉| 8411/8500 [47:50<02:06,  1.43s/pipeline]Optimization Progress: 100%|█████████▉| 8491/8500 [47:55<00:09,  1.02s/pipeline]
Generation 84 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8501pipeline [47:57,  1.02s/pipeline]Optimization Progress: 8501pipeline [47:57,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8501pipeline [47:57,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 8501pipeline [48:00,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8501pipeline [48:00,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8501pipeline [48:01,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8501pipeline [48:02,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8501pipeline [48:02,  1.33pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00050.
Optimization Progress: 8501pipeline [48:03,  1.33pipeline/s]Optimization Progress:  99%|█████████▉| 8510/8600 [48:04<01:08,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8510/8600 [48:04<01:08,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8511/8600 [48:04<01:07,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8512/8600 [48:04<01:06,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8513/8600 [48:04<01:06,  1.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8514/8600 [48:04<01:05,  1.31pipeline/s]Optimization Progress:  99%|█████████▉| 8516/8600 [48:15<01:30,  1.07s/pipeline]Optimization Progress: 100%|█████████▉| 8596/8600 [48:19<00:03,  1.30pipeline/s]
Generation 85 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [48:19,  1.30pipeline/s]Optimization Progress: 8601pipeline [48:19,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [48:20,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8601pipeline [48:21,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 8601pipeline [48:23,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [48:25,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8601pipeline [48:25,  1.81pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8601pipeline [48:26,  1.81pipeline/s]Optimization Progress:  99%|█████████▉| 8607/8700 [48:26<01:08,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8607/8700 [48:26<01:08,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8608/8700 [48:26<01:07,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8609/8700 [48:26<01:07,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8610/8700 [48:26<01:06,  1.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8611/8700 [48:26<01:05,  1.36pipeline/s]Optimization Progress:  99%|█████████▉| 8613/8700 [48:37<01:32,  1.06s/pipeline]Optimization Progress: 100%|█████████▉| 8693/8700 [48:41<00:05,  1.32pipeline/s]
Generation 86 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8701pipeline [48:42,  1.32pipeline/s]Optimization Progress: 8701pipeline [48:42,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 8701pipeline [48:44,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8701pipeline [48:44,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8701pipeline [48:45,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 8701pipeline [48:47,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 8701pipeline [48:47,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8701pipeline [48:48,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8701pipeline [48:48,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8701pipeline [48:48,  1.86pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8701pipeline [48:49,  1.86pipeline/s]Optimization Progress:  99%|█████████▉| 8708/8800 [48:49<01:05,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8708/8800 [48:49<01:05,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8709/8800 [48:49<01:05,  1.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8710/8800 [48:50<01:04,  1.40pipeline/s]Optimization Progress:  99%|█████████▉| 8711/8800 [49:00<01:03,  1.40pipeline/s]Optimization Progress:  99%|█████████▉| 8712/8800 [49:08<02:44,  1.87s/pipeline]Optimization Progress: 100%|█████████▉| 8792/8800 [49:12<00:10,  1.33s/pipeline]
Generation 87 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8801pipeline [49:12,  1.33s/pipeline]Optimization Progress: 8801pipeline [49:12,  1.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8801pipeline [49:15,  1.06pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 8801pipeline [49:20,  1.06pipeline/s]Optimization Progress:  99%|█████████▉| 8808/8900 [49:20<01:30,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8808/8900 [49:20<01:30,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8809/8900 [49:20<01:29,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8810/8900 [49:20<01:28,  1.01pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8811/8900 [49:20<01:27,  1.01pipeline/s]Optimization Progress:  99%|█████████▉| 8813/8900 [49:31<01:58,  1.37s/pipeline]Optimization Progress: 100%|█████████▉| 8893/8900 [49:44<00:07,  1.00s/pipeline]
Generation 88 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 8901pipeline [49:44,  1.00s/pipeline]Optimization Progress: 8901pipeline [49:44,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8901pipeline [49:49,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8901pipeline [49:50,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8901pipeline [49:50,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 8901pipeline [49:52,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 8901pipeline [49:52,  1.41pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:10:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 8901pipeline [49:53,  1.41pipeline/s]Optimization Progress:  99%|█████████▉| 8909/9000 [49:53<01:17,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8909/9000 [49:53<01:17,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8910/9000 [49:53<01:16,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8911/9000 [49:53<01:16,  1.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8912/9000 [49:53<01:15,  1.17pipeline/s]Optimization Progress:  99%|█████████▉| 8914/9000 [50:05<01:50,  1.29s/pipeline]Optimization Progress: 100%|█████████▉| 8994/9000 [50:10<00:05,  1.09pipeline/s]
Generation 89 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9001pipeline [50:11,  1.09pipeline/s]Optimization Progress: 9001pipeline [50:11,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9001pipeline [50:11,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9001pipeline [50:15,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9001pipeline [50:15,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 9001pipeline [50:17,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9001pipeline [50:17,  1.46pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9001pipeline [50:18,  1.46pipeline/s]Optimization Progress:  99%|█████████▉| 9005/9100 [50:18<01:40,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9005/9100 [50:18<01:40,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9006/9100 [50:18<01:39,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9007/9100 [50:18<01:38,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9008/9100 [50:18<01:37,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9009/9100 [50:18<01:36,  1.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9010/9100 [50:18<01:35,  1.06s/pipeline]Optimization Progress:  99%|█████████▉| 9012/9100 [50:25<01:31,  1.04s/pipeline]Optimization Progress: 100%|█████████▉| 9092/9100 [50:29<00:05,  1.35pipeline/s]
Generation 90 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9101pipeline [50:30,  1.35pipeline/s]Optimization Progress: 9101pipeline [50:30,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9101pipeline [50:30,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9101pipeline [50:31,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9101pipeline [50:32,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9101pipeline [50:33,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9101pipeline [50:34,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9101pipeline [50:34,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 9101pipeline [50:36,  1.75pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9101pipeline [50:37,  1.75pipeline/s]Optimization Progress:  99%|█████████▉| 9113/9200 [50:37<00:50,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9113/9200 [50:37<00:50,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9114/9200 [50:37<00:49,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9115/9200 [50:37<00:49,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9116/9200 [50:37<00:48,  1.72pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9117/9200 [50:37<00:48,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 9119/9200 [50:52<01:33,  1.15s/pipeline]Optimization Progress: 100%|█████████▉| 9199/9200 [50:56<00:00,  1.22pipeline/s]
Generation 91 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9201pipeline [50:58,  1.22pipeline/s]Optimization Progress: 9201pipeline [50:58,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9201pipeline [51:00,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9201pipeline [51:00,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9201pipeline [51:00,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9201pipeline [51:01,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 9201pipeline [51:01,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9201pipeline [51:01,  1.18pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 9201pipeline [51:01,  1.18pipeline/s]Optimization Progress:  99%|█████████▉| 9210/9300 [51:05<01:13,  1.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9210/9300 [51:05<01:13,  1.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9211/9300 [51:05<01:12,  1.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9212/9300 [51:05<01:12,  1.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9213/9300 [51:05<01:11,  1.22pipeline/s]Optimization Progress:  99%|█████████▉| 9215/9300 [51:20<02:05,  1.47s/pipeline]Optimization Progress: 100%|█████████▉| 9295/9300 [51:32<00:05,  1.08s/pipeline]
Generation 92 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 9301pipeline [51:33,  1.08s/pipeline]Optimization Progress: 9301pipeline [51:33,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9301pipeline [51:35,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9301pipeline [51:35,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9301pipeline [51:37,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9301pipeline [51:38,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9301pipeline [51:38,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 9301pipeline [51:38,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 9301pipeline [51:40,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9301pipeline [51:40,  1.25pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 9301pipeline [51:41,  1.25pipeline/s]Optimization Progress:  99%|█████████▉| 9307/9400 [51:42<01:31,  1.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9307/9400 [51:42<01:31,  1.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9308/9400 [51:42<01:30,  1.02pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9309/9400 [51:42<01:29,  1.02pipeline/s]Optimization Progress:  99%|█████████▉| 9311/9400 [51:52<02:13,  1.50s/pipeline]Optimization Progress: 100%|█████████▉| 9391/9400 [51:57<00:09,  1.07s/pipeline]
Generation 93 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9401pipeline [51:59,  1.07s/pipeline]Optimization Progress: 9401pipeline [51:59,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 9401pipeline [52:01,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9401pipeline [52:02,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9401pipeline [52:02,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9401pipeline [52:03,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9401pipeline [52:05,  1.23pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:12:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 9401pipeline [52:06,  1.23pipeline/s]Optimization Progress:  99%|█████████▉| 9404/9500 [52:07<02:02,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9404/9500 [52:07<02:02,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9405/9500 [52:07<02:01,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9406/9500 [52:07<02:00,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9407/9500 [52:07<01:59,  1.28s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9408/9500 [52:07<01:57,  1.28s/pipeline]Optimization Progress:  99%|█████████▉| 9410/9500 [52:30<03:07,  2.09s/pipeline]Optimization Progress: 100%|█████████▉| 9490/9500 [52:34<00:14,  1.48s/pipeline]
Generation 94 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9501pipeline [52:35,  1.48s/pipeline]Optimization Progress: 9501pipeline [52:35,  1.04s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 9501pipeline [52:36,  1.04s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9501pipeline [52:41,  1.04s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9501pipeline [52:42,  1.04s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 9501pipeline [52:42,  1.04s/pipeline]Optimization Progress:  99%|█████████▉| 9503/9600 [52:44<03:25,  2.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9503/9600 [52:44<03:25,  2.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9504/9600 [52:44<03:23,  2.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9505/9600 [52:44<03:20,  2.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9506/9600 [52:44<03:18,  2.12s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9507/9600 [52:44<03:16,  2.12s/pipeline]Optimization Progress:  99%|█████████▉| 9509/9600 [52:56<03:11,  2.11s/pipeline]Optimization Progress: 100%|█████████▉| 9589/9600 [52:59<00:16,  1.49s/pipeline]
Generation 95 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [52:59,  1.49s/pipeline]Optimization Progress: 9601pipeline [52:59,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [53:00,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [53:00,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [53:01,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 9601pipeline [53:01,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9601pipeline [53:02,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 9601pipeline [53:03,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 [21:13:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 9601pipeline [53:07,  1.05s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 9601pipeline [53:08,  1.05s/pipeline]Optimization Progress:  99%|█████████▉| 9605/9700 [53:09<02:14,  1.42s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9605/9700 [53:09<02:14,  1.42s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9606/9700 [53:09<02:13,  1.42s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9607/9700 [53:09<02:11,  1.42s/pipeline]Optimization Progress:  99%|█████████▉| 9608/9700 [53:20<02:10,  1.42s/pipeline]Optimization Progress:  99%|█████████▉| 9609/9700 [53:23<03:12,  2.11s/pipeline]Optimization Progress: 100%|█████████▉| 9689/9700 [53:27<00:16,  1.49s/pipeline]
Generation 96 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9701pipeline [53:29,  1.49s/pipeline]Optimization Progress: 9701pipeline [53:29,  1.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9701pipeline [53:30,  1.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9701pipeline [53:34,  1.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9701pipeline [53:35,  1.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 9701pipeline [53:36,  1.08s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9701pipeline [53:36,  1.08s/pipeline]Optimization Progress:  99%|█████████▉| 9709/9800 [53:36<01:34,  1.04s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9709/9800 [53:36<01:34,  1.04s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9710/9800 [53:36<01:33,  1.04s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9711/9800 [53:36<01:32,  1.04s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9712/9800 [53:36<01:31,  1.04s/pipeline]Optimization Progress:  99%|█████████▉| 9714/9800 [53:52<02:20,  1.63s/pipeline]Optimization Progress: 100%|█████████▉| 9794/9800 [53:56<00:06,  1.16s/pipeline]
Generation 97 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9801pipeline [53:56,  1.16s/pipeline]Optimization Progress: 9801pipeline [53:56,  1.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9801pipeline [54:05,  1.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 9801pipeline [54:05,  1.20pipeline/s]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9801pipeline [54:05,  1.20pipeline/s]Optimization Progress:  99%|█████████▉| 9802/9900 [54:06<05:53,  3.61s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9802/9900 [54:06<05:53,  3.61s/pipeline]Optimization Progress:  99%|█████████▉| 9804/9900 [54:15<06:04,  3.79s/pipeline]Optimization Progress: 100%|█████████▉| 9884/9900 [54:19<00:42,  2.67s/pipeline]
Generation 98 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9901pipeline [54:19,  2.67s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 9901pipeline [54:21,  2.67s/pipeline]Optimization Progress: 9901pipeline [54:21,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9901pipeline [54:23,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9901pipeline [54:25,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 9901pipeline [54:26,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9901pipeline [54:26,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 9901pipeline [54:26,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 9901pipeline [54:27,  1.91s/pipeline]                                                            _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 9901pipeline [54:28,  1.91s/pipeline]Optimization Progress:  99%|█████████▉| 9902/10000 [54:31<06:45,  4.14s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9902/10000 [54:31<06:45,  4.14s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9903/10000 [54:31<06:41,  4.14s/pipeline]Optimization Progress:  99%|█████████▉| 9905/10000 [54:46<06:57,  4.39s/pipeline]Optimization Progress: 100%|█████████▉| 9985/10000 [54:49<00:46,  3.09s/pipeline]
Generation 99 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [54:51,  3.09s/pipeline]Optimization Progress: 10001pipeline [54:51,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [54:55,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10001pipeline [54:56,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10001pipeline [54:57,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10001pipeline [54:57,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10001pipeline [54:57,  2.20s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10001pipeline [54:58,  2.20s/pipeline]Optimization Progress:  99%|█████████▉| 10002/10100 [54:59<06:18,  3.86s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10002/10100 [54:59<06:18,  3.86s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10003/10100 [54:59<06:14,  3.86s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10004/10100 [54:59<06:10,  3.86s/pipeline]Optimization Progress:  99%|█████████▉| 10006/10100 [55:16<06:13,  3.97s/pipeline]Optimization Progress: 100%|█████████▉| 10086/10100 [55:21<00:39,  2.80s/pipeline]
Generation 100 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 10101pipeline [55:22,  2.80s/pipeline]Optimization Progress: 10101pipeline [55:22,  1.97s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10101pipeline [55:23,  1.97s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10101pipeline [55:26,  1.97s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10101pipeline [55:27,  1.97s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10101pipeline [55:27,  1.97s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10101pipeline [55:28,  1.97s/pipeline]Optimization Progress:  99%|█████████▉| 10104/10200 [55:31<03:37,  2.27s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10104/10200 [55:31<03:37,  2.27s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10105/10200 [55:31<03:35,  2.27s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10106/10200 [55:31<03:33,  2.27s/pipeline]Optimization Progress:  99%|█████████▉| 10108/10200 [55:47<04:21,  2.84s/pipeline]Optimization Progress: 100%|█████████▉| 10188/10200 [55:51<00:24,  2.00s/pipeline]
Generation 101 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-150685458.58427125	XGBRegressor(ElasticNetCV(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10201pipeline [55:53,  2.00s/pipeline]Optimization Progress: 10201pipeline [55:53,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 10201pipeline [55:54,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 [21:16:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f8324baddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f8324cbe669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f8324ccbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f8324cb2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f8324b9ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f793295a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f793295a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f793297227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f7932972cb4]

.
Optimization Progress: 10201pipeline [55:55,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10201pipeline [55:55,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10201pipeline [55:56,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 10201pipeline [55:57,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 10201pipeline [55:57,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 10201pipeline [55:58,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10201pipeline [55:58,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10201pipeline [55:59,  1.44s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 10201pipeline [55:59,  1.44s/pipeline]Optimization Progress:  99%|█████████▉| 10209/10300 [56:00<01:57,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10209/10300 [56:00<01:57,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10210/10300 [56:00<01:56,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10211/10300 [56:00<01:55,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10212/10300 [56:00<01:53,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10213/10300 [56:01<01:52,  1.30s/pipeline]Optimization Progress:  99%|█████████▉| 10215/10300 [56:15<02:18,  1.63s/pipeline]Optimization Progress: 100%|█████████▉| 10295/10300 [56:18<00:05,  1.16s/pipeline]
Generation 102 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10301pipeline [56:20,  1.16s/pipeline]Optimization Progress: 10301pipeline [56:20,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10301pipeline [56:21,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10301pipeline [56:21,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10301pipeline [56:21,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 10301pipeline [56:21,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10301pipeline [56:22,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10301pipeline [56:22,  1.10pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10301pipeline [56:27,  1.10pipeline/s]Optimization Progress:  99%|█████████▉| 10303/10400 [56:28<02:48,  1.74s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10303/10400 [56:28<02:48,  1.74s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10304/10400 [56:28<02:47,  1.74s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10305/10400 [56:28<02:45,  1.74s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10306/10400 [56:28<02:43,  1.74s/pipeline]Optimization Progress:  99%|█████████▉| 10308/10400 [56:42<03:10,  2.07s/pipeline]Optimization Progress: 100%|█████████▉| 10388/10400 [56:45<00:17,  1.46s/pipeline]
Generation 103 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10401pipeline [56:48,  1.46s/pipeline]Optimization Progress: 10401pipeline [56:48,  1.09s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10401pipeline [56:49,  1.09s/pipeline]Optimization Progress:  99%|█████████▉| 10403/10500 [56:53<02:22,  1.47s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10403/10500 [56:53<02:22,  1.47s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10404/10500 [56:53<02:21,  1.47s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10405/10500 [56:53<02:19,  1.47s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10406/10500 [56:53<02:18,  1.47s/pipeline]Optimization Progress:  99%|█████████▉| 10408/10500 [57:05<02:43,  1.78s/pipeline]Optimization Progress: 100%|█████████▉| 10488/10500 [57:14<00:15,  1.28s/pipeline]
Generation 104 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10501pipeline [57:16,  1.28s/pipeline]Optimization Progress: 10501pipeline [57:16,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10501pipeline [57:16,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10501pipeline [57:17,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10501pipeline [57:17,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10501pipeline [57:18,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10501pipeline [57:19,  1.08pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10501pipeline [57:20,  1.08pipeline/s]Optimization Progress:  99%|█████████▉| 10502/10600 [57:22<03:59,  2.45s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10502/10600 [57:22<03:59,  2.45s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10503/10600 [57:22<03:57,  2.45s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10504/10600 [57:22<03:54,  2.45s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10505/10600 [57:22<03:52,  2.45s/pipeline]Optimization Progress:  99%|█████████▉| 10507/10600 [57:35<03:56,  2.54s/pipeline]Optimization Progress: 100%|█████████▉| 10587/10600 [57:50<00:23,  1.83s/pipeline]
Generation 105 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10601pipeline [57:50,  1.83s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10601pipeline [57:51,  1.83s/pipeline]Optimization Progress: 10601pipeline [57:51,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10601pipeline [57:56,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10601pipeline [57:56,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10601pipeline [57:57,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10601pipeline [57:57,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10601pipeline [57:59,  1.29s/pipeline]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10601pipeline [57:59,  1.29s/pipeline]Optimization Progress:  99%|█████████▉| 10605/10700 [57:59<02:26,  1.55s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10605/10700 [57:59<02:26,  1.55s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10606/10700 [57:59<02:25,  1.55s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10607/10700 [57:59<02:23,  1.55s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10608/10700 [57:59<02:22,  1.55s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10609/10700 [57:59<02:20,  1.55s/pipeline]Optimization Progress:  99%|█████████▉| 10611/10700 [58:13<02:35,  1.75s/pipeline]Optimization Progress: 100%|█████████▉| 10691/10700 [58:24<00:11,  1.26s/pipeline]
Generation 106 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 10701pipeline [58:26,  1.26s/pipeline]Optimization Progress: 10701pipeline [58:26,  1.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 10701pipeline [58:26,  1.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 10701pipeline [58:26,  1.06pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10701pipeline [58:31,  1.06pipeline/s]Optimization Progress:  99%|█████████▉| 10706/10800 [58:32<01:35,  1.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10706/10800 [58:32<01:35,  1.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10707/10800 [58:32<01:34,  1.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10708/10800 [58:32<01:33,  1.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10709/10800 [58:32<01:32,  1.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10710/10800 [58:32<01:31,  1.02s/pipeline]Optimization Progress:  99%|█████████▉| 10712/10800 [58:44<01:57,  1.34s/pipeline]Optimization Progress: 100%|█████████▉| 10792/10800 [58:48<00:07,  1.05pipeline/s]
Generation 107 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10801pipeline [58:49,  1.05pipeline/s]Optimization Progress: 10801pipeline [58:49,  1.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 10801pipeline [58:51,  1.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10801pipeline [58:52,  1.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10801pipeline [58:53,  1.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10801pipeline [58:54,  1.44pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10801pipeline [58:56,  1.44pipeline/s]Optimization Progress:  99%|█████████▉| 10807/10900 [58:57<01:21,  1.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10807/10900 [58:57<01:21,  1.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10808/10900 [58:57<01:20,  1.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10809/10900 [58:57<01:19,  1.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10810/10900 [58:57<01:19,  1.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10811/10900 [58:57<01:18,  1.14pipeline/s]Optimization Progress:  99%|█████████▉| 10813/10900 [59:12<01:58,  1.36s/pipeline]Optimization Progress: 100%|█████████▉| 10893/10900 [59:14<00:06,  1.04pipeline/s]
Generation 108 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 10901pipeline [59:14,  1.04pipeline/s]Optimization Progress: 10901pipeline [59:14,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10901pipeline [59:15,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 10901pipeline [59:15,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 10901pipeline [59:15,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10901pipeline [59:17,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 10901pipeline [59:17,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10901pipeline [59:18,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10901pipeline [59:18,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 10901pipeline [59:18,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 10901pipeline [59:18,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10901pipeline [59:19,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 10901pipeline [59:19,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10901pipeline [59:19,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 10901pipeline [59:20,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 10901pipeline [59:20,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 10901pipeline [59:20,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 10901pipeline [59:21,  1.45pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 10901pipeline [59:21,  1.45pipeline/s]Optimization Progress:  99%|█████████▉| 10904/11000 [59:23<02:04,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10904/11000 [59:23<02:04,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10905/11000 [59:23<02:03,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10906/11000 [59:23<02:02,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10907/11000 [59:23<02:00,  1.30s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10908/11000 [59:23<01:59,  1.30s/pipeline]Optimization Progress:  99%|█████████▉| 10910/11000 [59:37<02:28,  1.65s/pipeline]Optimization Progress: 100%|█████████▉| 10990/11000 [59:40<00:11,  1.17s/pipeline]
Generation 109 - Current Pareto front scores:
-1	-190013481.55171123	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.6500000000000001, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-2	-166739482.71503004	GradientBoostingRegressor(Nystroem(input_matrix, Nystroem__gamma=0.75, Nystroem__kernel=additive_chi2, Nystroem__n_components=8), GradientBoostingRegressor__alpha=0.75, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=7, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)
-3	-154925371.68655545	XGBRegressor(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-137769008.99485782	XGBRegressor(ElasticNetCV(CombineDFs(input_matrix, XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.7000000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-5	-134972246.80919126	XGBRegressor(SGDRegressor(ElasticNetCV(CombineDFs(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=9, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)), ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001), SGDRegressor__alpha=0.01, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=constant, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=50.0), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 11001pipeline [59:41,  1.17s/pipeline]Optimization Progress: 11001pipeline [59:41,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 11001pipeline [59:42,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 11001pipeline [59:43,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 11001pipeline [59:46,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11001pipeline [59:47,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11001pipeline [59:48,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 11001pipeline [59:48,  1.20pipeline/s]                                                             _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 11001pipeline [59:48,  1.20pipeline/s]Optimization Progress:  99%|█████████▉| 11007/11100 [59:49<01:31,  1.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11007/11100 [59:49<01:31,  1.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11008/11100 [59:49<01:30,  1.01pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11009/11100 [59:49<01:29,  1.01pipeline/s]Optimization Progress:  99%|█████████▉| 11011/11100 [1:00:01<02:24,  1.62s/pipeline]                                                                                    
Optimization Progress: 100%|█████████▉| 11090/11100 [1:00:01<00:16,  1.62s/pipeline]                                                                                    60.11 minutes have elapsed. TPOT will close down.
TPOT closed during evaluation in one generation.
WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.
Optimization Progress: 100%|█████████▉| 11090/11100 [1:00:01<00:16,  1.62s/pipeline]                                                                                    
Optimization Progress: 100%|█████████▉| 11090/11100 [1:00:01<00:16,  1.62s/pipeline]                                                                                    
TPOT closed prematurely. Will use the current best pipeline.
Optimization Progress: 100%|█████████▉| 11090/11100 [1:00:01<00:16,  1.62s/pipeline]                                                                                    Best pipeline:
0. FeatureUnion(transformer_list=[('stackingestimator-1',
                                StackingEstimator(estimator=Pipeline(steps=[('featureunion',
                                                                             FeatureUnion(transformer_list=[('functiontransformer-1',
                                                                                                             FunctionTransformer(func=<function copy at 0x7f792c84fb00>)),
                                                                                                            ('functiontransformer-2',
                                                                                                             FunctionTransformer(func=<function copy at 0x7f792c84fb00>))])),
                                                                            ('xgbregressor',
                                                                             XGBRegressor(base_sc...
                                                                         importance_type='gain',
                                                                         interaction_constraints='',
                                                                         learning_rate=0.01,
                                                                         max_delta_step=0,
                                                                         max_depth=2,
                                                                         min_child_weight=9,
                                                                         missing=nan,
                                                                         monotone_constraints='()',
                                                                         n_estimators=100,
                                                                         n_jobs=1,
                                                                         nthread=1,
                                                                         num_parallel_tree=1,
                                                                         random_state=0,
                                                                         reg_alpha=0,
                                                                         reg_lambda=1,
                                                                         scale_pos_weight=1,
                                                                         subsample=0.6500000000000001,
                                                                         tree_method='exact',
                                                                         validate_parameters=1,
                                                                         verbosity=None)))])
1. StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.45, tol=0.001))
2. StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=1.0,
                                         fit_intercept=False, l1_ratio=0.0,
                                         learning_rate='constant',
                                         penalty='elasticnet', power_t=50.0))
3. XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=1.0, max_delta_step=0, max_depth=8,
             min_child_weight=6, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,
             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
             subsample=0.1, tree_method='exact', validate_parameters=1,
             verbosity=None)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
