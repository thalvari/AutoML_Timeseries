30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:  13%|█▎        | 13/100 [01:00<06:43,  4.64s/pipeline]Optimization Progress:  93%|█████████▎| 93/100 [01:02<00:22,  3.25s/pipeline]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [01:02<00:00,  3.25s/pipeline]Optimization Progress: 100%|██████████| 100/100 [01:02<00:00,  2.28s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [01:04<00:00,  2.28s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [01:05<00:00,  2.28s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [01:07<00:00,  2.28s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  51%|█████     | 102/200 [01:07<03:43,  2.28s/pipeline]Optimization Progress:  52%|█████▏    | 103/200 [01:07<03:26,  2.13s/pipeline]Optimization Progress:  52%|█████▏    | 103/200 [01:19<03:26,  2.13s/pipeline]Optimization Progress:  52%|█████▏    | 104/200 [01:21<08:48,  5.51s/pipeline]Optimization Progress:  92%|█████████▏| 184/200 [01:57<01:03,  3.99s/pipeline]
Generation 1 - Current Pareto front scores:
-1	-220711521.8407498	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-212053910.18716833	XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.2, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.25), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [01:57<00:00,  3.99s/pipeline]Optimization Progress: 100%|██████████| 200/200 [01:57<00:00,  2.80s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:57<00:00,  2.80s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [02:02<00:00,  2.80s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 200/200 [02:03<00:00,  2.80s/pipeline]Optimization Progress:  68%|██████▊   | 204/300 [02:05<04:04,  2.55s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  68%|██████▊   | 204/300 [02:05<04:04,  2.55s/pipeline]Optimization Progress:  69%|██████▊   | 206/300 [02:12<04:25,  2.83s/pipeline]Optimization Progress:  95%|█████████▌| 286/300 [02:14<00:27,  1.99s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-212053910.18716833	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 300/300 [02:17<00:00,  1.99s/pipeline]Optimization Progress: 100%|██████████| 300/300 [02:17<00:00,  1.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [02:18<00:00,  1.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [02:18<00:00,  1.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [02:20<00:00,  1.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [02:20<00:00,  1.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 300/300 [02:21<00:00,  1.46s/pipeline]Optimization Progress:  76%|███████▌  | 302/400 [02:22<02:55,  1.79s/pipeline]Optimization Progress:  76%|███████▌  | 302/400 [02:40<02:55,  1.79s/pipeline]Optimization Progress:  76%|███████▌  | 303/400 [02:43<12:08,  7.51s/pipeline]Optimization Progress:  96%|█████████▌| 383/400 [02:47<01:29,  5.27s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-212053910.18716833	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:47<00:00,  5.27s/pipeline]Optimization Progress: 100%|██████████| 400/400 [02:47<00:00,  3.70s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 400/400 [02:49<00:00,  3.70s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:51<00:00,  3.70s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:53<05:11,  3.21s/pipeline]Optimization Progress:  81%|████████  | 404/500 [03:48<29:48, 18.63s/pipeline]Optimization Progress:  97%|█████████▋| 484/500 [03:53<03:28, 13.06s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-212053910.18716833	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [03:55<00:00, 13.06s/pipeline]Optimization Progress: 100%|██████████| 500/500 [03:55<00:00,  9.18s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [03:57<00:00,  9.18s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [03:57<00:00,  9.18s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 500/500 [03:58<00:00,  9.18s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [03:59<00:00,  9.18s/pipeline]                                                                              _pre_test decorator: _mate_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 500/500 [04:00<00:00,  9.18s/pipeline]Optimization Progress:  84%|████████▎ | 501/600 [04:02<14:09,  8.58s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▎ | 501/600 [04:02<14:09,  8.58s/pipeline]Optimization Progress:  84%|████████▍ | 503/600 [04:23<14:45,  9.13s/pipeline]Optimization Progress:  97%|█████████▋| 583/600 [04:26<01:48,  6.40s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-212053910.18716833	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-211653718.3802868	XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _mate_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 600/600 [04:27<00:00,  6.40s/pipeline]Optimization Progress: 100%|██████████| 600/600 [04:27<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [04:27<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [04:29<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [04:29<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [04:31<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [04:31<00:00,  4.49s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [04:33<00:00,  4.49s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [04:33<06:42,  4.11s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [04:50<06:42,  4.11s/pipeline]Optimization Progress:  86%|████████▌ | 603/700 [04:57<15:55,  9.85s/pipeline]Optimization Progress:  98%|█████████▊| 683/700 [04:59<01:57,  6.90s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-212053910.18716833	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-207243924.9738372	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.25, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=16, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [05:00<00:00,  6.90s/pipeline]Optimization Progress: 100%|██████████| 700/700 [05:00<00:00,  4.85s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [05:01<00:00,  4.85s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [05:04<00:00,  4.85s/pipeline]Optimization Progress:  88%|████████▊ | 704/800 [05:10<06:33,  4.10s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 704/800 [05:10<06:33,  4.10s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 705/800 [05:10<06:29,  4.10s/pipeline]Optimization Progress:  88%|████████▊ | 707/800 [05:19<05:57,  3.85s/pipeline]Optimization Progress:  98%|█████████▊| 787/800 [05:22<00:35,  2.70s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [04:54:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:25<00:00,  2.70s/pipeline]Optimization Progress: 100%|██████████| 800/800 [05:25<00:00,  1.95s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [04:54:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 800/800 [05:25<00:00,  1.95s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 800/800 [05:26<00:00,  1.95s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 800/800 [05:26<00:00,  1.95s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [05:27<00:00,  1.95s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [05:27<00:00,  1.95s/pipeline]Optimization Progress:  89%|████████▉ | 801/900 [05:40<03:12,  1.95s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [05:40<05:59,  3.66s/pipeline]Optimization Progress:  98%|█████████▊| 882/900 [05:49<00:46,  2.60s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [04:55:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 900/900 [05:49<00:00,  2.60s/pipeline]Optimization Progress: 100%|██████████| 900/900 [05:49<00:00,  1.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [05:50<00:00,  1.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 100%|██████████| 900/900 [05:52<00:00,  1.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [05:52<00:00,  1.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [05:54<00:00,  1.82s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 900/900 [05:57<00:00,  1.82s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  90%|█████████ | 900/1000 [05:57<03:02,  1.82s/pipeline]Optimization Progress:  90%|█████████ | 901/1000 [06:00<03:00,  1.82s/pipeline]Optimization Progress:  90%|█████████ | 902/1000 [06:04<05:38,  3.45s/pipeline]Optimization Progress:  98%|█████████▊| 982/1000 [06:07<00:43,  2.43s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [06:07<00:00,  2.43s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [06:07<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1000/1000 [06:08<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 1000/1000 [06:10<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [06:10<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1000/1000 [06:11<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1000/1000 [06:12<00:00,  1.71s/pipeline]Optimization Progress:  91%|█████████ | 1000/1100 [06:20<02:50,  1.71s/pipeline]Optimization Progress:  91%|█████████ | 1001/1100 [06:25<10:48,  6.55s/pipeline]Optimization Progress:  98%|█████████▊| 1081/1100 [06:31<01:27,  4.61s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [06:31<00:00,  4.61s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [06:31<00:00,  3.23s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [06:31<00:00,  3.23s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [06:33<00:00,  3.23s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [06:33<00:00,  3.23s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress: 100%|██████████| 1100/1100 [06:33<00:00,  3.23s/pipeline]Optimization Progress:  92%|█████████▏| 1104/1200 [06:47<05:27,  3.41s/pipeline]Optimization Progress:  99%|█████████▊| 1183/1200 [06:52<00:40,  2.41s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..
Optimization Progress: 100%|██████████| 1200/1200 [06:53<00:00,  2.41s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [06:53<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:55<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [06:55<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [06:57<00:00,  1.71s/pipeline]Optimization Progress:  92%|█████████▏| 1201/1300 [06:58<04:03,  2.46s/pipeline]Optimization Progress:  92%|█████████▏| 1202/1300 [07:07<07:19,  4.48s/pipeline]Optimization Progress:  99%|█████████▊| 1282/1300 [07:14<00:56,  3.17s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 1300/1300 [07:17<00:00,  3.17s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [07:17<00:00,  2.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [07:18<00:00,  2.27s/pipeline]Optimization Progress:  93%|█████████▎| 1301/1400 [07:18<03:18,  2.01s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1301/1400 [07:18<03:18,  2.01s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1302/1400 [07:18<03:16,  2.01s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1303/1400 [07:18<03:14,  2.01s/pipeline]Optimization Progress:  93%|█████████▎| 1305/1400 [07:25<03:03,  1.93s/pipeline]Optimization Progress:  99%|█████████▉| 1385/1400 [07:28<00:20,  1.36s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-204341771.71114203	RandomForestRegressor(DecisionTreeRegressor(PolynomialFeatures(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.15000000000000002, GradientBoostingRegressor__min_samples_leaf=1, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.05), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=9, DecisionTreeRegressor__min_samples_split=13), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.2, RandomForestRegressor__min_samples_leaf=11, RandomForestRegressor__min_samples_split=11, RandomForestRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1400/1400 [07:30<00:00,  1.36s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [07:30<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1400/1400 [07:31<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1400/1400 [07:35<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:56:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1400/1400 [07:35<00:00,  1.01pipeline/s]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [07:37<00:00,  1.01pipeline/s]Optimization Progress:  93%|█████████▎| 1401/1500 [07:37<04:55,  2.99s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  93%|█████████▎| 1401/1500 [07:37<04:55,  2.99s/pipeline]Optimization Progress:  94%|█████████▎| 1403/1500 [08:54<21:55, 13.57s/pipeline]Optimization Progress:  99%|█████████▉| 1483/1500 [08:59<02:41,  9.52s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-195072537.8380814	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:58:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [09:02<00:00,  9.52s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [09:02<00:00,  6.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:58:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [09:02<00:00,  6.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1500/1500 [09:02<00:00,  6.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1500/1500 [09:03<00:00,  6.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:58:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [09:03<00:00,  6.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1500/1500 [09:04<00:00,  6.70s/pipeline]Optimization Progress:  94%|█████████▍| 1501/1600 [09:05<09:19,  5.66s/pipeline]Optimization Progress:  94%|█████████▍| 1502/1600 [09:12<10:00,  6.13s/pipeline]Optimization Progress:  99%|█████████▉| 1582/1600 [09:20<01:17,  4.32s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-195072537.8380814	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [09:21<00:00,  4.32s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [09:21<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [09:22<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1600/1600 [09:24<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [09:24<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:58:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [09:25<00:00,  3.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [04:58:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [09:25<00:00,  3.04s/pipeline]Optimization Progress:  94%|█████████▍| 1602/1700 [09:27<04:54,  3.01s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [09:34<06:54,  4.27s/pipeline]Optimization Progress:  99%|█████████▉| 1683/1700 [10:57<00:56,  3.30s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-195072537.8380814	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1700/1700 [10:57<00:00,  3.30s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [10:57<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1700/1700 [10:57<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [10:57<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [10:58<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1700/1700 [10:59<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:00:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [10:59<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 1700/1700 [11:00<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.00500.
Optimization Progress: 100%|██████████| 1700/1700 [11:00<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:00:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [11:01<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [11:02<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [11:03<00:00,  2.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 1700/1700 [11:04<00:00,  2.31s/pipeline]Optimization Progress:  94%|█████████▍| 1700/1800 [11:10<03:51,  2.31s/pipeline]Optimization Progress:  94%|█████████▍| 1701/1800 [11:15<11:46,  7.13s/pipeline]Optimization Progress:  99%|█████████▉| 1781/1800 [11:21<01:35,  5.01s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1800/1800 [11:21<00:00,  5.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [11:22<00:00,  5.01s/pipeline]Optimization Progress: 100%|██████████| 1800/1800 [11:22<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [11:22<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [11:23<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:00:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [11:25<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:00:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [11:29<00:00,  3.52s/pipeline]Optimization Progress:  95%|█████████▍| 1802/1900 [11:29<05:48,  3.55s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1802/1900 [11:29<05:48,  3.55s/pipeline]Optimization Progress:  95%|█████████▍| 1804/1900 [11:36<05:41,  3.56s/pipeline]Optimization Progress:  99%|█████████▉| 1884/1900 [11:39<00:40,  2.50s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1900/1900 [11:39<00:00,  2.50s/pipeline]Optimization Progress: 100%|██████████| 1900/1900 [11:39<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 1900/1900 [11:40<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:01:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1900/1900 [11:40<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1900/1900 [11:40<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [11:40<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 1900/1900 [11:42<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:01:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 1900/1900 [11:45<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [11:47<00:00,  1.76s/pipeline]Optimization Progress:  95%|█████████▌| 1902/2000 [11:47<04:00,  2.46s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 1902/2000 [11:47<04:00,  2.46s/pipeline]Optimization Progress:  95%|█████████▌| 1904/2000 [11:55<04:35,  2.87s/pipeline]Optimization Progress:  99%|█████████▉| 1984/2000 [12:01<00:32,  2.03s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:01:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2000/2000 [12:05<00:00,  2.03s/pipeline]Optimization Progress: 100%|██████████| 2000/2000 [12:05<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [12:07<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2000/2000 [12:08<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2000/2000 [12:08<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2000/2000 [12:10<00:00,  1.50s/pipeline]Optimization Progress:  95%|█████████▌| 2001/2100 [12:10<04:28,  2.71s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2001/2100 [12:10<04:28,  2.71s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2002/2100 [12:10<04:25,  2.71s/pipeline]Optimization Progress:  95%|█████████▌| 2004/2100 [12:21<04:41,  2.93s/pipeline]Optimization Progress:  99%|█████████▉| 2084/2100 [12:27<00:33,  2.07s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-205457854.99483013	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2100/2100 [12:28<00:00,  2.07s/pipeline]Optimization Progress: 100%|██████████| 2100/2100 [12:28<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:01:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2100/2100 [12:28<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:01:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2100/2100 [12:28<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2100/2100 [12:30<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [12:31<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [12:32<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 2100/2100 [12:32<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:01:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2100/2100 [12:33<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2100/2100 [12:34<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [12:34<00:00,  1.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [12:35<00:00,  1.47s/pipeline]Optimization Progress:  96%|█████████▌| 2102/2200 [12:36<03:40,  2.25s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2102/2200 [12:36<03:40,  2.25s/pipeline]Optimization Progress:  96%|█████████▌| 2104/2200 [13:18<12:37,  7.89s/pipeline]Optimization Progress:  99%|█████████▉| 2184/2200 [13:25<01:28,  5.55s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-202536422.9619315	XGBRegressor(LassoLarsCV(input_matrix, LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [13:27<00:00,  5.55s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [13:27<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [13:28<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [13:28<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [13:29<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [13:29<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [13:29<00:00,  3.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:02:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2200/2200 [13:32<00:00,  3.91s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  96%|█████████▌| 2200/2300 [13:32<06:31,  3.91s/pipeline]Optimization Progress:  96%|█████████▌| 2201/2300 [13:32<07:25,  4.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2201/2300 [13:32<07:25,  4.50s/pipeline]Optimization Progress:  96%|█████████▌| 2203/2300 [14:11<14:33,  9.00s/pipeline]Optimization Progress:  99%|█████████▉| 2283/2300 [14:21<01:47,  6.34s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [14:23<00:00,  6.34s/pipeline]Optimization Progress: 100%|██████████| 2300/2300 [14:23<00:00,  4.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [14:23<00:00,  4.48s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [14:24<00:00,  4.48s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2300/2300 [14:25<00:00,  4.48s/pipeline]Optimization Progress:  96%|█████████▌| 2301/2400 [14:30<08:35,  5.21s/pipeline]Optimization Progress:  96%|█████████▌| 2302/2400 [15:19<29:45, 18.22s/pipeline]Optimization Progress:  99%|█████████▉| 2382/2400 [15:24<03:49, 12.77s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-207143435.58875692	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [15:25<00:00, 12.77s/pipeline]Optimization Progress: 100%|██████████| 2400/2400 [15:25<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:26<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:29<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2400/2400 [15:29<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [15:30<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:04:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [15:31<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:32<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2400/2400 [15:33<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 2400/2400 [15:34<00:00,  8.96s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [15:34<00:00,  8.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2400/2500 [15:35<14:55,  8.96s/pipeline]Optimization Progress:  96%|█████████▌| 2401/2500 [15:35<15:11,  9.21s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  96%|█████████▌| 2401/2500 [15:35<15:11,  9.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2402/2500 [15:35<15:02,  9.21s/pipeline]Optimization Progress:  96%|█████████▌| 2404/2500 [15:42<11:26,  7.15s/pipeline]Optimization Progress:  99%|█████████▉| 2484/2500 [15:46<01:20,  5.02s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:46<00:00,  5.02s/pipeline]Optimization Progress: 100%|██████████| 2500/2500 [15:46<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:47<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:48<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:51<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2500/2500 [15:51<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2500/2500 [15:54<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 2500/2500 [15:54<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [15:55<00:00,  3.52s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2500/2500 [15:55<00:00,  3.52s/pipeline]Optimization Progress:  96%|█████████▋| 2504/2600 [15:57<05:14,  3.28s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  96%|█████████▋| 2504/2600 [15:57<05:14,  3.28s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  96%|█████████▋| 2505/2600 [15:57<05:11,  3.28s/pipeline]Optimization Progress:  96%|█████████▋| 2507/2600 [16:07<05:09,  3.32s/pipeline]Optimization Progress: 100%|█████████▉| 2587/2600 [16:14<00:30,  2.35s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:05:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [16:17<00:00,  2.35s/pipeline]Optimization Progress: 100%|██████████| 2600/2600 [16:17<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [16:18<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2600/2600 [16:18<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 2600/2600 [16:19<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 2600/2600 [16:21<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [16:22<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:05:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [16:22<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 [05:05:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [16:23<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2600/2600 [16:24<00:00,  1.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [16:24<00:00,  1.71s/pipeline]Optimization Progress:  96%|█████████▋| 2603/2700 [16:25<03:12,  1.99s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2603/2700 [16:25<03:12,  1.99s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2604/2700 [16:25<03:10,  1.99s/pipeline]Optimization Progress:  97%|█████████▋| 2606/2700 [16:36<03:54,  2.50s/pipeline]Optimization Progress:  99%|█████████▉| 2686/2700 [16:41<00:24,  1.77s/pipeline]
Generation 26 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.77s/pipeline]Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:42<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:43<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2700/2700 [16:43<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:43<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2700/2700 [16:44<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:45<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [16:48<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:50<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [16:50<00:00,  1.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [16:51<00:00,  1.25s/pipeline]Optimization Progress:  97%|█████████▋| 2703/2800 [16:52<02:58,  1.84s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2703/2800 [16:52<02:58,  1.84s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2704/2800 [16:52<02:57,  1.84s/pipeline]Optimization Progress:  97%|█████████▋| 2706/2800 [17:05<04:09,  2.66s/pipeline]Optimization Progress: 100%|█████████▉| 2786/2800 [17:10<00:26,  1.88s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-190184882.7071221	GradientBoostingRegressor(Nystroem(VarianceThreshold(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), VarianceThreshold__threshold=0.001), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [17:11<00:00,  1.88s/pipeline]Optimization Progress: 100%|██████████| 2800/2800 [17:11<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:12<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:14<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2800/2800 [17:14<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 2800/2800 [17:15<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:06:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [17:15<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 2800/2800 [17:16<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2800/2800 [17:16<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [17:16<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [17:17<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:06:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [17:18<00:00,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 2800/2800 [17:18<00:00,  1.34s/pipeline]Optimization Progress:  97%|█████████▋| 2802/2900 [17:19<03:25,  2.09s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2802/2900 [17:19<03:25,  2.09s/pipeline]Optimization Progress:  97%|█████████▋| 2804/2900 [17:27<04:13,  2.64s/pipeline]Optimization Progress:  99%|█████████▉| 2884/2900 [17:33<00:29,  1.87s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-200090783.2629199	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.55, GradientBoostingRegressor__min_samples_leaf=4, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.8500000000000001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.1)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-190184882.7071221	GradientBoostingRegressor(Nystroem(VarianceThreshold(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), VarianceThreshold__threshold=0.001), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-6	-183635783.52802417	XGBRegressor(LassoLarsCV(XGBRegressor(OneHotEncoder(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.8500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.9)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:33<00:00,  1.87s/pipeline]Optimization Progress: 100%|██████████| 2900/2900 [17:33<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2900/2900 [17:34<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:36<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [17:36<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 2900/2900 [17:36<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:06:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2900/2900 [17:37<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:06:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 2900/2900 [17:37<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:38<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [17:39<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2900/2900 [17:39<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [17:40<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 2900/2900 [17:40<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [17:40<00:00,  1.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [17:40<00:00,  1.31s/pipeline]Optimization Progress:  97%|█████████▋| 2903/3000 [17:50<02:07,  1.31s/pipeline]Optimization Progress:  97%|█████████▋| 2904/3000 [19:16<13:49,  8.64s/pipeline]Optimization Progress:  99%|█████████▉| 2984/3000 [19:19<01:37,  6.06s/pipeline]
Generation 29 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199824704.75834712	GradientBoostingRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-190184882.7071221	GradientBoostingRegressor(Nystroem(VarianceThreshold(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), VarianceThreshold__threshold=0.001), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-6	-183635783.52802417	XGBRegressor(LassoLarsCV(XGBRegressor(OneHotEncoder(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.8500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.9)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3000/3000 [19:21<00:00,  6.06s/pipeline]Optimization Progress: 100%|██████████| 3000/3000 [19:21<00:00,  4.27s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [19:21<00:00,  4.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [19:23<00:00,  4.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 3000/3000 [19:27<00:00,  4.27s/pipeline]Optimization Progress:  97%|█████████▋| 3001/3100 [19:39<14:12,  8.62s/pipeline]Optimization Progress:  99%|█████████▉| 3081/3100 [19:46<01:55,  6.06s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199824704.75834712	GradientBoostingRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-189490959.61078566	XGBRegressor(ZeroCount(ElasticNetCV(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-183635783.52802417	XGBRegressor(LassoLarsCV(XGBRegressor(OneHotEncoder(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.8500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.9)), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 3100/3100 [19:47<00:00,  6.06s/pipeline]Optimization Progress: 100%|██████████| 3100/3100 [19:47<00:00,  4.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [19:47<00:00,  4.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3100/3100 [19:47<00:00,  4.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 3100/3100 [19:51<00:00,  4.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [19:54<00:00,  4.26s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [19:54<00:00,  4.26s/pipeline]Optimization Progress:  97%|█████████▋| 3102/3200 [19:54<06:31,  3.99s/pipeline]Optimization Progress:  97%|█████████▋| 3103/3200 [20:00<07:20,  4.54s/pipeline]Optimization Progress:  99%|█████████▉| 3183/3200 [20:04<00:54,  3.20s/pipeline]
Generation 31 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199824704.75834712	GradientBoostingRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3)
-3	-193685795.05341476	XGBRegressor(LassoLarsCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-189490959.61078566	XGBRegressor(ZeroCount(ElasticNetCV(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-180459749.46813732	XGBRegressor(LassoLarsCV(SelectFromModel(ExtraTreesRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.05, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.55), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=6, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), SelectFromModel__ExtraTreesRegressor__max_features=0.15000000000000002, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.25), LassoLarsCV__normalize=True), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [20:04<00:00,  3.20s/pipeline]Optimization Progress: 100%|██████████| 3200/3200 [20:04<00:00,  2.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:09:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 3200/3200 [20:06<00:00,  2.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [20:08<00:00,  2.24s/pipeline]Optimization Progress:  97%|█████████▋| 3201/3300 [20:20<03:41,  2.24s/pipeline]Optimization Progress:  97%|█████████▋| 3202/3300 [20:23<07:10,  4.40s/pipeline]Optimization Progress:  99%|█████████▉| 3282/3300 [20:27<00:55,  3.09s/pipeline]
Generation 32 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199824704.75834712	GradientBoostingRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.2, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3)
-3	-189490515.4532122	XGBRegressor(OneHotEncoder(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-185478587.87383717	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)
-5	-180459749.46813732	XGBRegressor(LassoLarsCV(SelectFromModel(ExtraTreesRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.05, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.55), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=6, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), SelectFromModel__ExtraTreesRegressor__max_features=0.15000000000000002, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.25), LassoLarsCV__normalize=True), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [20:29<00:00,  3.09s/pipeline]Optimization Progress: 100%|██████████| 3300/3300 [20:29<00:00,  2.20s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:09:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 3300/3300 [20:32<00:00,  2.20s/pipeline]Optimization Progress:  97%|█████████▋| 3302/3400 [20:37<04:24,  2.70s/pipeline]Optimization Progress:  97%|█████████▋| 3303/3400 [21:36<31:51, 19.71s/pipeline]Optimization Progress: 100%|█████████▉| 3383/3400 [21:44<03:55, 13.82s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199170330.21809214	XGBRegressor(CombineDFs(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-189490515.4532122	XGBRegressor(OneHotEncoder(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-185478587.87383717	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)
-5	-178998376.2489589	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [21:47<00:00, 13.82s/pipeline]Optimization Progress: 100%|██████████| 3400/3400 [21:47<00:00,  9.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [21:48<00:00,  9.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 3400/3400 [21:50<00:00,  9.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 3400/3400 [21:51<00:00,  9.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [21:52<00:00,  9.73s/pipeline]Optimization Progress:  97%|█████████▋| 3403/3500 [21:53<11:56,  7.39s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3403/3500 [21:53<11:56,  7.39s/pipeline]Optimization Progress:  97%|█████████▋| 3405/3500 [22:03<10:36,  6.70s/pipeline]Optimization Progress: 100%|█████████▉| 3485/3500 [22:07<01:10,  4.70s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199170330.21809214	XGBRegressor(CombineDFs(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-189490515.4532122	XGBRegressor(OneHotEncoder(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-185478587.87383717	GradientBoostingRegressor(Nystroem(ZeroCount(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.55, RandomForestRegressor__min_samples_leaf=19, RandomForestRegressor__min_samples_split=18, RandomForestRegressor__n_estimators=100)), Nystroem__gamma=0.30000000000000004, Nystroem__kernel=linear, Nystroem__n_components=7), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=1.0, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=10, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=6, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6500000000000001)
-5	-178998376.2489589	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [22:08<00:00,  4.70s/pipeline]Optimization Progress: 100%|██████████| 3500/3500 [22:08<00:00,  3.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [22:09<00:00,  3.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3500/3500 [22:11<00:00,  3.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [22:13<00:00,  3.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3500/3500 [22:13<00:00,  3.32s/pipeline]Optimization Progress:  97%|█████████▋| 3501/3600 [22:21<10:13,  6.20s/pipeline]Optimization Progress:  99%|█████████▉| 3581/3600 [22:25<01:22,  4.35s/pipeline]
Generation 35 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-199170330.21809214	XGBRegressor(CombineDFs(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), input_matrix), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-189490515.4532122	XGBRegressor(OneHotEncoder(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-173468846.02905345	XGBRegressor(LassoLarsCV(ElasticNetCV(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), ElasticNetCV__l1_ratio=0.4, ElasticNetCV__tol=0.01), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [22:25<00:00,  4.35s/pipeline]Optimization Progress: 100%|██████████| 3600/3600 [22:25<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3600/3600 [22:28<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3600/3600 [22:28<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3600/3600 [22:29<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [22:30<00:00,  3.05s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3600/3600 [22:31<00:00,  3.05s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3600/3700 [22:32<05:05,  3.05s/pipeline]Optimization Progress:  97%|█████████▋| 3601/3700 [22:40<05:01,  3.05s/pipeline]Optimization Progress:  97%|█████████▋| 3602/3700 [22:43<07:49,  4.79s/pipeline]Optimization Progress: 100%|█████████▉| 3682/3700 [22:49<01:00,  3.38s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-197709940.5103249	GradientBoostingRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=9, DecisionTreeRegressor__min_samples_leaf=1, DecisionTreeRegressor__min_samples_split=4), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=4, GradientBoostingRegressor__max_features=0.05, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 3700/3700 [22:49<00:00,  3.38s/pipeline]Optimization Progress: 100%|██████████| 3700/3700 [22:49<00:00,  2.37s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [22:54<00:00,  2.37s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:12:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 3700/3700 [22:54<00:00,  2.37s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [22:55<00:00,  2.37s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [22:56<00:00,  2.37s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [22:57<00:00,  2.37s/pipeline]Optimization Progress:  97%|█████████▋| 3700/3800 [23:00<03:56,  2.37s/pipeline]Optimization Progress:  97%|█████████▋| 3701/3800 [23:05<10:54,  6.61s/pipeline]Optimization Progress: 100%|█████████▉| 3781/3800 [23:11<01:28,  4.65s/pipeline]
Generation 37 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-195197235.58475974	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3800/3800 [23:12<00:00,  4.65s/pipeline]Optimization Progress: 100%|██████████| 3800/3800 [23:12<00:00,  3.27s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3800/3800 [23:13<00:00,  3.27s/pipeline]Optimization Progress:  98%|█████████▊| 3804/3900 [23:19<04:30,  2.81s/pipeline]Optimization Progress:  98%|█████████▊| 3805/3900 [23:28<07:23,  4.67s/pipeline]Optimization Progress: 100%|█████████▉| 3885/3900 [23:34<00:49,  3.29s/pipeline]
Generation 38 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [23:35<00:00,  3.29s/pipeline]Optimization Progress: 100%|██████████| 3900/3900 [23:35<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3900/3900 [23:41<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 3900/3900 [23:41<00:00,  2.33s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3900/4000 [23:42<03:52,  2.33s/pipeline]Optimization Progress:  98%|█████████▊| 3901/4000 [23:42<06:15,  3.80s/pipeline]Optimization Progress:  98%|█████████▊| 3902/4000 [23:51<08:26,  5.17s/pipeline]Optimization Progress: 100%|█████████▉| 3982/4000 [23:59<01:05,  3.65s/pipeline]
Generation 39 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:13:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4000/4000 [24:03<00:00,  3.65s/pipeline]Optimization Progress: 100%|██████████| 4000/4000 [24:03<00:00,  2.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4000/4000 [24:04<00:00,  2.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4000/4000 [24:05<00:00,  2.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:13:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4000/4000 [24:06<00:00,  2.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4000/4000 [24:08<00:00,  2.61s/pipeline]Optimization Progress:  98%|█████████▊| 4001/4100 [24:08<05:45,  3.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4001/4100 [24:08<05:45,  3.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4002/4100 [24:08<05:41,  3.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4003/4100 [24:08<05:38,  3.49s/pipeline]Optimization Progress:  98%|█████████▊| 4005/4100 [24:16<04:46,  3.02s/pipeline]Optimization Progress: 100%|█████████▉| 4085/4100 [24:19<00:31,  2.12s/pipeline]
Generation 40 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 100%|██████████| 4100/4100 [24:20<00:00,  2.12s/pipeline]Optimization Progress: 100%|██████████| 4100/4100 [24:20<00:00,  1.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 4100/4100 [24:21<00:00,  1.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 4100/4100 [24:21<00:00,  1.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [24:23<00:00,  1.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4100/4100 [24:25<00:00,  1.51s/pipeline]Optimization Progress:  98%|█████████▊| 4102/4200 [24:26<03:17,  2.01s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4102/4200 [24:26<03:17,  2.01s/pipeline]Optimization Progress:  98%|█████████▊| 4104/4200 [24:58<09:59,  6.25s/pipeline]Optimization Progress: 100%|█████████▉| 4184/4200 [25:03<01:10,  4.39s/pipeline]
Generation 41 - Current Pareto front scores:
-1	-206551718.00264966	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4200/4200 [25:03<00:00,  4.39s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 4200/4200 [25:03<00:00,  4.39s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4200/4200 [25:04<00:00,  4.39s/pipeline]Optimization Progress: 100%|██████████| 4200/4200 [25:04<00:00,  3.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 4200/4200 [25:10<00:00,  3.10s/pipeline]Optimization Progress:  98%|█████████▊| 4201/4300 [25:11<06:51,  4.15s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4201/4300 [25:11<06:51,  4.15s/pipeline]Optimization Progress:  98%|█████████▊| 4203/4300 [25:18<06:30,  4.03s/pipeline]Optimization Progress: 100%|█████████▉| 4283/4300 [25:26<00:48,  2.85s/pipeline]
Generation 42 - Current Pareto front scores:
-1	-206022510.20044953	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4300/4300 [25:27<00:00,  2.85s/pipeline]Optimization Progress: 100%|██████████| 4300/4300 [25:27<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 4300/4300 [25:28<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4300/4300 [25:30<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4300/4300 [25:33<00:00,  2.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:14:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4300/4300 [25:35<00:00,  2.01s/pipeline]Optimization Progress:  98%|█████████▊| 4301/4400 [25:35<06:17,  3.81s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4301/4400 [25:35<06:17,  3.81s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4302/4400 [25:35<06:13,  3.81s/pipeline]Optimization Progress:  98%|█████████▊| 4304/4400 [25:53<07:12,  4.50s/pipeline]Optimization Progress: 100%|█████████▉| 4384/4400 [25:56<00:50,  3.16s/pipeline]
Generation 43 - Current Pareto front scores:
-1	-206022510.20044953	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [25:58<00:00,  3.16s/pipeline]Optimization Progress: 100%|██████████| 4400/4400 [25:58<00:00,  2.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 4400/4400 [25:59<00:00,  2.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4400/4400 [26:02<00:00,  2.25s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4400/4500 [26:05<03:45,  2.25s/pipeline]Optimization Progress:  98%|█████████▊| 4401/4500 [26:05<06:13,  3.77s/pipeline]Optimization Progress:  98%|█████████▊| 4402/4500 [26:12<07:47,  4.78s/pipeline]Optimization Progress: 100%|█████████▉| 4482/4500 [26:15<01:00,  3.35s/pipeline]
Generation 44 - Current Pareto front scores:
-1	-206022510.20044953	GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=9, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=1.0)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 4500/4500 [26:17<00:00,  3.35s/pipeline]Optimization Progress: 100%|██████████| 4500/4500 [26:17<00:00,  2.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4500/4500 [26:19<00:00,  2.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4500/4500 [26:23<00:00,  2.38s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4500/4600 [26:24<03:57,  2.38s/pipeline]Optimization Progress:  98%|█████████▊| 4501/4600 [26:30<03:55,  2.38s/pipeline]Optimization Progress:  98%|█████████▊| 4502/4600 [26:32<06:18,  3.87s/pipeline]Optimization Progress: 100%|█████████▉| 4582/4600 [26:37<00:49,  2.73s/pipeline]
Generation 45 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-112679258.7876288	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4600/4600 [26:39<00:00,  2.73s/pipeline]Optimization Progress: 100%|██████████| 4600/4600 [26:39<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4600/4600 [26:40<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 4600/4600 [26:41<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4600/4600 [26:43<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [26:45<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4600/4600 [26:45<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:16:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4600/4600 [26:46<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 [05:16:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4600/4600 [26:46<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [26:46<00:00,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4600/4700 [26:47<03:14,  1.94s/pipeline]Optimization Progress:  98%|█████████▊| 4601/4700 [26:47<05:59,  3.63s/pipeline]Optimization Progress:  98%|█████████▊| 4602/4700 [27:18<19:29, 11.93s/pipeline]Optimization Progress: 100%|█████████▉| 4682/4700 [27:21<02:30,  8.37s/pipeline]
Generation 46 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-112679258.7876288	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 4700/4700 [27:22<00:00,  8.37s/pipeline]Optimization Progress: 100%|██████████| 4700/4700 [27:22<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4700/4700 [27:22<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4700/4700 [27:22<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4700/4700 [27:23<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4700/4700 [27:23<00:00,  5.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4700/4700 [27:27<00:00,  5.86s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4701/4800 [27:29<09:40,  5.86s/pipeline]Optimization Progress:  98%|█████████▊| 4702/4800 [27:29<08:27,  5.18s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4702/4800 [27:29<08:27,  5.18s/pipeline]Optimization Progress:  98%|█████████▊| 4703/4800 [27:40<08:22,  5.18s/pipeline]Optimization Progress:  98%|█████████▊| 4704/4800 [27:52<11:17,  7.06s/pipeline]Optimization Progress: 100%|█████████▉| 4784/4800 [27:55<01:19,  4.95s/pipeline]
Generation 47 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-112679258.7876288	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4800/4800 [27:55<00:00,  4.95s/pipeline]Optimization Progress: 100%|██████████| 4800/4800 [27:55<00:00,  3.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4800/4800 [28:00<00:00,  3.47s/pipeline]Optimization Progress:  98%|█████████▊| 4801/4900 [28:10<05:43,  3.47s/pipeline]Optimization Progress:  98%|█████████▊| 4802/4900 [28:24<11:07,  6.81s/pipeline]Optimization Progress: 100%|█████████▉| 4882/4900 [28:28<01:26,  4.78s/pipeline]
Generation 48 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-112679258.7876288	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 4900/4900 [28:31<00:00,  4.78s/pipeline]Optimization Progress: 100%|██████████| 4900/4900 [28:31<00:00,  3.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 4900/4900 [28:36<00:00,  3.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:17:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 4900/4900 [28:36<00:00,  3.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4900/4900 [28:36<00:00,  3.38s/pipeline]Optimization Progress:  98%|█████████▊| 4901/5000 [29:07<21:56, 13.29s/pipeline]Optimization Progress: 100%|█████████▉| 4981/5000 [29:15<02:57,  9.33s/pipeline]
Generation 49 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-164844732.21778116	XGBRegressor(LassoLarsCV(XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=1e-05), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-5	-112679258.7876288	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 5000/5000 [29:15<00:00,  9.33s/pipeline]Optimization Progress: 100%|██████████| 5000/5000 [29:15<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:18:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 5000/5000 [29:18<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5000/5000 [29:18<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5000/5000 [29:20<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5000/5000 [29:21<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:18:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 5000/5000 [29:22<00:00,  6.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5000/5000 [29:22<00:00,  6.55s/pipeline]Optimization Progress:  98%|█████████▊| 5001/5100 [29:56<27:23, 16.60s/pipeline]Optimization Progress: 100%|█████████▉| 5081/5100 [29:59<03:41, 11.63s/pipeline]
Generation 50 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 5100/5100 [30:02<00:00, 11.63s/pipeline]Optimization Progress: 100%|██████████| 5100/5100 [30:02<00:00,  8.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5100/5100 [30:03<00:00,  8.19s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [30:06<00:00,  8.19s/pipeline]Optimization Progress:  98%|█████████▊| 5102/5200 [30:08<10:47,  6.61s/pipeline]Optimization Progress:  98%|█████████▊| 5103/5200 [30:22<14:17,  8.84s/pipeline]Optimization Progress: 100%|█████████▉| 5183/5200 [30:27<01:45,  6.21s/pipeline]
Generation 51 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5200/5200 [30:29<00:00,  6.21s/pipeline]Optimization Progress: 100%|██████████| 5200/5200 [30:29<00:00,  4.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5200/5200 [30:36<00:00,  4.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5200/5200 [30:36<00:00,  4.38s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:19:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 5200/5200 [30:37<00:00,  4.38s/pipeline]Optimization Progress:  98%|█████████▊| 5201/5300 [30:37<08:50,  5.36s/pipeline]Optimization Progress:  98%|█████████▊| 5202/5300 [30:44<09:39,  5.92s/pipeline]Optimization Progress: 100%|█████████▉| 5282/5300 [30:47<01:14,  4.16s/pipeline]
Generation 52 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5300/5300 [30:48<00:00,  4.16s/pipeline]Optimization Progress: 100%|██████████| 5300/5300 [30:48<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5300/5300 [30:50<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5300/5300 [30:52<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5300/5300 [30:52<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5300/5300 [30:53<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5300/5300 [30:55<00:00,  2.91s/pipeline]Optimization Progress:  98%|█████████▊| 5300/5400 [31:00<04:51,  2.91s/pipeline]Optimization Progress:  98%|█████████▊| 5301/5400 [31:03<10:47,  6.55s/pipeline]Optimization Progress: 100%|█████████▉| 5381/5400 [31:25<01:28,  4.66s/pipeline]
Generation 53 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)Optimization Progress: 100%|██████████| 5400/5400 [31:35<00:00,  3.43s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5401/5500 [31:35<05:39,  3.43s/pipeline]Optimization Progress:  98%|█████████▊| 5403/5500 [31:59<07:40,  4.74s/pipeline]Optimization Progress: 100%|█████████▉| 5483/5500 [32:01<00:56,  3.33s/pipeline]
Generation 54 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)Optimization Progress: 100%|██████████| 5500/5500 [32:11<00:00,  2.51s/pipeline]Optimization Progress:  98%|█████████▊| 5501/5600 [32:18<05:55,  3.59s/pipeline]Optimization Progress: 100%|█████████▉| 5581/5600 [32:23<00:48,  2.53s/pipeline]
Generation 55 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5600/5600 [32:25<00:00,  2.53s/pipeline]Optimization Progress: 100%|██████████| 5600/5600 [32:25<00:00,  1.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5600/5600 [32:26<00:00,  1.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5600/5600 [32:28<00:00,  1.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5600/5600 [32:29<00:00,  1.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5600/5600 [32:29<00:00,  1.81s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5600/5600 [32:33<00:00,  1.81s/pipeline]Optimization Progress:  98%|█████████▊| 5601/5700 [32:33<06:10,  3.75s/pipeline]Optimization Progress:  98%|█████████▊| 5602/5700 [33:02<18:05, 11.08s/pipeline]Optimization Progress: 100%|█████████▉| 5682/5700 [33:07<02:19,  7.78s/pipeline]
Generation 56 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5700/5700 [33:08<00:00,  7.78s/pipeline]Optimization Progress: 100%|██████████| 5700/5700 [33:08<00:00,  5.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5700/5700 [33:08<00:00,  5.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 5700/5700 [33:11<00:00,  5.46s/pipeline]Optimization Progress:  98%|█████████▊| 5701/5800 [33:18<11:08,  6.75s/pipeline]Optimization Progress:  98%|█████████▊| 5702/5800 [33:46<21:25, 13.11s/pipeline]Optimization Progress: 100%|█████████▉| 5782/5800 [33:50<02:45,  9.20s/pipeline]
Generation 57 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5800/5800 [33:51<00:00,  9.20s/pipeline]Optimization Progress: 100%|██████████| 5800/5800 [33:51<00:00,  6.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5800/5800 [33:56<00:00,  6.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:23:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 5800/5800 [33:56<00:00,  6.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5800/5800 [33:59<00:00,  6.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress: 100%|██████████| 5800/5800 [34:00<00:00,  6.46s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5800/5900 [34:00<10:45,  6.46s/pipeline]Optimization Progress:  98%|█████████▊| 5801/5900 [34:00<11:44,  7.11s/pipeline]Optimization Progress:  98%|█████████▊| 5802/5900 [34:06<11:01,  6.75s/pipeline]Optimization Progress: 100%|█████████▉| 5882/5900 [34:11<01:25,  4.74s/pipeline]
Generation 58 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5900/5900 [34:11<00:00,  4.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5900/5900 [34:11<00:00,  4.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 5900/5900 [34:13<00:00,  4.74s/pipeline]Optimization Progress: 100%|██████████| 5900/5900 [34:13<00:00,  3.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 5900/5900 [34:14<00:00,  3.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5900/5900 [34:15<00:00,  3.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:23:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 5900/5900 [34:15<00:00,  3.36s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5900/5900 [34:17<00:00,  3.36s/pipeline]Optimization Progress:  98%|█████████▊| 5902/6000 [34:21<05:42,  3.49s/pipeline]Optimization Progress:  98%|█████████▊| 5903/6000 [34:28<07:34,  4.68s/pipeline]Optimization Progress: 100%|█████████▉| 5983/6000 [34:32<00:56,  3.29s/pipeline]
Generation 59 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6000/6000 [34:34<00:00,  3.29s/pipeline]Optimization Progress: 100%|██████████| 6000/6000 [34:34<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:23:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 6000/6000 [34:37<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6000/6000 [34:38<00:00,  2.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6000/6000 [34:39<00:00,  2.33s/pipeline]Optimization Progress:  98%|█████████▊| 6001/6100 [34:51<11:27,  6.95s/pipeline]Optimization Progress: 100%|█████████▉| 6081/6100 [34:57<01:32,  4.88s/pipeline]
Generation 60 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6100/6100 [34:58<00:00,  4.88s/pipeline]Optimization Progress: 100%|██████████| 6100/6100 [34:58<00:00,  3.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6100/6100 [35:00<00:00,  3.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6100/6100 [35:02<00:00,  3.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:24:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 6100/6100 [35:03<00:00,  3.43s/pipeline]Optimization Progress:  98%|█████████▊| 6102/6200 [35:07<06:10,  3.78s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6102/6200 [35:07<06:10,  3.78s/pipeline]Optimization Progress:  98%|█████████▊| 6104/6200 [35:28<09:21,  5.85s/pipeline]Optimization Progress: 100%|█████████▉| 6184/6200 [35:33<01:05,  4.11s/pipeline]
Generation 61 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6200/6200 [35:34<00:00,  4.11s/pipeline]Optimization Progress: 100%|██████████| 6200/6200 [35:34<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6200/6200 [35:36<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6200/6200 [35:36<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6200/6200 [35:37<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6200/6200 [35:38<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6200/6200 [35:38<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6200/6200 [35:39<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6200/6200 [35:39<00:00,  2.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6200/6200 [35:39<00:00,  2.90s/pipeline]Optimization Progress:  98%|█████████▊| 6201/6300 [35:42<07:24,  4.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6201/6300 [35:42<07:24,  4.49s/pipeline]Optimization Progress:  98%|█████████▊| 6203/6300 [35:50<06:54,  4.28s/pipeline]Optimization Progress: 100%|█████████▉| 6283/6300 [35:55<00:51,  3.01s/pipeline]
Generation 62 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6300/6300 [35:56<00:00,  3.01s/pipeline]Optimization Progress: 100%|██████████| 6300/6300 [35:56<00:00,  2.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6300/6300 [36:01<00:00,  2.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6300/6300 [36:01<00:00,  2.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6300/6300 [36:02<00:00,  2.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6300/6300 [36:04<00:00,  2.13s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6301/6400 [36:05<03:30,  2.13s/pipeline]Optimization Progress:  98%|█████████▊| 6302/6400 [36:05<04:39,  2.85s/pipeline]Optimization Progress:  98%|█████████▊| 6303/6400 [36:11<06:14,  3.86s/pipeline]Optimization Progress: 100%|█████████▉| 6383/6400 [36:14<00:46,  2.71s/pipeline]
Generation 63 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6400/6400 [36:14<00:00,  2.71s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 6400/6400 [36:14<00:00,  2.71s/pipeline]Optimization Progress: 100%|██████████| 6400/6400 [36:14<00:00,  1.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6400/6400 [36:14<00:00,  1.90s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6400/6500 [36:24<03:10,  1.90s/pipeline]Optimization Progress:  99%|█████████▊| 6405/6500 [36:28<03:28,  2.19s/pipeline]Optimization Progress: 100%|█████████▉| 6482/6500 [36:32<00:27,  1.55s/pipeline]
Generation 64 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 6500/6500 [36:35<00:00,  1.55s/pipeline]Optimization Progress: 100%|██████████| 6500/6500 [36:35<00:00,  1.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6500/6500 [36:37<00:00,  1.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6500/6500 [36:38<00:00,  1.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 6500/6500 [36:39<00:00,  1.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6500/6500 [36:39<00:00,  1.13s/pipeline]Optimization Progress:  98%|█████████▊| 6501/6600 [36:41<04:36,  2.79s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6501/6600 [36:41<04:36,  2.79s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6502/6600 [36:41<04:33,  2.79s/pipeline]Optimization Progress:  99%|█████████▊| 6504/6600 [37:04<06:46,  4.23s/pipeline]Optimization Progress: 100%|█████████▉| 6584/6600 [37:09<00:47,  2.98s/pipeline]
Generation 65 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6600/6600 [37:10<00:00,  2.98s/pipeline]Optimization Progress: 100%|██████████| 6600/6600 [37:10<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6600/6600 [37:11<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6600/6600 [37:15<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:26:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 6600/6600 [37:15<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6600/6600 [37:17<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 6600/6600 [37:17<00:00,  2.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:26:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 6600/6600 [37:18<00:00,  2.11s/pipeline]Optimization Progress:  99%|█████████▊| 6602/6700 [37:18<04:25,  2.71s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6602/6700 [37:18<04:25,  2.71s/pipeline]Optimization Progress:  99%|█████████▊| 6604/6700 [37:24<04:17,  2.68s/pipeline]Optimization Progress: 100%|█████████▉| 6684/6700 [37:27<00:30,  1.89s/pipeline]
Generation 66 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [37:28<00:00,  1.89s/pipeline]Optimization Progress: 100%|██████████| 6700/6700 [37:28<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 6700/6700 [37:31<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6700/6700 [37:31<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6700/6700 [37:31<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6700/6700 [37:32<00:00,  1.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 6700/6700 [37:35<00:00,  1.33s/pipeline]Optimization Progress:  99%|█████████▊| 6701/6800 [37:40<02:11,  1.33s/pipeline]Optimization Progress:  99%|█████████▊| 6702/6800 [38:00<09:26,  5.78s/pipeline]Optimization Progress: 100%|█████████▉| 6782/6800 [38:03<01:13,  4.06s/pipeline]
Generation 67 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6800/6800 [38:06<00:00,  4.06s/pipeline]Optimization Progress: 100%|██████████| 6800/6800 [38:06<00:00,  2.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 6800/6800 [38:06<00:00,  2.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6800/6800 [38:07<00:00,  2.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:27:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 6800/6800 [38:10<00:00,  2.89s/pipeline]Optimization Progress:  99%|█████████▊| 6801/6900 [38:20<09:59,  6.06s/pipeline]Optimization Progress: 100%|█████████▉| 6881/6900 [38:24<01:20,  4.26s/pipeline]
Generation 68 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6900/6900 [38:30<00:00,  4.26s/pipeline]Optimization Progress: 100%|██████████| 6900/6900 [38:30<00:00,  3.06s/pipeline]Optimization Progress:  99%|█████████▊| 6902/7000 [38:34<04:26,  2.71s/pipeline]Optimization Progress:  99%|█████████▊| 6903/7000 [38:39<05:35,  3.46s/pipeline]Optimization Progress: 100%|█████████▉| 6983/7000 [38:45<00:41,  2.45s/pipeline]
Generation 69 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-180462251.24147347	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7000/7000 [38:46<00:00,  2.45s/pipeline]Optimization Progress: 100%|██████████| 7000/7000 [38:46<00:00,  1.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7000/7000 [38:49<00:00,  1.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 7000/7000 [38:51<00:00,  1.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7000/7000 [38:56<00:00,  1.73s/pipeline]Optimization Progress:  99%|█████████▊| 7001/7100 [38:56<06:56,  4.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7001/7100 [38:56<06:56,  4.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7002/7100 [38:56<06:52,  4.21s/pipeline]Optimization Progress:  99%|█████████▊| 7004/7100 [39:08<06:37,  4.14s/pipeline]Optimization Progress: 100%|█████████▉| 7084/7100 [39:13<00:46,  2.91s/pipeline]
Generation 70 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177773461.5263313	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:28:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 7100/7100 [39:14<00:00,  2.91s/pipeline]Optimization Progress: 100%|██████████| 7100/7100 [39:14<00:00,  2.07s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.
Optimization Progress: 100%|██████████| 7100/7100 [39:21<00:00,  2.07s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [39:21<00:00,  2.07s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:28:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 7100/7100 [39:21<00:00,  2.07s/pipeline]Optimization Progress:  99%|█████████▊| 7101/7200 [39:23<06:31,  3.96s/pipeline]Optimization Progress:  99%|█████████▊| 7102/7200 [40:27<35:56, 22.00s/pipeline]Optimization Progress: 100%|█████████▉| 7182/7200 [40:32<04:37, 15.42s/pipeline]
Generation 71 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 7200/7200 [40:34<00:00, 15.42s/pipeline]Optimization Progress: 100%|██████████| 7200/7200 [40:34<00:00, 10.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7200/7200 [40:36<00:00, 10.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7200/7200 [40:38<00:00, 10.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7200/7200 [40:41<00:00, 10.84s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 7200/7200 [40:41<00:00, 10.84s/pipeline]Optimization Progress:  99%|█████████▊| 7201/7300 [40:42<16:33, 10.03s/pipeline]Optimization Progress:  99%|█████████▊| 7202/7300 [40:49<14:48,  9.06s/pipeline]Optimization Progress: 100%|█████████▉| 7282/7300 [40:54<01:54,  6.36s/pipeline]
Generation 72 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7300/7300 [40:59<00:00,  6.36s/pipeline]Optimization Progress: 100%|██████████| 7300/7300 [40:59<00:00,  4.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 7300/7300 [40:59<00:00,  4.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 7300/7300 [41:00<00:00,  4.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7300/7300 [41:02<00:00,  4.54s/pipeline]Optimization Progress:  99%|█████████▊| 7301/7400 [41:15<13:00,  7.88s/pipeline]Optimization Progress: 100%|█████████▉| 7381/7400 [41:22<01:45,  5.55s/pipeline]
Generation 73 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7400/7400 [41:23<00:00,  5.55s/pipeline]Optimization Progress: 100%|██████████| 7400/7400 [41:23<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7400/7400 [41:24<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7400/7400 [41:28<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7400/7400 [41:29<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 7400/7400 [41:32<00:00,  3.90s/pipeline]Optimization Progress:  99%|█████████▊| 7401/7500 [41:41<13:19,  8.08s/pipeline]Optimization Progress: 100%|█████████▉| 7481/7500 [41:47<01:47,  5.67s/pipeline]
Generation 74 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7500/7500 [41:48<00:00,  5.67s/pipeline]Optimization Progress: 100%|██████████| 7500/7500 [41:48<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7500/7500 [41:48<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7500/7500 [41:51<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7500/7500 [41:54<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7500/7500 [41:57<00:00,  3.99s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7500/7600 [41:58<06:39,  3.99s/pipeline]Optimization Progress:  99%|█████████▊| 7501/7600 [41:58<09:28,  5.74s/pipeline]Optimization Progress:  99%|█████████▊| 7502/7600 [42:06<10:36,  6.49s/pipeline]Optimization Progress: 100%|█████████▉| 7582/7600 [42:11<01:22,  4.56s/pipeline]
Generation 75 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 7600/7600 [42:16<00:00,  4.56s/pipeline]Optimization Progress: 100%|██████████| 7600/7600 [42:16<00:00,  3.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7600/7600 [42:17<00:00,  3.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7600/7600 [42:18<00:00,  3.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7600/7600 [42:22<00:00,  3.28s/pipeline]Optimization Progress:  99%|█████████▊| 7602/7700 [42:23<05:17,  3.24s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7602/7700 [42:23<05:17,  3.24s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7603/7700 [42:23<05:14,  3.24s/pipeline]Optimization Progress:  99%|█████████▉| 7605/7700 [42:37<05:57,  3.76s/pipeline]Optimization Progress: 100%|█████████▉| 7685/7700 [43:04<00:41,  2.73s/pipeline]
Generation 76 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 7700/7700 [43:07<00:00,  2.73s/pipeline]Optimization Progress: 100%|██████████| 7700/7700 [43:07<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7700/7700 [43:12<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7700/7700 [43:13<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7700/7700 [43:14<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 7700/7700 [43:14<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 7700/7700 [43:14<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:32:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 7700/7700 [43:15<00:00,  1.96s/pipeline]Optimization Progress:  99%|█████████▊| 7701/7800 [43:15<06:37,  4.02s/pipeline]Optimization Progress:  99%|█████████▊| 7702/7800 [43:22<07:54,  4.84s/pipeline]Optimization Progress: 100%|█████████▉| 7782/7800 [43:27<01:01,  3.41s/pipeline]
Generation 77 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 7800/7800 [43:30<00:00,  3.41s/pipeline]Optimization Progress: 100%|██████████| 7800/7800 [43:30<00:00,  2.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7800/7800 [43:32<00:00,  2.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7800/7800 [43:34<00:00,  2.43s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7800/7800 [43:35<00:00,  2.43s/pipeline]Optimization Progress:  99%|█████████▊| 7801/7900 [43:47<11:28,  6.95s/pipeline]Optimization Progress: 100%|█████████▉| 7881/7900 [43:53<01:32,  4.89s/pipeline]
Generation 78 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 7900/7900 [43:55<00:00,  4.89s/pipeline]Optimization Progress: 100%|██████████| 7900/7900 [43:55<00:00,  3.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7900/7900 [43:55<00:00,  3.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 7900/7900 [44:02<00:00,  3.45s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 7900/7900 [44:03<00:00,  3.45s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7900/8000 [44:03<05:44,  3.45s/pipeline]Optimization Progress:  99%|█████████▉| 7901/8000 [44:03<08:19,  5.04s/pipeline]Optimization Progress:  99%|█████████▉| 7902/8000 [44:10<08:46,  5.37s/pipeline]Optimization Progress: 100%|█████████▉| 7982/8000 [44:14<01:07,  3.77s/pipeline]
Generation 79 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8000/8000 [44:15<00:00,  3.77s/pipeline]Optimization Progress: 100%|██████████| 8000/8000 [44:15<00:00,  2.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8000/8000 [44:17<00:00,  2.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8000/8000 [44:18<00:00,  2.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 8000/8000 [44:19<00:00,  2.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:33:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 8000/8000 [44:20<00:00,  2.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8000/8000 [44:22<00:00,  2.66s/pipeline]Optimization Progress:  99%|█████████▉| 8002/8100 [44:24<05:21,  3.28s/pipeline]Optimization Progress:  99%|█████████▉| 8003/8100 [45:07<24:28, 15.14s/pipeline]Optimization Progress: 100%|█████████▉| 8083/8100 [45:10<03:00, 10.61s/pipeline]
Generation 80 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [45:13<00:00, 10.61s/pipeline]Optimization Progress: 100%|██████████| 8100/8100 [45:13<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 8100/8100 [45:15<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:34:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 8100/8100 [45:17<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8100/8100 [45:17<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 8100/8100 [45:18<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [45:18<00:00,  7.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:34:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 8100/8100 [45:20<00:00,  7.47s/pipeline]Optimization Progress:  99%|█████████▉| 8101/8200 [45:20<12:26,  7.54s/pipeline]Optimization Progress:  99%|█████████▉| 8102/8200 [45:26<11:18,  6.92s/pipeline]Optimization Progress: 100%|█████████▉| 8182/8200 [45:29<01:27,  4.86s/pipeline]
Generation 81 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [45:29<00:00,  4.86s/pipeline]Optimization Progress: 100%|██████████| 8200/8200 [45:29<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8200/8200 [45:29<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 8200/8200 [45:32<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8200/8200 [45:33<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8200/8200 [45:33<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8200/8200 [45:36<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 8200/8200 [45:36<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 8200/8200 [45:36<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8200/8200 [45:36<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8200/8200 [45:37<00:00,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8200/8200 [45:38<00:00,  3.41s/pipeline]Optimization Progress:  99%|█████████▉| 8200/8300 [45:40<05:40,  3.41s/pipeline]Optimization Progress:  99%|█████████▉| 8201/8300 [45:45<11:37,  7.04s/pipeline]Optimization Progress: 100%|█████████▉| 8281/8300 [45:48<01:33,  4.94s/pipeline]
Generation 82 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [45:54<00:00,  4.94s/pipeline]Optimization Progress: 100%|██████████| 8300/8300 [45:54<00:00,  3.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 8300/8300 [45:59<00:00,  3.54s/pipeline]Optimization Progress:  99%|█████████▉| 8301/8400 [46:11<12:53,  7.82s/pipeline]Optimization Progress: 100%|█████████▉| 8381/8400 [46:35<01:45,  5.56s/pipeline]
Generation 83 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [46:36<00:00,  5.56s/pipeline]Optimization Progress: 100%|██████████| 8400/8400 [46:36<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8400/8400 [46:38<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [46:44<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8400/8400 [46:45<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8400/8400 [46:45<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [46:45<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8400/8400 [46:45<00:00,  3.90s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [46:45<00:00,  3.90s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8400/8500 [46:46<06:30,  3.90s/pipeline]Optimization Progress:  99%|█████████▉| 8401/8500 [46:46<09:37,  5.83s/pipeline]Optimization Progress:  99%|█████████▉| 8402/8500 [46:52<09:32,  5.84s/pipeline]Optimization Progress: 100%|█████████▉| 8482/8500 [46:56<01:13,  4.10s/pipeline]
Generation 84 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 8500/8500 [46:58<00:00,  4.10s/pipeline]Optimization Progress: 100%|██████████| 8500/8500 [46:58<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [46:58<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8500/8500 [47:01<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8500/8500 [47:02<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:36:24] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 8500/8500 [47:03<00:00,  2.91s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:36:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 8500/8500 [47:04<00:00,  2.91s/pipeline]Optimization Progress:  99%|█████████▉| 8501/8600 [47:13<10:57,  6.64s/pipeline]Optimization Progress: 100%|█████████▉| 8581/8600 [47:18<01:28,  4.67s/pipeline]
Generation 85 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-177109214.66541865	XGBRegressor(ElasticNetCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=20, ExtraTreesRegressor__min_samples_split=9, ExtraTreesRegressor__n_estimators=100), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-112671652.5809646	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=6), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8600/8600 [47:20<00:00,  4.67s/pipeline]Optimization Progress: 100%|██████████| 8600/8600 [47:20<00:00,  3.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 8600/8600 [47:23<00:00,  3.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 8600/8600 [47:24<00:00,  3.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8600/8600 [47:25<00:00,  3.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8600/8600 [47:28<00:00,  3.30s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8600/8600 [47:28<00:00,  3.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8600/8700 [47:29<05:29,  3.30s/pipeline]Optimization Progress:  99%|█████████▉| 8601/8700 [47:29<08:04,  4.89s/pipeline]Optimization Progress:  99%|█████████▉| 8602/8700 [47:37<09:41,  5.94s/pipeline]Optimization Progress: 100%|█████████▉| 8682/8700 [47:43<01:15,  4.18s/pipeline]
Generation 86 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-112671652.5809646	XGBRegressor(LassoLarsCV(CombineDFs(ZeroCount(XGBRegressor(SelectFromModel(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=7, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=6), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), input_matrix), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 8700/8700 [47:44<00:00,  4.18s/pipeline]Optimization Progress: 100%|██████████| 8700/8700 [47:44<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 8700/8700 [47:46<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8700/8700 [47:47<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 8700/8700 [47:48<00:00,  2.95s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [47:50<00:00,  2.95s/pipeline]Optimization Progress:  99%|█████████▉| 8701/8800 [48:01<11:45,  7.12s/pipeline]Optimization Progress: 100%|█████████▉| 8781/8800 [48:06<01:35,  5.00s/pipeline]
Generation 87 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8800/8800 [48:08<00:00,  5.00s/pipeline]Optimization Progress: 100%|██████████| 8800/8800 [48:08<00:00,  3.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 8800/8800 [48:11<00:00,  3.54s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8800/8800 [48:11<00:00,  3.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8800/8800 [48:11<00:00,  3.54s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8800/8800 [48:16<00:00,  3.54s/pipeline]Optimization Progress:  99%|█████████▉| 8801/8900 [48:17<08:41,  5.27s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8801/8900 [48:17<08:41,  5.27s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8802/8900 [48:17<08:36,  5.27s/pipeline]Optimization Progress:  99%|█████████▉| 8804/8900 [48:41<09:37,  6.02s/pipeline]Optimization Progress: 100%|█████████▉| 8884/8900 [49:04<01:08,  4.30s/pipeline]
Generation 88 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8900/8900 [49:04<00:00,  4.30s/pipeline]Optimization Progress: 100%|██████████| 8900/8900 [49:04<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8900/8900 [49:05<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 8900/8900 [49:05<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 8900/8900 [49:10<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8900/8900 [49:10<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [49:11<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8900/8900 [49:12<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 8900/8900 [49:12<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8900/8900 [49:12<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 8900/8900 [49:12<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 8900/8900 [49:12<00:00,  3.02s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [49:13<00:00,  3.02s/pipeline]Optimization Progress:  99%|█████████▉| 8900/9000 [49:20<05:01,  3.02s/pipeline]Optimization Progress:  99%|█████████▉| 8901/9000 [49:21<11:50,  7.18s/pipeline]Optimization Progress: 100%|█████████▉| 8981/9000 [49:24<01:35,  5.04s/pipeline]
Generation 89 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:38:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9000/9000 [49:26<00:00,  5.04s/pipeline]Optimization Progress: 100%|██████████| 9000/9000 [49:26<00:00,  3.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 9000/9000 [49:26<00:00,  3.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9000/9000 [49:27<00:00,  3.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9000/9000 [49:30<00:00,  3.55s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:38:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9000/9000 [49:32<00:00,  3.55s/pipeline]Optimization Progress:  99%|█████████▉| 9001/9100 [49:39<10:48,  6.55s/pipeline]Optimization Progress: 100%|█████████▉| 9081/9100 [49:47<01:27,  4.62s/pipeline]
Generation 90 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9100/9100 [49:48<00:00,  4.62s/pipeline]Optimization Progress: 100%|██████████| 9100/9100 [49:48<00:00,  3.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9100/9100 [49:50<00:00,  3.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:39:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9100/9100 [49:51<00:00,  3.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 9100/9100 [49:54<00:00,  3.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9100/9100 [49:56<00:00,  3.25s/pipeline]Optimization Progress:  99%|█████████▉| 9101/9200 [49:56<08:03,  4.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9101/9200 [49:56<08:03,  4.89s/pipeline]Optimization Progress:  99%|█████████▉| 9103/9200 [50:03<07:10,  4.44s/pipeline]Optimization Progress: 100%|█████████▉| 9183/9200 [50:06<00:52,  3.12s/pipeline]
Generation 91 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 9200/9200 [50:06<00:00,  3.12s/pipeline]Optimization Progress: 100%|██████████| 9200/9200 [50:06<00:00,  2.18s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9200/9200 [50:08<00:00,  2.18s/pipeline]Optimization Progress:  99%|█████████▉| 9201/9300 [50:20<03:36,  2.18s/pipeline]Optimization Progress:  99%|█████████▉| 9202/9300 [50:27<07:41,  4.70s/pipeline]Optimization Progress: 100%|█████████▉| 9282/9300 [50:32<00:59,  3.31s/pipeline]
Generation 92 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 9300/9300 [50:32<00:00,  3.31s/pipeline]Optimization Progress: 100%|██████████| 9300/9300 [50:32<00:00,  2.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 9300/9300 [50:33<00:00,  2.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 9300/9300 [50:36<00:00,  2.32s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9300/9300 [50:38<00:00,  2.32s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9301/9400 [50:42<03:49,  2.32s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9302/9400 [50:42<03:47,  2.32s/pipeline]Optimization Progress:  99%|█████████▉| 9304/9400 [50:49<04:38,  2.90s/pipeline]Optimization Progress: 100%|█████████▉| 9384/9400 [50:52<00:32,  2.04s/pipeline]
Generation 93 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9400/9400 [50:56<00:00,  2.04s/pipeline]Optimization Progress: 100%|██████████| 9400/9400 [50:56<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [50:58<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 9400/9400 [50:59<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9400/9400 [50:59<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9400/9400 [51:00<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [51:00<00:00,  1.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:40:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [51:01<00:00,  1.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9400/9500 [51:02<02:30,  1.50s/pipeline]Optimization Progress:  99%|█████████▉| 9401/9500 [51:02<04:31,  2.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9401/9500 [51:02<04:31,  2.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9402/9500 [51:02<04:28,  2.74s/pipeline]Optimization Progress:  99%|█████████▉| 9404/9500 [51:31<07:46,  4.86s/pipeline]Optimization Progress: 100%|█████████▉| 9484/9500 [51:34<00:54,  3.41s/pipeline]
Generation 94 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 9500/9500 [51:35<00:00,  3.41s/pipeline]Optimization Progress: 100%|██████████| 9500/9500 [51:35<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 9500/9500 [51:38<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9500/9500 [51:38<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:40:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9500/9500 [51:38<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 9500/9500 [51:42<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 9500/9500 [51:43<00:00,  2.40s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [51:43<00:00,  2.40s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9501/9600 [51:44<03:57,  2.40s/pipeline]Optimization Progress:  99%|█████████▉| 9502/9600 [51:44<05:07,  3.14s/pipeline]Optimization Progress:  99%|█████████▉| 9503/9600 [51:50<06:11,  3.83s/pipeline]Optimization Progress: 100%|█████████▉| 9583/9600 [51:53<00:45,  2.69s/pipeline]
Generation 95 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 9600/9600 [51:56<00:00,  2.69s/pipeline]Optimization Progress: 100%|██████████| 9600/9600 [51:56<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 9600/9600 [51:56<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:41:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9600/9600 [51:56<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9600/9600 [52:00<00:00,  1.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 9600/9600 [52:02<00:00,  1.94s/pipeline]Optimization Progress:  99%|█████████▉| 9601/9700 [52:10<03:11,  1.94s/pipeline]Optimization Progress:  99%|█████████▉| 9602/9700 [52:10<05:44,  3.52s/pipeline]Optimization Progress: 100%|█████████▉| 9682/9700 [52:13<00:44,  2.47s/pipeline]
Generation 96 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [52:15<00:00,  2.47s/pipeline]Optimization Progress: 100%|██████████| 9700/9700 [52:15<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [52:19<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 9700/9700 [52:20<00:00,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9700/9700 [52:21<00:00,  1.76s/pipeline]Optimization Progress:  99%|█████████▉| 9701/9800 [52:23<06:08,  3.72s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9701/9800 [52:23<06:08,  3.72s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9702/9800 [52:23<06:04,  3.72s/pipeline]Optimization Progress:  99%|█████████▉| 9704/9800 [52:47<08:02,  5.03s/pipeline]Optimization Progress: 100%|█████████▉| 9784/9800 [52:52<00:56,  3.54s/pipeline]
Generation 97 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9800/9800 [52:53<00:00,  3.54s/pipeline]Optimization Progress: 100%|██████████| 9800/9800 [52:53<00:00,  2.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [52:57<00:00,  2.49s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 9800/9800 [52:57<00:00,  2.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9801/9900 [53:01<04:06,  2.49s/pipeline]Optimization Progress:  99%|█████████▉| 9802/9900 [53:01<04:56,  3.02s/pipeline]Optimization Progress:  99%|█████████▉| 9803/9900 [53:08<06:53,  4.26s/pipeline]Optimization Progress: 100%|█████████▉| 9883/9900 [53:11<00:50,  2.99s/pipeline]
Generation 98 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [05:42:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [53:12<00:00,  2.99s/pipeline]Optimization Progress: 100%|██████████| 9900/9900 [53:12<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:42:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [53:12<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9900/9900 [53:12<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [53:16<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9900/9900 [53:19<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 9900/9900 [53:20<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 9900/9900 [53:21<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [05:42:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [53:21<00:00,  2.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9900/9900 [53:21<00:00,  2.10s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9900/10000 [53:22<03:30,  2.10s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9901/10000 [53:22<03:28,  2.10s/pipeline]Optimization Progress:  99%|█████████▉| 9903/10000 [53:28<05:00,  3.09s/pipeline]Optimization Progress: 100%|█████████▉| 9983/10000 [53:34<00:37,  2.19s/pipeline]
Generation 99 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 10000/10000 [53:34<00:00,  2.19s/pipeline]Optimization Progress: 100%|██████████| 10000/10000 [53:34<00:00,  1.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10000/10000 [53:34<00:00,  1.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 10000/10000 [53:39<00:00,  1.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [05:43:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f905de7edc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f905df8f669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f905df9cf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f905df83cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f905de70f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f866bc6b9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f866bc6b067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f866bc8327e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f866bc83cb4]

.
Optimization Progress: 100%|██████████| 10000/10000 [53:39<00:00,  1.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10000/10000 [53:42<00:00,  1.54s/pipeline]Optimization Progress:  99%|█████████▉| 10003/10100 [53:49<04:09,  2.57s/pipeline]Optimization Progress: 100%|█████████▉| 10081/10100 [53:54<00:34,  1.82s/pipeline]
Generation 100 - Current Pareto front scores:
-1	-203940578.62053567	GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=3, GradientBoostingRegressor__max_features=0.5, GradientBoostingRegressor__min_samples_leaf=10, GradientBoostingRegressor__min_samples_split=18, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001)
-2	-189490515.4532122	XGBRegressor(ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-176957732.50083432	XGBRegressor(CombineDFs(ElasticNetCV(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=5, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=17, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), ElasticNetCV__l1_ratio=0.8500000000000001, ElasticNetCV__tol=0.001), CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-112679258.7876288	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-6	-109476935.99646547	XGBRegressor(LassoLarsCV(ZeroCount(XGBRegressor(SelectFromModel(CombineDFs(input_matrix, RobustScaler(input_matrix)), SelectFromModel__ExtraTreesRegressor__max_features=0.9500000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.5), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)), LassoLarsCV__normalize=False), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                  Best pipeline:
0. FeatureUnion(transformer_list=[('functiontransformer',
                                FunctionTransformer(func=<function copy at 0x7f8665b5db00>)),
                               ('robustscaler', RobustScaler())])
1. SelectFromModel(estimator=ExtraTreesRegressor(max_features=0.9500000000000001),
                threshold=0.5)
2. StackingEstimator(estimator=XGBRegressor(base_score=0.5, booster='gbtree',
                                         colsample_bylevel=1,
                                         colsample_bynode=1, colsample_bytree=1,
                                         gamma=0, gpu_id=-1,
                                         importance_type='gain',
                                         interaction_constraints='',
                                         learning_rate=0.1, max_delta_step=0,
                                         max_depth=4, min_child_weight=1,
                                         missing=nan, monotone_constraints='()',
                                         n_estimators=100, n_jobs=1, nthread=1,
                                         num_parallel_tree=1, random_state=0,
                                         reg_alpha=0, reg_lambda=1,
                                         scale_pos_weight=1, subsample=1.0,
                                         tree_method='exact',
                                         validate_parameters=1,
                                         verbosity=None))
3. ZeroCount()
4. StackingEstimator(estimator=LassoLarsCV(normalize=False))
5. XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=1.0, max_delta_step=0, max_depth=4,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=1, nthread=1, num_parallel_tree=1,
             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
             subsample=0.05, tree_method='exact', validate_parameters=1,
             verbosity=None)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
