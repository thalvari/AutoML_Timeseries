30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   8%|▊         | 8/100 [00:27<05:12,  3.40s/pipeline]Optimization Progress:  88%|████████▊ | 88/100 [00:29<00:28,  2.39s/pipeline]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:30<00:00,  2.39s/pipeline]Optimization Progress: 100%|██████████| 100/100 [00:30<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 100/100 [00:31<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:32<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:32<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:33<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:33<00:00,  1.69s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 100/100 [00:34<00:00,  1.69s/pipeline]Optimization Progress:  52%|█████▏    | 103/200 [00:35<02:46,  1.72s/pipeline]Optimization Progress:  52%|█████▏    | 104/200 [02:43<1:03:09, 39.48s/pipeline]Optimization Progress:  92%|█████████▏| 184/200 [02:49<07:22, 27.66s/pipeline]  
Generation 1 - Current Pareto front scores:
-1	-452907147.9505083	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.8)
-2	-416924686.2900227	XGBRegressor(RBFSampler(input_matrix, RBFSampler__gamma=0.4), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 200/200 [02:49<00:00, 27.66s/pipeline]Optimization Progress: 100%|██████████| 200/200 [02:49<00:00, 19.36s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 200/200 [02:51<00:00, 19.36s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [02:55<00:00, 19.36s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [02:56<00:00, 19.36s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [02:56<00:00, 19.36s/pipeline]Optimization Progress:  68%|██████▊   | 205/300 [02:56<22:06, 13.96s/pipeline]Optimization Progress:  68%|██████▊   | 205/300 [03:10<22:06, 13.96s/pipeline]Optimization Progress:  69%|██████▊   | 206/300 [03:44<37:49, 24.15s/pipeline]Optimization Progress:  95%|█████████▌| 286/300 [03:47<03:56, 16.91s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-374087900.9479412	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 300/300 [03:49<00:00, 16.91s/pipeline]Optimization Progress: 100%|██████████| 300/300 [03:49<00:00, 11.88s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 300/300 [03:53<00:00, 11.88s/pipeline]Optimization Progress:  77%|███████▋  | 308/400 [03:54<13:03,  8.52s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  77%|███████▋  | 308/400 [03:54<13:03,  8.52s/pipeline]Optimization Progress:  78%|███████▊  | 310/400 [04:43<19:53, 13.26s/pipeline]Optimization Progress:  98%|█████████▊| 390/400 [04:45<01:32,  9.29s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-357491029.06223553	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 400/400 [04:46<00:00,  9.29s/pipeline]Optimization Progress: 100%|██████████| 400/400 [04:46<00:00,  6.52s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [04:47<00:00,  6.52s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 400/400 [04:50<00:00,  6.52s/pipeline]Optimization Progress:  80%|████████  | 402/500 [04:50<08:31,  5.22s/pipeline]Optimization Progress:  81%|████████  | 403/500 [05:40<29:59, 18.55s/pipeline]Optimization Progress:  97%|█████████▋| 483/500 [06:07<03:42, 13.09s/pipeline]
Generation 4 - Current Pareto front scores:
-1	-357491029.06223553	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.3)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 500/500 [06:08<00:00, 13.09s/pipeline]Optimization Progress: 100%|██████████| 500/500 [06:08<00:00,  9.16s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:47:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 500/500 [06:08<00:00,  9.16s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [06:10<00:00,  9.16s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:47:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 500/500 [06:11<00:00,  9.16s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 500/500 [06:11<00:00,  9.16s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [06:11<00:00,  9.16s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  83%|████████▎ | 500/600 [06:12<15:16,  9.16s/pipeline]Optimization Progress:  84%|████████▎ | 501/600 [06:20<15:07,  9.16s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [07:03<24:03, 14.73s/pipeline]Optimization Progress:  97%|█████████▋| 582/600 [07:09<03:05, 10.33s/pipeline]
Generation 5 - Current Pareto front scores:
-1	-357231728.8307681	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [07:10<00:00, 10.33s/pipeline]Optimization Progress: 100%|██████████| 600/600 [07:10<00:00,  7.24s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [07:11<00:00,  7.24s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 600/600 [07:13<00:00,  7.24s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 600/600 [07:13<00:00,  7.24s/pipeline]Optimization Progress:  86%|████████▌ | 601/700 [07:14<10:30,  6.36s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [08:20<39:18, 24.06s/pipeline]Optimization Progress:  97%|█████████▋| 682/700 [08:34<05:04, 16.90s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-357231728.8307681	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:49:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 700/700 [08:34<00:00, 16.90s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [08:34<00:00, 16.90s/pipeline]Optimization Progress: 100%|██████████| 700/700 [08:34<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [08:34<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [08:34<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [08:36<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [08:36<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [08:37<00:00, 11.83s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [08:38<00:00, 11.83s/pipeline]Optimization Progress:  88%|████████▊ | 705/800 [08:38<13:28,  8.51s/pipeline]Optimization Progress:  88%|████████▊ | 705/800 [08:50<13:28,  8.51s/pipeline]Optimization Progress:  88%|████████▊ | 706/800 [12:00<1:44:10, 66.50s/pipeline]Optimization Progress:  98%|█████████▊| 786/800 [12:12<10:52, 46.59s/pipeline]  
Generation 7 - Current Pareto front scores:
-1	-357231728.8307681	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-357160501.7600676	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:53:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 800/800 [12:12<00:00, 46.59s/pipeline]Optimization Progress: 100%|██████████| 800/800 [12:12<00:00, 32.62s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:53:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 800/800 [12:12<00:00, 32.62s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [12:13<00:00, 32.62s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 800/800 [12:14<00:00, 32.62s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [06:53:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 800/800 [12:15<00:00, 32.62s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 800/800 [12:15<00:00, 32.62s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [12:16<38:10, 23.37s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [12:30<38:10, 23.37s/pipeline]Optimization Progress:  89%|████████▉ | 803/900 [13:14<54:46, 33.88s/pipeline]Optimization Progress:  98%|█████████▊| 883/900 [13:35<06:44, 23.80s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-357231728.8307681	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-2	-357160501.7600676	XGBRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=14, RandomForestRegressor__min_samples_split=14, RandomForestRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [13:36<00:00, 23.80s/pipeline]Optimization Progress: 100%|██████████| 900/900 [13:36<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 900/900 [13:36<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [13:37<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 900/900 [13:37<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [13:38<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 900/900 [13:38<00:00, 16.67s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [13:38<00:00, 16.67s/pipeline]Optimization Progress:  90%|█████████ | 901/1000 [14:04<33:01, 20.02s/pipeline]Optimization Progress:  98%|█████████▊| 981/1000 [14:25<04:27, 14.09s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-354403778.0731017	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 [06:55:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [14:26<00:00, 14.09s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [14:26<00:00,  9.88s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 1000/1000 [14:26<00:00,  9.88s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1000/1000 [14:27<00:00,  9.88s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1000/1000 [14:29<00:00,  9.88s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:55:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1000/1000 [14:29<00:00,  9.88s/pipeline]Optimization Progress:  91%|█████████▏| 1004/1100 [14:29<11:29,  7.18s/pipeline]Optimization Progress:  91%|█████████▏| 1005/1100 [14:55<20:17, 12.82s/pipeline]Optimization Progress:  99%|█████████▊| 1085/1100 [15:06<02:15,  9.01s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-354403778.0731017	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [15:07<00:00,  9.01s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [15:07<00:00,  6.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [15:09<00:00,  6.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [15:09<00:00,  6.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:56:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [15:10<00:00,  6.33s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:56:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [15:11<00:00,  6.33s/pipeline]Optimization Progress:  92%|█████████▏| 1101/1200 [15:11<09:19,  5.65s/pipeline]Optimization Progress:  92%|█████████▏| 1102/1200 [15:32<16:44, 10.25s/pipeline]Optimization Progress:  98%|█████████▊| 1182/1200 [15:56<02:10,  7.27s/pipeline]
Generation 11 - Current Pareto front scores:
-1	-354403778.0731017	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [15:57<00:00,  7.27s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [15:57<00:00,  5.10s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [15:58<00:00,  5.10s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  92%|█████████▏| 1201/1300 [16:01<08:24,  5.10s/pipeline]Optimization Progress:  92%|█████████▏| 1202/1300 [16:01<06:49,  4.18s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  92%|█████████▏| 1202/1300 [16:01<06:49,  4.18s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1203/1300 [16:01<06:45,  4.18s/pipeline]Optimization Progress:  93%|█████████▎| 1205/1300 [16:26<08:38,  5.46s/pipeline]Optimization Progress:  99%|█████████▉| 1285/1300 [16:30<00:57,  3.83s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-354403778.0731017	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:57:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1300/1300 [16:33<00:00,  3.83s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [16:33<00:00,  2.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 1300/1300 [16:33<00:00,  2.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:57:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1300/1300 [16:33<00:00,  2.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 1300/1300 [16:34<00:00,  2.74s/pipeline]Optimization Progress:  93%|█████████▎| 1302/1400 [16:35<03:32,  2.17s/pipeline]Optimization Progress:  93%|█████████▎| 1303/1400 [17:07<17:56, 11.10s/pipeline]Optimization Progress:  99%|█████████▉| 1383/1400 [17:10<02:12,  7.78s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-354242772.9340952	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-349289732.41293997	XGBRegressor(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 1400/1400 [17:10<00:00,  7.78s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1400/1400 [17:11<00:00,  7.78s/pipeline]Optimization Progress: 100%|██████████| 1400/1400 [17:11<00:00,  5.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1400/1400 [17:11<00:00,  5.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 1400/1400 [17:12<00:00,  5.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.
Optimization Progress: 100%|██████████| 1400/1400 [17:15<00:00,  5.46s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1401/1500 [17:17<09:00,  5.46s/pipeline]Optimization Progress:  93%|█████████▎| 1402/1500 [17:17<07:43,  4.73s/pipeline]Optimization Progress:  94%|█████████▎| 1403/1500 [17:47<20:02, 12.39s/pipeline]Optimization Progress:  99%|█████████▉| 1483/1500 [17:51<02:27,  8.69s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-354242772.9340952	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-349289732.41293997	XGBRegressor(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-7	-343949531.1259433	XGBRegressor(ExtraTreesRegressor(RBFSampler(RobustScaler(ZeroCount(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.9), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0))), RBFSampler__gamma=0.4), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=9, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1500/1500 [17:51<00:00,  8.69s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [17:51<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1500/1500 [17:52<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:58:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [17:53<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:58:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [17:53<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:58:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [17:53<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [17:54<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:59:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [17:55<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [17:55<00:00,  6.09s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1500/1500 [17:56<00:00,  6.09s/pipeline]Optimization Progress:  94%|█████████▍| 1500/1600 [18:10<10:08,  6.09s/pipeline]Optimization Progress:  94%|█████████▍| 1501/1600 [18:27<25:02, 15.17s/pipeline]Optimization Progress:  99%|█████████▉| 1581/1600 [18:50<03:23, 10.71s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-349289732.41293997	XGBRegressor(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-7	-343949531.1259433	XGBRegressor(ExtraTreesRegressor(RBFSampler(RobustScaler(ZeroCount(XGBRegressor(FastICA(input_matrix, FastICA__tol=0.9), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0))), RBFSampler__gamma=0.4), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=9, ExtraTreesRegressor__min_samples_split=14, ExtraTreesRegressor__n_estimators=100), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=1.0)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [18:50<00:00, 10.71s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [18:50<00:00,  7.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [06:59:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1600/1600 [18:53<00:00,  7.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [18:53<00:00,  7.51s/pipeline]Optimization Progress:  94%|█████████▍| 1602/1700 [18:54<09:23,  5.75s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [19:16<17:08, 10.60s/pipeline]Optimization Progress:  99%|█████████▉| 1683/1700 [19:48<02:08,  7.54s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-349289732.41293997	XGBRegressor(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-335010589.941265	XGBRegressor(Normalizer(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1700/1700 [19:48<00:00,  7.54s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [19:48<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:00:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [19:48<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 1700/1700 [19:49<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:00:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1700/1700 [19:49<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [19:49<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1700/1700 [19:49<00:00,  5.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1700/1700 [19:49<00:00,  5.28s/pipeline]Optimization Progress:  95%|█████████▍| 1702/1800 [20:00<08:37,  5.28s/pipeline]Optimization Progress:  95%|█████████▍| 1703/1800 [20:29<12:42,  7.86s/pipeline]Optimization Progress:  99%|█████████▉| 1783/1800 [20:32<01:33,  5.51s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353497997.9227719	XGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-349289732.41293997	XGBRegressor(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)
-4	-335010589.941265	XGBRegressor(Normalizer(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 1800/1800 [20:33<00:00,  5.51s/pipeline]Optimization Progress: 100%|██████████| 1800/1800 [20:33<00:00,  3.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [20:34<00:00,  3.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:01:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [20:34<00:00,  3.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [20:35<00:00,  3.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:01:40] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 1800/1800 [20:35<00:00,  3.89s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress: 100%|██████████| 1800/1800 [20:35<00:00,  3.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1800/1900 [20:36<06:28,  3.89s/pipeline]Optimization Progress:  95%|█████████▍| 1801/1900 [20:50<06:24,  3.89s/pipeline]Optimization Progress:  95%|█████████▍| 1802/1900 [21:04<12:01,  7.37s/pipeline]Optimization Progress:  99%|█████████▉| 1882/1900 [21:33<01:34,  5.26s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353497997.9227719	XGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-345734925.56997174	XGBRegressor(Nystroem(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=cosine, Nystroem__n_components=9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-335010589.941265	XGBRegressor(Normalizer(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [21:34<00:00,  5.26s/pipeline]Optimization Progress: 100%|██████████| 1900/1900 [21:34<00:00,  3.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 1900/1900 [21:34<00:00,  3.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [21:35<00:00,  3.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1900/1900 [21:36<00:00,  3.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [21:36<00:00,  3.70s/pipeline]Optimization Progress:  95%|█████████▌| 1902/2000 [21:36<04:44,  2.90s/pipeline]Optimization Progress:  95%|█████████▌| 1903/2000 [22:06<17:50, 11.04s/pipeline]Optimization Progress:  99%|█████████▉| 1983/2000 [22:26<02:12,  7.80s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353497997.9227719	XGBRegressor(LinearSVR(input_matrix, LinearSVR__C=0.0001, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-345734925.56997174	XGBRegressor(Nystroem(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=cosine, Nystroem__n_components=9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-335010589.941265	XGBRegressor(Normalizer(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.45, RandomForestRegressor__min_samples_leaf=18, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=6, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.2)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 100%|██████████| 2000/2000 [22:27<00:00,  7.80s/pipeline]Optimization Progress: 100%|██████████| 2000/2000 [22:27<00:00,  5.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:03:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2000/2000 [22:28<00:00,  5.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:03:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2000/2000 [22:28<00:00,  5.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [22:30<00:00,  5.47s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2000/2000 [22:30<00:00,  5.47s/pipeline]Optimization Progress:  95%|█████████▌| 2002/2100 [22:31<07:15,  4.44s/pipeline]Optimization Progress:  95%|█████████▌| 2003/2100 [23:08<22:57, 14.20s/pipeline]Optimization Progress:  99%|█████████▉| 2083/2100 [23:32<02:50, 10.03s/pipeline]
Generation 20 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353207646.14059764	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-345734925.56997174	XGBRegressor(Nystroem(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Nystroem__gamma=0.15000000000000002, Nystroem__kernel=cosine, Nystroem__n_components=9), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-4	-315380363.0517405	LinearSVR(Normalizer(Normalizer(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), Normalizer__norm=l1), Normalizer__norm=l2), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2100/2100 [23:32<00:00, 10.03s/pipeline]Optimization Progress: 100%|██████████| 2100/2100 [23:32<00:00,  7.03s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [23:34<00:00,  7.03s/pipeline]Optimization Progress:  96%|█████████▌| 2103/2200 [23:36<08:32,  5.28s/pipeline]Optimization Progress:  96%|█████████▌| 2103/2200 [23:50<08:32,  5.28s/pipeline]Optimization Progress:  96%|█████████▌| 2104/2200 [24:11<22:44, 14.21s/pipeline]Optimization Progress:  99%|█████████▉| 2184/2200 [24:38<02:40, 10.05s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353207646.14059764	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=14, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-246556004.02629247	LinearSVR(Normalizer(Normalizer(input_matrix, Normalizer__norm=l1), Normalizer__norm=l2), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [24:39<00:00, 10.05s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [24:39<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2200/2200 [24:40<00:00,  7.06s/pipeline]Optimization Progress:  96%|█████████▌| 2201/2300 [24:42<09:36,  5.83s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2201/2300 [24:42<09:36,  5.83s/pipeline]Optimization Progress:  96%|█████████▌| 2203/2300 [25:12<13:52,  8.58s/pipeline]Optimization Progress:  99%|█████████▉| 2283/2300 [25:18<01:42,  6.03s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-353148126.83443	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.01, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.3, GradientBoostingRegressor__min_samples_leaf=16, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.6000000000000001), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-208509824.73021954	LinearSVR(Normalizer(Normalizer(input_matrix, Normalizer__norm=l1), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-207137394.83734423	LinearSVR(Normalizer(DecisionTreeRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [25:19<00:00,  6.03s/pipeline]Optimization Progress: 100%|██████████| 2300/2300 [25:19<00:00,  4.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [25:19<00:00,  4.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [25:20<00:00,  4.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:06:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2300/2300 [25:20<00:00,  4.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [25:20<00:00,  4.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [25:20<00:00,  4.25s/pipeline]Optimization Progress:  96%|█████████▌| 2304/2400 [25:21<04:56,  3.08s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2304/2400 [25:21<04:56,  3.08s/pipeline]Optimization Progress:  96%|█████████▌| 2306/2400 [25:31<05:52,  3.75s/pipeline]Optimization Progress:  99%|█████████▉| 2386/2400 [25:33<00:36,  2.63s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-352886248.1365475	XGBRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=4, DecisionTreeRegressor__min_samples_leaf=17, DecisionTreeRegressor__min_samples_split=20), XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-3	-208509824.73021954	LinearSVR(Normalizer(Normalizer(input_matrix, Normalizer__norm=l1), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-207137394.83734423	LinearSVR(Normalizer(DecisionTreeRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-206430716.46501717	LinearSVR(Normalizer(RandomForestRegressor(DecisionTreeRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:06:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2400/2400 [25:34<00:00,  2.63s/pipeline]Optimization Progress: 100%|██████████| 2400/2400 [25:34<00:00,  1.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 2400/2400 [25:35<00:00,  1.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [25:35<00:00,  1.86s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:06:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2400/2400 [25:35<00:00,  1.86s/pipeline]Optimization Progress:  96%|█████████▌| 2403/2500 [25:36<02:26,  1.51s/pipeline]Optimization Progress:  96%|█████████▌| 2405/2500 [25:40<02:30,  1.58s/pipeline]Optimization Progress:  99%|█████████▉| 2484/2500 [25:43<00:17,  1.12s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-246548918.87129498	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=5.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-208509824.73021954	LinearSVR(Normalizer(Normalizer(input_matrix, Normalizer__norm=l1), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-207137394.83734423	LinearSVR(Normalizer(DecisionTreeRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-206430716.46501717	LinearSVR(Normalizer(RandomForestRegressor(DecisionTreeRegressor(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2500/2500 [25:43<00:00,  1.12s/pipeline]Optimization Progress: 100%|██████████| 2500/2500 [25:43<00:00,  1.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [25:44<00:00,  1.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2500/2500 [25:44<00:00,  1.27pipeline/s]Optimization Progress:  96%|█████████▋| 2508/2600 [25:45<00:57,  1.59pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2508/2600 [25:45<00:57,  1.59pipeline/s]Optimization Progress:  97%|█████████▋| 2511/2600 [25:48<01:03,  1.39pipeline/s]Optimization Progress: 100%|█████████▉| 2590/2600 [25:49<00:05,  1.98pipeline/s]
Generation 25 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208450034.60818776	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-206249305.47483772	LinearSVR(Normalizer(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-205699771.4556685	LinearSVR(Normalizer(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2600/2600 [25:49<00:00,  1.98pipeline/s]Optimization Progress: 100%|██████████| 2600/2600 [25:49<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:06:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [25:49<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2600/2600 [25:50<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2600/2600 [25:50<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2600/2600 [25:50<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 2600/2600 [25:51<00:00,  2.77pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:06:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2600/2600 [25:51<00:00,  2.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2603/2700 [25:51<00:34,  2.77pipeline/s]Optimization Progress:  96%|█████████▋| 2604/2700 [25:51<00:41,  2.33pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2604/2700 [25:51<00:41,  2.33pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2605/2700 [25:51<00:40,  2.33pipeline/s]Optimization Progress:  97%|█████████▋| 2607/2700 [25:56<01:11,  1.30pipeline/s]Optimization Progress: 100%|█████████▉| 2687/2700 [25:57<00:07,  1.84pipeline/s]
Generation 26 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208136385.35160998	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-206249305.47483772	LinearSVR(Normalizer(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-205699771.4556685	LinearSVR(Normalizer(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 2700/2700 [25:57<00:00,  1.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [25:58<00:00,  1.84pipeline/s]Optimization Progress: 100%|██████████| 2700/2700 [25:58<00:00,  2.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 2700/2700 [25:59<00:00,  2.56pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:07:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2700/2700 [25:59<00:00,  2.56pipeline/s]Optimization Progress:  97%|█████████▋| 2707/2800 [26:00<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2707/2800 [26:00<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2708/2800 [26:00<00:32,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2709/2800 [26:00<00:32,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2710/2800 [26:00<00:32,  2.81pipeline/s]Optimization Progress:  97%|█████████▋| 2712/2800 [26:04<00:46,  1.88pipeline/s]Optimization Progress: 100%|█████████▉| 2792/2800 [26:13<00:03,  2.46pipeline/s]
Generation 27 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208136385.35160998	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-206249305.47483772	LinearSVR(Normalizer(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-205699771.4556685	LinearSVR(Normalizer(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=1, DecisionTreeRegressor__min_samples_leaf=5, DecisionTreeRegressor__min_samples_split=16), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=13, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [26:13<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 2800/2800 [26:13<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:07:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [26:15<00:00,  2.46pipeline/s]Optimization Progress: 100%|██████████| 2800/2800 [26:15<00:00,  2.75pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [26:15<00:00,  2.75pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2800/2800 [26:16<00:00,  2.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2801/2900 [26:16<00:36,  2.75pipeline/s]Optimization Progress:  97%|█████████▋| 2802/2900 [26:16<00:32,  2.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2802/2900 [26:16<00:32,  2.99pipeline/s]Optimization Progress:  97%|█████████▋| 2804/2900 [26:19<01:04,  1.48pipeline/s]Optimization Progress:  99%|█████████▉| 2884/2900 [26:21<00:07,  2.09pipeline/s]
Generation 28 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208136385.35160998	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2900/2900 [26:21<00:00,  2.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 2900/2900 [26:22<00:00,  2.09pipeline/s]Optimization Progress: 100%|██████████| 2900/2900 [26:22<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2900/2900 [26:22<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [26:23<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2900/2900 [26:23<00:00,  2.84pipeline/s]Optimization Progress:  97%|█████████▋| 2902/3000 [26:23<00:44,  2.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2902/3000 [26:23<00:44,  2.20pipeline/s]Optimization Progress:  97%|█████████▋| 2904/3000 [26:27<01:26,  1.12pipeline/s]Optimization Progress:  99%|█████████▉| 2984/3000 [26:30<00:10,  1.57pipeline/s]
Generation 29 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-208136385.35160998	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [26:30<00:00,  1.57pipeline/s]Optimization Progress: 100%|██████████| 3000/3000 [26:30<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:07:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3000/3000 [26:30<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3000/3000 [26:31<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3000/3000 [26:32<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3000/3000 [26:32<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:07:37] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3000/3000 [26:32<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3000/3000 [26:32<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [26:32<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3000/3000 [26:33<00:00,  2.22pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3000/3000 [26:33<00:00,  2.22pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3003/3100 [26:33<00:43,  2.22pipeline/s]Optimization Progress:  97%|█████████▋| 3004/3100 [26:33<00:50,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3004/3100 [26:33<00:50,  1.90pipeline/s]Optimization Progress:  97%|█████████▋| 3007/3100 [26:38<01:25,  1.08pipeline/s]Optimization Progress: 100%|█████████▉| 3086/3100 [26:40<00:09,  1.54pipeline/s]
Generation 30 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205697020.78931487	LinearSVR(Binarizer(input_matrix, Binarizer__threshold=0.55), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 3100/3100 [26:40<00:00,  1.54pipeline/s]Optimization Progress: 100%|██████████| 3100/3100 [26:40<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3100/3100 [26:40<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [26:40<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 3100/3100 [26:41<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 3100/3100 [26:42<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 3100/3100 [26:42<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:07:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3100/3100 [26:43<00:00,  2.18pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3103/3200 [26:43<00:44,  2.18pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3104/3200 [26:43<00:44,  2.18pipeline/s]Optimization Progress:  97%|█████████▋| 3105/3200 [26:43<00:48,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3105/3200 [26:43<00:48,  1.94pipeline/s]Optimization Progress:  97%|█████████▋| 3109/3200 [26:48<01:03,  1.44pipeline/s]Optimization Progress: 100%|█████████▉| 3187/3200 [26:50<00:06,  2.03pipeline/s]
Generation 31 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205697020.78931487	LinearSVR(Binarizer(input_matrix, Binarizer__threshold=0.55), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-202477051.06147808	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 3200/3200 [26:51<00:00,  2.03pipeline/s]Optimization Progress: 100%|██████████| 3200/3200 [26:51<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3200/3200 [26:51<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [26:52<00:00,  2.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [26:52<00:00,  2.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3202/3300 [26:52<00:36,  2.70pipeline/s]Optimization Progress:  97%|█████████▋| 3203/3300 [26:52<00:43,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3203/3300 [26:53<00:43,  2.23pipeline/s]Optimization Progress:  97%|█████████▋| 3205/3300 [26:56<01:24,  1.12pipeline/s]Optimization Progress: 100%|█████████▉| 3285/3300 [27:08<00:10,  1.50pipeline/s]
Generation 32 - Current Pareto front scores:
-1	-353498107.5002518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205697020.78931487	LinearSVR(Binarizer(input_matrix, Binarizer__threshold=0.55), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-202477051.06147808	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:08:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3300/3300 [27:09<00:00,  1.50pipeline/s]Optimization Progress: 100%|██████████| 3300/3300 [27:09<00:00,  2.05pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:08:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3300/3300 [27:10<00:00,  2.05pipeline/s]Optimization Progress:  97%|█████████▋| 3302/3400 [27:10<00:55,  1.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3302/3400 [27:10<00:55,  1.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3303/3400 [27:10<00:54,  1.78pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3304/3400 [27:10<00:54,  1.78pipeline/s]Optimization Progress:  97%|█████████▋| 3306/3400 [27:14<01:04,  1.47pipeline/s]Optimization Progress: 100%|█████████▉| 3386/3400 [27:18<00:06,  2.04pipeline/s]
Generation 33 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205640081.5232902	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-202477051.06147808	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3400/3400 [27:18<00:00,  2.04pipeline/s]Optimization Progress: 100%|██████████| 3400/3400 [27:18<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [27:18<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [27:19<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [27:19<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [27:20<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [27:20<00:00,  2.87pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3400/3400 [27:20<00:00,  2.87pipeline/s]Optimization Progress:  97%|█████████▋| 3403/3500 [27:20<00:47,  2.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3403/3500 [27:20<00:47,  2.04pipeline/s]Optimization Progress:  97%|█████████▋| 3405/3500 [27:26<01:52,  1.19s/pipeline]Optimization Progress: 100%|█████████▉| 3485/3500 [27:27<00:12,  1.20pipeline/s]
Generation 34 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-202477051.06147808	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [27:28<00:00,  1.20pipeline/s]Optimization Progress: 100%|██████████| 3500/3500 [27:28<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [27:28<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [27:28<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [27:29<00:00,  1.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3500/3500 [27:29<00:00,  1.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3502/3600 [27:30<00:58,  1.69pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3503/3600 [27:30<00:57,  1.69pipeline/s]Optimization Progress:  97%|█████████▋| 3504/3600 [27:30<00:54,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3504/3600 [27:30<00:54,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3505/3600 [27:30<00:53,  1.77pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3506/3600 [27:30<00:52,  1.77pipeline/s]Optimization Progress:  97%|█████████▋| 3508/3600 [27:33<01:03,  1.45pipeline/s]Optimization Progress: 100%|█████████▉| 3588/3600 [27:37<00:05,  2.01pipeline/s]
Generation 35 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-202477051.06147808	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3600/3600 [27:37<00:00,  2.01pipeline/s]Optimization Progress: 100%|██████████| 3600/3600 [27:37<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3600/3600 [27:38<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:08:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3600/3600 [27:38<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 3600/3600 [27:38<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3600/3600 [27:39<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3600/3600 [27:39<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:08:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3600/3600 [27:39<00:00,  2.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3600/3600 [27:39<00:00,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3601/3700 [27:40<00:34,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3602/3700 [27:40<00:34,  2.84pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3603/3700 [27:40<00:34,  2.84pipeline/s]Optimization Progress:  97%|█████████▋| 3604/3700 [27:40<00:40,  2.40pipeline/s]Optimization Progress:  97%|█████████▋| 3607/3700 [27:45<01:19,  1.17pipeline/s]Optimization Progress: 100%|█████████▉| 3685/3700 [27:48<00:09,  1.64pipeline/s]
Generation 36 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203870302.9779472	LinearSVR(Normalizer(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=4, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201706505.88341406	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3700/3700 [27:49<00:00,  1.64pipeline/s]Optimization Progress: 100%|██████████| 3700/3700 [27:49<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3700/3700 [27:49<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [27:49<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [27:50<00:00,  2.29pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3702/3800 [27:51<00:42,  2.29pipeline/s]Optimization Progress:  97%|█████████▋| 3703/3800 [27:51<00:49,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3703/3800 [27:51<00:49,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3704/3800 [27:51<00:48,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3705/3800 [27:51<00:48,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3706/3800 [27:51<00:47,  1.98pipeline/s]Optimization Progress:  98%|█████████▊| 3708/3800 [27:55<00:54,  1.70pipeline/s]Optimization Progress: 100%|█████████▉| 3788/3800 [27:56<00:05,  2.38pipeline/s]
Generation 37 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-203021986.14985853	LinearSVR(Normalizer(KNeighborsRegressor(CombineDFs(input_matrix, input_matrix), KNeighborsRegressor__n_neighbors=24, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-201706505.88341406	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  2.38pipeline/s]Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3800/3800 [27:57<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 3800/3800 [27:58<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3800/3800 [27:59<00:00,  3.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3802/3900 [27:59<00:29,  3.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3803/3900 [27:59<00:28,  3.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3804/3900 [27:59<00:28,  3.37pipeline/s]Optimization Progress:  98%|█████████▊| 3805/3900 [27:59<00:32,  2.89pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3805/3900 [27:59<00:32,  2.89pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3806/3900 [27:59<00:32,  2.89pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3807/3900 [27:59<00:32,  2.89pipeline/s]Optimization Progress:  98%|█████████▊| 3809/3900 [28:02<00:41,  2.22pipeline/s]Optimization Progress: 100%|█████████▉| 3889/3900 [28:02<00:03,  3.14pipeline/s]
Generation 38 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202994917.46802002	LinearSVR(Normalizer(CombineDFs(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=24, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), input_matrix), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-201706505.88341406	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  3.14pipeline/s]Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 3900/3900 [28:03<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3900/3900 [28:04<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 3900/3900 [28:04<00:00,  4.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3900/3900 [28:05<00:00,  4.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3900/4000 [28:05<00:23,  4.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 3901/4000 [28:05<00:23,  4.25pipeline/s]Optimization Progress:  98%|█████████▊| 3905/4000 [28:09<00:54,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 3983/4000 [28:11<00:06,  2.46pipeline/s]
Generation 39 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202994917.46802002	LinearSVR(Normalizer(CombineDFs(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=24, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), input_matrix), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-201706505.88341406	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 4000/4000 [28:12<00:00,  2.46pipeline/s]Optimization Progress: 100%|██████████| 4000/4000 [28:12<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4000/4000 [28:12<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4000/4000 [28:13<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4000/4000 [28:13<00:00,  3.44pipeline/s]Optimization Progress:  98%|█████████▊| 4004/4100 [28:13<00:30,  3.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4004/4100 [28:13<00:30,  3.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4005/4100 [28:13<00:29,  3.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4006/4100 [28:13<00:29,  3.19pipeline/s]Optimization Progress:  98%|█████████▊| 4008/4100 [28:16<00:41,  2.19pipeline/s]Optimization Progress: 100%|█████████▉| 4087/4100 [28:30<00:05,  2.19pipeline/s]Optimization Progress: 100%|█████████▉| 4088/4100 [28:32<00:04,  2.65pipeline/s]
Generation 40 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202994917.46802002	LinearSVR(Normalizer(CombineDFs(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=24, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), input_matrix), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-201706505.88341406	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4100/4100 [28:32<00:00,  2.65pipeline/s]Optimization Progress: 100%|██████████| 4100/4100 [28:32<00:00,  3.67pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4100/4100 [28:33<00:00,  3.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4100/4200 [28:35<00:27,  3.67pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4101/4200 [28:35<00:26,  3.67pipeline/s]Optimization Progress:  98%|█████████▊| 4102/4200 [28:35<00:57,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4102/4200 [28:35<00:57,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4103/4200 [28:35<00:56,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4104/4200 [28:35<00:55,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4105/4200 [28:35<00:55,  1.71pipeline/s]Optimization Progress:  98%|█████████▊| 4107/4200 [28:39<01:01,  1.52pipeline/s]Optimization Progress: 100%|█████████▉| 4187/4200 [28:43<00:06,  2.10pipeline/s]
Generation 41 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202994917.46802002	LinearSVR(Normalizer(CombineDFs(KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=24, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance), input_matrix), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-201543651.7353596	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4200/4200 [28:44<00:00,  2.10pipeline/s]Optimization Progress: 100%|██████████| 4200/4200 [28:44<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4200/4200 [28:44<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [28:45<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [28:45<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4200/4200 [28:45<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4200/4200 [28:45<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4200/4200 [28:46<00:00,  2.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4201/4300 [28:46<00:33,  2.94pipeline/s]Optimization Progress:  98%|█████████▊| 4202/4300 [28:46<00:57,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4202/4300 [28:46<00:57,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4203/4300 [28:46<00:56,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4204/4300 [28:46<00:56,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4205/4300 [28:46<00:55,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4206/4300 [28:46<00:54,  1.71pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4207/4300 [28:46<00:54,  1.71pipeline/s]Optimization Progress:  98%|█████████▊| 4209/4300 [28:51<00:56,  1.61pipeline/s]Optimization Progress: 100%|█████████▉| 4289/4300 [28:52<00:04,  2.29pipeline/s]
Generation 42 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202413552.38889197	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201543651.7353596	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199682456.18888652	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 4300/4300 [28:52<00:00,  2.29pipeline/s]Optimization Progress: 100%|██████████| 4300/4300 [28:52<00:00,  3.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 4300/4300 [28:52<00:00,  3.16pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 4300/4300 [28:53<00:00,  3.16pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4303/4400 [28:54<00:30,  3.16pipeline/s]Optimization Progress:  98%|█████████▊| 4304/4400 [28:54<00:33,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4304/4400 [28:54<00:33,  2.83pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4305/4400 [28:54<00:33,  2.83pipeline/s]Optimization Progress:  98%|█████████▊| 4307/4400 [28:57<00:54,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 4387/4400 [28:58<00:05,  2.39pipeline/s]
Generation 43 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202413552.38889197	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201543651.7353596	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=18, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199682456.18888652	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4400/4400 [28:59<00:00,  2.39pipeline/s]Optimization Progress: 100%|██████████| 4400/4400 [28:59<00:00,  3.24pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4400/4400 [29:00<00:00,  3.24pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 4400/4400 [29:00<00:00,  3.24pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4400/4500 [29:00<00:30,  3.24pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4401/4500 [29:00<00:30,  3.24pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4402/4500 [29:00<00:30,  3.24pipeline/s]Optimization Progress:  98%|█████████▊| 4403/4500 [29:00<00:32,  2.98pipeline/s]Optimization Progress:  98%|█████████▊| 4406/4500 [29:04<00:59,  1.58pipeline/s]Optimization Progress: 100%|█████████▉| 4484/4500 [29:06<00:07,  2.22pipeline/s]
Generation 44 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202413552.38889197	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201343207.05408463	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-199682456.18888652	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4500/4500 [29:06<00:00,  2.22pipeline/s]Optimization Progress: 100%|██████████| 4500/4500 [29:06<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [29:06<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [29:07<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4500/4500 [29:08<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4500/4500 [29:08<00:00,  3.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4501/4600 [29:09<00:31,  3.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4502/4600 [29:09<00:31,  3.15pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 4503/4600 [29:09<00:30,  3.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4504/4600 [29:09<00:30,  3.15pipeline/s]Optimization Progress:  98%|█████████▊| 4507/4600 [29:12<00:46,  1.99pipeline/s]Optimization Progress: 100%|█████████▉| 4586/4600 [29:16<00:05,  2.74pipeline/s]
Generation 45 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205573833.07285583	LinearSVR(Normalizer(input_matrix, Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202399644.00374636	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201343207.05408463	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-199668535.06946564	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [29:16<00:00,  2.74pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 4600/4600 [29:17<00:00,  2.74pipeline/s]Optimization Progress: 100%|██████████| 4600/4600 [29:17<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [29:18<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [29:18<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [29:18<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4600/4600 [29:18<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4600/4600 [29:18<00:00,  3.49pipeline/s]Optimization Progress:  98%|█████████▊| 4603/4700 [29:19<00:32,  2.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4603/4700 [29:19<00:32,  2.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4604/4700 [29:19<00:32,  2.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4605/4700 [29:19<00:31,  2.98pipeline/s]Optimization Progress:  98%|█████████▊| 4607/4700 [29:25<01:02,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 4687/4700 [29:28<00:06,  2.06pipeline/s]
Generation 46 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202399644.00374636	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199668535.06946564	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 4700/4700 [29:28<00:00,  2.06pipeline/s]Optimization Progress: 100%|██████████| 4700/4700 [29:28<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [29:28<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4700/4700 [29:28<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:10:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 4700/4700 [29:29<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [29:29<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [29:29<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 4700/4700 [29:30<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4700/4700 [29:30<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4700/4700 [29:30<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4700/4700 [29:30<00:00,  2.89pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4702/4800 [29:30<00:33,  2.89pipeline/s]Optimization Progress:  98%|█████████▊| 4703/4800 [29:30<00:45,  2.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4703/4800 [29:31<00:45,  2.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4704/4800 [29:31<00:45,  2.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4705/4800 [29:31<00:44,  2.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4706/4800 [29:31<00:44,  2.13pipeline/s]Optimization Progress:  98%|█████████▊| 4708/4800 [29:44<01:43,  1.12s/pipeline]Optimization Progress: 100%|█████████▉| 4788/4800 [29:46<00:09,  1.26pipeline/s]
Generation 47 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202399644.00374636	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199668535.06946564	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196683286.39825734	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 4800/4800 [29:49<00:00,  1.26pipeline/s]Optimization Progress: 100%|██████████| 4800/4800 [29:49<00:00,  1.61pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 4800/4800 [29:49<00:00,  1.61pipeline/s]Optimization Progress:  98%|█████████▊| 4801/4900 [29:49<00:53,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4801/4900 [29:49<00:53,  1.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4802/4900 [29:49<00:52,  1.87pipeline/s]Optimization Progress:  98%|█████████▊| 4804/4900 [29:54<01:21,  1.17pipeline/s]Optimization Progress: 100%|█████████▉| 4884/4900 [30:03<00:10,  1.59pipeline/s]
Generation 48 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202399644.00374636	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199668535.06946564	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-198833448.8201711	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001)), Nystroem__gamma=0.4, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [30:04<00:00,  1.59pipeline/s]Optimization Progress: 100%|██████████| 4900/4900 [30:04<00:00,  2.23pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 4900/4900 [30:04<00:00,  2.23pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 4900/4900 [30:04<00:00,  2.23pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 4900/4900 [30:05<00:00,  2.23pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 4900/4900 [30:05<00:00,  2.23pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 4900/4900 [30:06<00:00,  2.23pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4901/5000 [30:06<00:44,  2.23pipeline/s]Optimization Progress:  98%|█████████▊| 4902/5000 [30:06<01:01,  1.58pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 4902/5000 [30:06<01:01,  1.58pipeline/s]Optimization Progress:  98%|█████████▊| 4904/5000 [30:16<03:05,  1.93s/pipeline]Optimization Progress: 100%|█████████▉| 4984/5000 [30:18<00:21,  1.36s/pipeline]
Generation 49 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202399644.00374636	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-199668535.06946564	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-198833448.8201711	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(MinMaxScaler(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001)), Nystroem__gamma=0.4, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5000/5000 [30:18<00:00,  1.36s/pipeline]Optimization Progress: 100%|██████████| 5000/5000 [30:18<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5000/5000 [30:18<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5000/5000 [30:19<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5000/5000 [30:19<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:11:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5000/5000 [30:20<00:00,  1.04pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 5000/5000 [30:20<00:00,  1.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5000/5100 [30:21<01:35,  1.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5001/5100 [30:21<01:35,  1.04pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5002/5100 [30:21<01:34,  1.04pipeline/s]Optimization Progress:  98%|█████████▊| 5003/5100 [30:21<01:26,  1.12pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5003/5100 [30:21<01:26,  1.12pipeline/s]Optimization Progress:  98%|█████████▊| 5005/5100 [30:26<02:11,  1.38s/pipeline]Optimization Progress: 100%|█████████▉| 5085/5100 [30:30<00:14,  1.02pipeline/s]
Generation 50 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202386335.32458037	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-198038592.99318928	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.4, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [30:30<00:00,  1.02pipeline/s]Optimization Progress: 100%|██████████| 5100/5100 [30:30<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:11:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5100/5100 [30:30<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:11:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5100/5100 [30:30<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:11:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5100/5100 [30:30<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5100/5100 [30:31<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5100/5100 [30:33<00:00,  1.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5100/5100 [30:33<00:00,  1.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5103/5200 [30:33<01:07,  1.45pipeline/s]Optimization Progress:  98%|█████████▊| 5104/5200 [30:33<01:05,  1.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5104/5200 [30:33<01:05,  1.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5105/5200 [30:33<01:04,  1.47pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5106/5200 [30:33<01:04,  1.47pipeline/s]Optimization Progress:  98%|█████████▊| 5108/5200 [30:48<02:26,  1.59s/pipeline]Optimization Progress: 100%|█████████▉| 5188/5200 [31:06<00:14,  1.18s/pipeline]
Generation 51 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-202386335.32458037	LinearSVR(Normalizer(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=13, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45), Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-198017537.83489183	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-197332019.37685266	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 5200/5200 [31:06<00:00,  1.18s/pipeline]Optimization Progress: 100%|██████████| 5200/5200 [31:06<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5200/5200 [31:06<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5200/5200 [31:06<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5200/5200 [31:06<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5200/5200 [31:07<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5200/5200 [31:07<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 5200/5200 [31:08<00:00,  1.20pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5200/5200 [31:08<00:00,  1.20pipeline/s]Optimization Progress:  98%|█████████▊| 5203/5300 [31:08<01:20,  1.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5203/5300 [31:08<01:20,  1.20pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5204/5300 [31:08<01:19,  1.20pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5205/5300 [31:08<01:19,  1.20pipeline/s]Optimization Progress:  98%|█████████▊| 5207/5300 [31:16<01:49,  1.18s/pipeline]Optimization Progress: 100%|█████████▉| 5287/5300 [31:18<00:10,  1.20pipeline/s]
Generation 52 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201795963.45812345	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-201130718.1319782	LinearSVR(Normalizer(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-5	-198017537.83489183	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-197332019.37685266	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5300/5300 [31:19<00:00,  1.20pipeline/s]Optimization Progress: 100%|██████████| 5300/5300 [31:19<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5300/5300 [31:19<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress: 100%|██████████| 5300/5300 [31:19<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 5300/5300 [31:20<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5300/5300 [31:20<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5300/5300 [31:21<00:00,  1.70pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5300/5300 [31:21<00:00,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5301/5400 [31:21<00:58,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5302/5400 [31:21<00:57,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5303/5400 [31:21<00:56,  1.70pipeline/s]Optimization Progress:  98%|█████████▊| 5304/5400 [31:21<00:58,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5304/5400 [31:21<00:58,  1.65pipeline/s]Optimization Progress:  98%|█████████▊| 5307/5400 [31:25<01:16,  1.22pipeline/s]Optimization Progress: 100%|█████████▉| 5386/5400 [31:27<00:08,  1.71pipeline/s]
Generation 53 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201795963.45812345	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-200132758.69140753	LinearSVR(Normalizer(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.4, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-197718165.99541476	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-197332019.37685266	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 5400/5400 [31:27<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5400/5400 [31:27<00:00,  1.71pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  1.71pipeline/s]Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5400/5400 [31:28<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5400/5400 [31:29<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5400/5400 [31:29<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5400/5400 [31:29<00:00,  2.41pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5400/5400 [31:29<00:00,  2.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5403/5500 [31:30<00:40,  2.41pipeline/s]Optimization Progress:  98%|█████████▊| 5404/5500 [31:30<00:42,  2.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5404/5500 [31:30<00:42,  2.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5405/5500 [31:30<00:42,  2.25pipeline/s]Optimization Progress:  98%|█████████▊| 5407/5500 [31:33<00:56,  1.65pipeline/s]Optimization Progress: 100%|█████████▉| 5487/5500 [31:34<00:05,  2.34pipeline/s]
Generation 54 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201795963.45812345	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-200132758.69140753	LinearSVR(Normalizer(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.4, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-5	-197718165.99541476	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-197332019.37685266	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5500/5500 [31:34<00:00,  2.34pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5500/5500 [31:34<00:00,  2.34pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 5500/5500 [31:34<00:00,  2.34pipeline/s]Optimization Progress: 100%|██████████| 5500/5500 [31:34<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5500/5500 [31:34<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5500/5500 [31:35<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5500/5500 [31:35<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5500/5500 [31:35<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 5500/5500 [31:36<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 5500/5500 [31:36<00:00,  3.27pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5500/5500 [31:36<00:00,  3.27pipeline/s]Optimization Progress:  98%|█████████▊| 5504/5600 [31:37<00:37,  2.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5504/5600 [31:37<00:37,  2.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5505/5600 [31:37<00:37,  2.55pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5506/5600 [31:37<00:36,  2.55pipeline/s]Optimization Progress:  98%|█████████▊| 5508/5600 [31:40<00:51,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 5588/5600 [31:43<00:04,  2.49pipeline/s]
Generation 55 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201755842.61039704	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-197150253.01368383	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 5600/5600 [31:43<00:00,  2.49pipeline/s]Optimization Progress: 100%|██████████| 5600/5600 [31:43<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5600/5600 [31:43<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5600/5600 [31:43<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 5600/5600 [31:44<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5600/5600 [31:44<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 5600/5600 [31:44<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5600/5600 [31:44<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5600/5600 [31:45<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5600/5600 [31:45<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 5600/5600 [31:46<00:00,  3.52pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5600/5600 [31:46<00:00,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5603/5700 [31:46<00:27,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5604/5700 [31:46<00:27,  3.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5605/5700 [31:46<00:26,  3.52pipeline/s]Optimization Progress:  98%|█████████▊| 5606/5700 [31:46<00:35,  2.68pipeline/s]Optimization Progress:  98%|█████████▊| 5610/5700 [31:51<00:54,  1.66pipeline/s]Optimization Progress: 100%|█████████▉| 5687/5700 [31:54<00:05,  2.30pipeline/s]
Generation 56 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201755842.61039704	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-197150253.01368383	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-196579716.03639442	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=1.0, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5700/5700 [31:55<00:00,  2.30pipeline/s]Optimization Progress: 100%|██████████| 5700/5700 [31:55<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5700/5700 [31:55<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5700/5700 [31:56<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 5700/5700 [31:56<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 5700/5700 [31:56<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 5700/5700 [31:57<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:13:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 5700/5700 [31:57<00:00,  3.21pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5700/5800 [31:57<00:31,  3.21pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5701/5800 [31:57<00:30,  3.21pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5702/5800 [31:57<00:30,  3.21pipeline/s]Optimization Progress:  98%|█████████▊| 5703/5800 [31:57<00:47,  2.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5703/5800 [31:57<00:47,  2.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5704/5800 [31:57<00:47,  2.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5705/5800 [31:57<00:46,  2.03pipeline/s]Optimization Progress:  98%|█████████▊| 5707/5800 [32:00<00:51,  1.80pipeline/s]Optimization Progress: 100%|█████████▉| 5787/5800 [32:03<00:05,  2.52pipeline/s]
Generation 57 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201755842.61039704	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-197150253.01368383	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-191218700.1235989	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5800/5800 [32:03<00:00,  2.52pipeline/s]Optimization Progress: 100%|██████████| 5800/5800 [32:03<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5800/5800 [32:04<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5800/5800 [32:05<00:00,  3.49pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5800/5800 [32:06<00:00,  3.49pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  98%|█████████▊| 5801/5900 [32:06<00:28,  3.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5802/5900 [32:06<00:28,  3.49pipeline/s]Optimization Progress:  98%|█████████▊| 5803/5900 [32:06<00:48,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5803/5900 [32:06<00:48,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5804/5900 [32:06<00:48,  1.99pipeline/s]Optimization Progress:  98%|█████████▊| 5806/5900 [32:10<01:08,  1.37pipeline/s]Optimization Progress: 100%|█████████▉| 5886/5900 [32:12<00:07,  1.94pipeline/s]
Generation 58 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201728042.78936678	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-197150253.01368383	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-191218700.1235989	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5900/5900 [32:12<00:00,  1.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [32:12<00:00,  1.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 5900/5900 [32:12<00:00,  1.94pipeline/s]Optimization Progress: 100%|██████████| 5900/5900 [32:12<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 5900/5900 [32:13<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 5900/5900 [32:13<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 5900/5900 [32:13<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 5900/5900 [32:13<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 5900/5900 [32:13<00:00,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5901/6000 [32:14<00:36,  2.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5902/6000 [32:14<00:36,  2.68pipeline/s]Optimization Progress:  98%|█████████▊| 5903/6000 [32:14<00:46,  2.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5903/6000 [32:14<00:46,  2.08pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 5904/6000 [32:14<00:46,  2.08pipeline/s]Optimization Progress:  98%|█████████▊| 5906/6000 [32:18<01:07,  1.40pipeline/s]Optimization Progress: 100%|█████████▉| 5986/6000 [32:21<00:07,  1.95pipeline/s]
Generation 59 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201688787.34629273	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=7, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-197150253.01368383	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-191218700.1235989	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-155593022.40199357	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6000/6000 [32:22<00:00,  1.95pipeline/s]Optimization Progress: 100%|██████████| 6000/6000 [32:22<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6000/6000 [32:23<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 6000/6000 [32:23<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6000/6000 [32:23<00:00,  2.68pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6000/6000 [32:24<00:00,  2.68pipeline/s]Optimization Progress:  98%|█████████▊| 6004/6100 [32:24<00:38,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6004/6100 [32:24<00:38,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6005/6100 [32:24<00:37,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6006/6100 [32:24<00:37,  2.52pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6007/6100 [32:24<00:36,  2.52pipeline/s]Optimization Progress:  99%|█████████▊| 6009/6100 [32:29<00:53,  1.70pipeline/s]Optimization Progress: 100%|█████████▉| 6089/6100 [32:30<00:04,  2.41pipeline/s]
Generation 60 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196974570.11157116	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-191218700.1235989	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-155593022.40199357	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 6100/6100 [32:30<00:00,  2.41pipeline/s]Optimization Progress: 100%|██████████| 6100/6100 [32:30<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6100/6100 [32:31<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:13:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6100/6100 [32:31<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:13:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6100/6100 [32:31<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6100/6100 [32:31<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6100/6100 [32:32<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6100/6100 [32:32<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6100/6100 [32:33<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 6100/6100 [32:33<00:00,  3.31pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 6100/6100 [32:33<00:00,  3.31pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6103/6200 [32:33<00:29,  3.31pipeline/s]Optimization Progress:  98%|█████████▊| 6104/6200 [32:33<00:42,  2.25pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6104/6200 [32:33<00:42,  2.25pipeline/s]Optimization Progress:  98%|█████████▊| 6107/6200 [32:38<01:13,  1.27pipeline/s]Optimization Progress: 100%|█████████▉| 6186/6200 [32:41<00:07,  1.78pipeline/s]
Generation 61 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196974570.11157116	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 6200/6200 [32:41<00:00,  1.78pipeline/s]Optimization Progress: 100%|██████████| 6200/6200 [32:41<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:13:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6200/6200 [32:41<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6200/6200 [32:41<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6200/6200 [32:42<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 6200/6200 [32:42<00:00,  2.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:13:47] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6200/6200 [32:42<00:00,  2.46pipeline/s]Optimization Progress:  98%|█████████▊| 6203/6300 [32:43<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6203/6300 [32:43<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6204/6300 [32:43<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6205/6300 [32:43<00:40,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6206/6300 [32:43<00:39,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6207/6300 [32:43<00:39,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6208/6300 [32:43<00:38,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6209/6300 [32:43<00:38,  2.37pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6210/6300 [32:43<00:38,  2.37pipeline/s]Optimization Progress:  99%|█████████▊| 6212/6300 [32:49<00:46,  1.91pipeline/s]Optimization Progress: 100%|█████████▉| 6291/6300 [33:00<00:04,  1.91pipeline/s]Optimization Progress: 100%|█████████▉| 6292/6300 [33:05<00:03,  2.35pipeline/s]
Generation 62 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 6300/6300 [33:05<00:00,  2.35pipeline/s]Optimization Progress: 100%|██████████| 6300/6300 [33:05<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 6300/6300 [33:06<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:14:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6300/6300 [33:06<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6300/6300 [33:06<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6300/6300 [33:07<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 6300/6300 [33:07<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6300/6300 [33:07<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6300/6300 [33:08<00:00,  3.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 6300/6300 [33:08<00:00,  3.21pipeline/s]Optimization Progress:  98%|█████████▊| 6302/6400 [33:08<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6302/6400 [33:09<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6303/6400 [33:09<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6304/6400 [33:09<01:04,  1.49pipeline/s]Optimization Progress:  99%|█████████▊| 6306/6400 [33:12<01:09,  1.35pipeline/s]Optimization Progress: 100%|█████████▉| 6386/6400 [33:16<00:07,  1.88pipeline/s]
Generation 63 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6400/6400 [33:17<00:00,  1.88pipeline/s]Optimization Progress: 100%|██████████| 6400/6400 [33:17<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6400/6400 [33:18<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6400/6400 [33:18<00:00,  2.53pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6400/6400 [33:18<00:00,  2.53pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6401/6500 [33:18<00:39,  2.53pipeline/s]Optimization Progress:  98%|█████████▊| 6402/6500 [33:18<00:51,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 6402/6500 [33:18<00:51,  1.90pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6403/6500 [33:18<00:51,  1.90pipeline/s]Optimization Progress:  99%|█████████▊| 6405/6500 [33:23<01:14,  1.28pipeline/s]Optimization Progress: 100%|█████████▉| 6485/6500 [33:25<00:08,  1.79pipeline/s]
Generation 64 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 6500/6500 [33:27<00:00,  1.79pipeline/s]Optimization Progress: 100%|██████████| 6500/6500 [33:27<00:00,  2.39pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:14:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6500/6500 [33:27<00:00,  2.39pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:14:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6500/6500 [33:27<00:00,  2.39pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 6500/6500 [33:28<00:00,  2.39pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 6500/6500 [33:28<00:00,  2.39pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6500/6500 [33:29<00:00,  2.39pipeline/s]Optimization Progress:  99%|█████████▊| 6505/6600 [33:29<00:38,  2.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6505/6600 [33:29<00:38,  2.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6506/6600 [33:29<00:38,  2.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6507/6600 [33:29<00:37,  2.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6508/6600 [33:29<00:37,  2.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6509/6600 [33:29<00:37,  2.45pipeline/s]Optimization Progress:  99%|█████████▊| 6511/6600 [33:34<00:51,  1.74pipeline/s]Optimization Progress: 100%|█████████▉| 6591/6600 [33:38<00:03,  2.40pipeline/s]
Generation 65 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6600/6600 [33:38<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:14:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6600/6600 [33:39<00:00,  2.40pipeline/s]Optimization Progress: 100%|██████████| 6600/6600 [33:39<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6600/6600 [33:39<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [33:40<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:14:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6600/6600 [33:40<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [33:41<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6600/6600 [33:41<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 [07:14:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6600/6600 [33:41<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 6600/6600 [33:42<00:00,  3.12pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6600/6600 [33:42<00:00,  3.12pipeline/s]Optimization Progress:  99%|█████████▊| 6603/6700 [33:42<00:55,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6603/6700 [33:42<00:55,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6604/6700 [33:42<00:55,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6605/6700 [33:42<00:54,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6606/6700 [33:42<00:53,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6607/6700 [33:42<00:53,  1.75pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6608/6700 [33:42<00:52,  1.75pipeline/s]Optimization Progress:  99%|█████████▊| 6610/6700 [33:49<01:01,  1.46pipeline/s]Optimization Progress: 100%|█████████▉| 6690/6700 [33:50<00:04,  2.08pipeline/s]
Generation 66 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201597616.00695044	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [33:50<00:00,  2.08pipeline/s]Optimization Progress: 100%|██████████| 6700/6700 [33:50<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6700/6700 [33:51<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6700/6700 [33:52<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 6700/6700 [33:52<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 6700/6700 [33:53<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6700/6700 [33:53<00:00,  2.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6700/6700 [33:53<00:00,  2.86pipeline/s]Optimization Progress:  99%|█████████▊| 6704/6800 [33:53<00:48,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6704/6800 [33:53<00:48,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6705/6800 [33:53<00:47,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6706/6800 [33:53<00:47,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6707/6800 [33:53<00:46,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6708/6800 [33:53<00:46,  1.99pipeline/s]Optimization Progress:  99%|█████████▊| 6710/6800 [33:57<00:49,  1.83pipeline/s]Optimization Progress: 100%|█████████▉| 6790/6800 [34:02<00:04,  2.50pipeline/s]
Generation 67 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196961522.06725636	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 6800/6800 [34:02<00:00,  2.50pipeline/s]Optimization Progress: 100%|██████████| 6800/6800 [34:02<00:00,  3.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6800/6800 [34:02<00:00,  3.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 6800/6800 [34:03<00:00,  3.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6800/6800 [34:03<00:00,  3.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6800/6800 [34:04<00:00,  3.45pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:15:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6800/6800 [34:05<00:00,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6801/6900 [34:05<00:28,  3.45pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6802/6900 [34:05<00:28,  3.45pipeline/s]Optimization Progress:  99%|█████████▊| 6803/6900 [34:05<00:50,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6803/6900 [34:05<00:50,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6804/6900 [34:05<00:49,  1.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6805/6900 [34:05<00:49,  1.94pipeline/s]Optimization Progress:  99%|█████████▊| 6807/6900 [34:19<02:06,  1.36s/pipeline]Optimization Progress: 100%|█████████▉| 6887/6900 [34:21<00:12,  1.04pipeline/s]
Generation 68 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 6900/6900 [34:22<00:00,  1.04pipeline/s]Optimization Progress: 100%|██████████| 6900/6900 [34:22<00:00,  1.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 6900/6900 [34:23<00:00,  1.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 6900/6900 [34:23<00:00,  1.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:15:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6900/6900 [34:23<00:00,  1.46pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:15:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 6900/6900 [34:24<00:00,  1.46pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6901/7000 [34:25<01:07,  1.46pipeline/s]Optimization Progress:  99%|█████████▊| 6902/7000 [34:25<01:26,  1.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6902/7000 [34:25<01:26,  1.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6903/7000 [34:25<01:25,  1.13pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 6904/7000 [34:25<01:24,  1.13pipeline/s]Optimization Progress:  99%|█████████▊| 6906/7000 [34:29<01:31,  1.03pipeline/s]Optimization Progress: 100%|█████████▉| 6986/7000 [34:32<00:09,  1.45pipeline/s]
Generation 69 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)Optimization Progress: 100%|██████████| 7000/7000 [34:35<00:00,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7001/7100 [34:35<00:55,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7002/7100 [34:35<00:54,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7003/7100 [34:35<00:53,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7004/7100 [34:35<00:53,  1.80pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7005/7100 [34:35<00:52,  1.80pipeline/s]Optimization Progress:  99%|█████████▊| 7007/7100 [34:40<00:57,  1.62pipeline/s]Optimization Progress: 100%|█████████▉| 7087/7100 [34:46<00:05,  2.21pipeline/s]
Generation 70 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7100/7100 [34:46<00:00,  2.21pipeline/s]Optimization Progress: 100%|██████████| 7100/7100 [34:46<00:00,  3.05pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [34:47<00:00,  3.05pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7100/7100 [34:47<00:00,  3.05pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7100/7100 [34:48<00:00,  3.05pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 7100/7100 [34:49<00:00,  3.05pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7101/7200 [34:49<00:32,  3.05pipeline/s]Optimization Progress:  99%|█████████▊| 7102/7200 [34:49<01:05,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7102/7200 [34:49<01:05,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7103/7200 [34:49<01:04,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7104/7200 [34:49<01:04,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7105/7200 [34:49<01:03,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7106/7200 [34:49<01:02,  1.50pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7107/7200 [34:49<01:02,  1.50pipeline/s]Optimization Progress:  99%|█████████▊| 7109/7200 [34:56<01:08,  1.32pipeline/s]Optimization Progress: 100%|█████████▉| 7189/7200 [34:58<00:05,  1.86pipeline/s]
Generation 71 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7200/7200 [34:59<00:00,  1.86pipeline/s]Optimization Progress: 100%|██████████| 7200/7200 [34:59<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7200/7200 [34:59<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 7200/7200 [34:59<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7200/7200 [35:00<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7200/7200 [35:00<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 7200/7200 [35:01<00:00,  2.54pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:16:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 7200/7200 [35:02<00:00,  2.54pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7201/7300 [35:02<00:38,  2.54pipeline/s]Optimization Progress:  99%|█████████▊| 7202/7300 [35:02<01:09,  1.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7202/7300 [35:02<01:09,  1.41pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7203/7300 [35:02<01:08,  1.41pipeline/s]Optimization Progress:  99%|█████████▊| 7205/7300 [35:06<01:30,  1.05pipeline/s]Optimization Progress: 100%|█████████▉| 7285/7300 [35:09<00:10,  1.47pipeline/s]
Generation 72 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-196666303.6633542	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7300/7300 [35:11<00:00,  1.47pipeline/s]Optimization Progress: 100%|██████████| 7300/7300 [35:11<00:00,  1.96pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:16:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 7300/7300 [35:12<00:00,  1.96pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:16:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 7300/7300 [35:12<00:00,  1.96pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 7300/7300 [35:12<00:00,  1.96pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7300/7300 [35:13<00:00,  1.96pipeline/s]Optimization Progress:  99%|█████████▊| 7306/7400 [35:13<00:42,  2.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7306/7400 [35:13<00:42,  2.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7307/7400 [35:13<00:42,  2.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7308/7400 [35:13<00:42,  2.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7309/7400 [35:13<00:41,  2.19pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7310/7400 [35:13<00:41,  2.19pipeline/s]Optimization Progress:  99%|█████████▉| 7312/7400 [35:18<00:48,  1.81pipeline/s]Optimization Progress: 100%|█████████▉| 7392/7400 [35:21<00:03,  2.51pipeline/s]
Generation 73 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199771266.69390994	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-162519570.23748678	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7400/7400 [35:21<00:00,  2.51pipeline/s]Optimization Progress: 100%|██████████| 7400/7400 [35:21<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 7400/7400 [35:21<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 7400/7400 [35:22<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7400/7400 [35:22<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7400/7400 [35:23<00:00,  3.44pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 7400/7400 [35:23<00:00,  3.44pipeline/s]Optimization Progress:  99%|█████████▊| 7403/7500 [35:24<00:45,  2.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7403/7500 [35:24<00:45,  2.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7404/7500 [35:24<00:44,  2.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7405/7500 [35:24<00:44,  2.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7406/7500 [35:24<00:43,  2.14pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7407/7500 [35:24<00:43,  2.14pipeline/s]Optimization Progress:  99%|█████████▉| 7409/7500 [35:27<00:46,  1.97pipeline/s]Optimization Progress: 100%|█████████▉| 7489/7500 [35:29<00:03,  2.76pipeline/s]
Generation 74 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-162519570.23748678	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7500/7500 [35:29<00:00,  2.76pipeline/s]Optimization Progress: 100%|██████████| 7500/7500 [35:29<00:00,  3.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 7500/7500 [35:29<00:00,  3.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7500/7500 [35:29<00:00,  3.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7500/7500 [35:30<00:00,  3.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 7500/7500 [35:30<00:00,  3.76pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7500/7500 [35:31<00:00,  3.76pipeline/s]Optimization Progress:  99%|█████████▊| 7503/7600 [35:32<00:42,  2.27pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7503/7600 [35:32<00:42,  2.27pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▊| 7504/7600 [35:32<00:42,  2.27pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7505/7600 [35:32<00:41,  2.27pipeline/s]Optimization Progress:  99%|█████████▉| 7507/7600 [35:40<01:23,  1.11pipeline/s]Optimization Progress: 100%|█████████▉| 7587/7600 [35:42<00:08,  1.57pipeline/s]
Generation 75 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-196950187.91112936	LinearSVR(Normalizer(Nystroem(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 7600/7600 [35:42<00:00,  1.57pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 7600/7600 [35:42<00:00,  1.57pipeline/s]Optimization Progress: 100%|██████████| 7600/7600 [35:42<00:00,  2.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7600/7600 [35:43<00:00,  2.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 7600/7600 [35:44<00:00,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 7604/7700 [35:45<00:48,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7604/7700 [35:45<00:48,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7605/7700 [35:45<00:48,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7606/7700 [35:45<00:47,  1.98pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7607/7700 [35:45<00:47,  1.98pipeline/s]Optimization Progress:  99%|█████████▉| 7609/7700 [35:50<01:00,  1.50pipeline/s]Optimization Progress: 100%|█████████▉| 7689/7700 [35:53<00:05,  2.09pipeline/s]
Generation 76 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7700/7700 [35:54<00:00,  2.09pipeline/s]Optimization Progress: 100%|██████████| 7700/7700 [35:54<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:16:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 7700/7700 [35:54<00:00,  2.89pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:17:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 7700/7700 [35:55<00:00,  2.89pipeline/s]Optimization Progress:  99%|█████████▉| 7703/7800 [35:56<00:46,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7703/7800 [35:56<00:46,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7704/7800 [35:56<00:46,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7705/7800 [35:56<00:45,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7706/7800 [35:56<00:45,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7707/7800 [35:56<00:44,  2.07pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7708/7800 [35:56<00:44,  2.07pipeline/s]Optimization Progress:  99%|█████████▉| 7710/7800 [36:01<00:48,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 7790/7800 [36:03<00:03,  2.61pipeline/s]
Generation 77 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7800/7800 [36:04<00:00,  2.61pipeline/s]Optimization Progress: 100%|██████████| 7800/7800 [36:04<00:00,  3.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 7800/7800 [36:05<00:00,  3.38pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 7800/7800 [36:06<00:00,  3.38pipeline/s]Optimization Progress:  99%|█████████▉| 7805/7900 [36:06<00:33,  2.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7805/7900 [36:06<00:33,  2.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7806/7900 [36:06<00:32,  2.87pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7807/7900 [36:06<00:32,  2.87pipeline/s]Optimization Progress:  99%|█████████▉| 7809/7900 [36:10<00:45,  2.01pipeline/s]Optimization Progress: 100%|█████████▉| 7889/7900 [36:17<00:04,  2.67pipeline/s]
Generation 78 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 7900/7900 [36:17<00:00,  2.67pipeline/s]Optimization Progress: 100%|██████████| 7900/7900 [36:17<00:00,  3.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 7900/7900 [36:18<00:00,  3.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 7900/7900 [36:19<00:00,  3.69pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 7900/7900 [36:20<00:00,  3.69pipeline/s]Optimization Progress:  99%|█████████▉| 7902/8000 [36:20<01:08,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7902/8000 [36:20<01:08,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7903/8000 [36:20<01:07,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7904/8000 [36:20<01:06,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7905/8000 [36:20<01:06,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7906/8000 [36:20<01:05,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7907/8000 [36:20<01:04,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7908/8000 [36:20<01:04,  1.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 7909/8000 [36:20<01:03,  1.43pipeline/s]Optimization Progress:  99%|█████████▉| 7911/8000 [36:25<00:56,  1.59pipeline/s]Optimization Progress: 100%|█████████▉| 7991/8000 [36:27<00:04,  2.23pipeline/s]
Generation 79 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-201060590.1053655	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8000/8000 [36:27<00:00,  2.23pipeline/s]Optimization Progress: 100%|██████████| 8000/8000 [36:27<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8000/8000 [36:27<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8000/8000 [36:27<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:17:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8000/8000 [36:27<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8000/8000 [36:30<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 8000/8000 [36:30<00:00,  3.09pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8000/8000 [36:30<00:00,  3.09pipeline/s]Optimization Progress:  99%|█████████▉| 8003/8100 [36:30<00:57,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8003/8100 [36:30<00:57,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8004/8100 [36:30<00:56,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8005/8100 [36:30<00:55,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8006/8100 [36:30<00:55,  1.70pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8007/8100 [36:30<00:54,  1.70pipeline/s]Optimization Progress:  99%|█████████▉| 8009/8100 [36:33<00:51,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 8089/8100 [36:38<00:04,  2.43pipeline/s]
Generation 80 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [36:38<00:00,  2.43pipeline/s]Optimization Progress: 100%|██████████| 8100/8100 [36:38<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [36:39<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [36:39<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:17:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8100/8100 [36:40<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8100/8100 [36:40<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:17:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8100/8100 [36:40<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8100/8100 [36:41<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8100/8100 [36:41<00:00,  3.43pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8100/8100 [36:41<00:00,  3.43pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8103/8200 [36:41<00:28,  3.43pipeline/s]Optimization Progress:  99%|█████████▉| 8104/8200 [36:41<00:40,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8104/8200 [36:41<00:40,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8105/8200 [36:41<00:40,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8106/8200 [36:41<00:39,  2.36pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8107/8200 [36:41<00:39,  2.36pipeline/s]Optimization Progress:  99%|█████████▉| 8109/8200 [36:52<01:23,  1.09pipeline/s]Optimization Progress: 100%|█████████▉| 8189/8200 [36:53<00:07,  1.55pipeline/s]
Generation 81 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199656117.39334846	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8200/8200 [36:53<00:00,  1.55pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 8200/8200 [36:53<00:00,  1.55pipeline/s]Optimization Progress: 100%|██████████| 8200/8200 [36:53<00:00,  2.17pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8200/8200 [36:54<00:00,  2.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8200/8300 [36:56<00:46,  2.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8201/8300 [36:56<00:45,  2.17pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8202/8300 [36:56<00:45,  2.17pipeline/s]Optimization Progress:  99%|█████████▉| 8203/8300 [36:56<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8203/8300 [36:56<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8204/8300 [36:56<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8205/8300 [36:56<00:56,  1.68pipeline/s]Optimization Progress:  99%|█████████▉| 8207/8300 [37:04<01:35,  1.03s/pipeline]Optimization Progress: 100%|█████████▉| 8287/8300 [37:06<00:09,  1.38pipeline/s]
Generation 82 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8300/8300 [37:06<00:00,  1.38pipeline/s]Optimization Progress: 100%|██████████| 8300/8300 [37:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [37:07<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8300/8300 [37:07<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8300/8300 [37:08<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8300/8300 [37:08<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8300/8300 [37:08<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 8300/8300 [37:09<00:00,  1.91pipeline/s]Optimization Progress:  99%|█████████▉| 8303/8400 [37:09<01:00,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8303/8400 [37:09<01:00,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8304/8400 [37:09<00:59,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8305/8400 [37:09<00:59,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8306/8400 [37:09<00:58,  1.60pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8307/8400 [37:09<00:58,  1.60pipeline/s]Optimization Progress:  99%|█████████▉| 8309/8400 [37:14<01:01,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 8389/8400 [37:15<00:05,  2.09pipeline/s]
Generation 83 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [37:15<00:00,  2.09pipeline/s]Optimization Progress: 100%|██████████| 8400/8400 [37:15<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8400/8400 [37:15<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 8400/8400 [37:16<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8400/8400 [37:16<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8400/8400 [37:16<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8400/8400 [37:17<00:00,  2.94pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 8400/8400 [37:18<00:00,  2.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8400/8500 [37:19<00:34,  2.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8401/8500 [37:19<00:33,  2.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8402/8500 [37:19<00:33,  2.94pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8403/8500 [37:19<00:33,  2.94pipeline/s]Optimization Progress:  99%|█████████▉| 8404/8500 [37:19<00:48,  1.99pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8404/8500 [37:19<00:48,  1.99pipeline/s]Optimization Progress:  99%|█████████▉| 8407/8500 [37:23<01:08,  1.35pipeline/s]Optimization Progress: 100%|█████████▉| 8486/8500 [37:30<00:07,  1.84pipeline/s]
Generation 84 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-153082197.01342526	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8500/8500 [37:30<00:00,  1.84pipeline/s]Optimization Progress: 100%|██████████| 8500/8500 [37:30<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8500/8500 [37:30<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [37:30<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8500/8500 [37:31<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [37:31<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [37:32<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 8500/8500 [37:32<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [37:32<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8500/8500 [37:32<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8500/8500 [37:33<00:00,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 8503/8600 [37:33<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8503/8600 [37:33<00:57,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8504/8600 [37:33<00:56,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8505/8600 [37:33<00:56,  1.68pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8506/8600 [37:33<00:55,  1.68pipeline/s]Optimization Progress:  99%|█████████▉| 8508/8600 [37:37<00:57,  1.60pipeline/s]Optimization Progress: 100%|█████████▉| 8588/8600 [37:38<00:05,  2.27pipeline/s]
Generation 85 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8600/8600 [37:38<00:00,  2.27pipeline/s]Optimization Progress: 100%|██████████| 8600/8600 [37:38<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8600/8600 [37:38<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8600/8600 [37:38<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:18:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8600/8600 [37:40<00:00,  3.15pipeline/s]Optimization Progress:  99%|█████████▉| 8603/8700 [37:41<00:46,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8603/8700 [37:41<00:46,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8604/8700 [37:41<00:45,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8605/8700 [37:41<00:45,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8606/8700 [37:41<00:45,  2.09pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8607/8700 [37:41<00:44,  2.09pipeline/s]Optimization Progress:  99%|█████████▉| 8609/8700 [37:44<00:47,  1.94pipeline/s]Optimization Progress: 100%|█████████▉| 8689/8700 [37:48<00:04,  2.68pipeline/s]
Generation 86 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200220620.44794375	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [37:49<00:00,  2.68pipeline/s]Optimization Progress: 100%|██████████| 8700/8700 [37:49<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [37:49<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8700/8700 [37:49<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8700/8700 [37:50<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8700/8700 [37:50<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 8700/8700 [37:50<00:00,  3.37pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8700/8700 [37:51<00:00,  3.37pipeline/s]Optimization Progress:  99%|█████████▉| 8705/8800 [37:51<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8705/8800 [37:51<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8706/8800 [37:51<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8707/8800 [37:51<00:33,  2.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8708/8800 [37:51<00:32,  2.81pipeline/s]Optimization Progress:  99%|█████████▉| 8710/8800 [37:58<01:00,  1.49pipeline/s]Optimization Progress: 100%|█████████▉| 8790/8800 [38:00<00:04,  2.10pipeline/s]
Generation 87 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8800/8800 [38:01<00:00,  2.10pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 8800/8800 [38:01<00:00,  2.10pipeline/s]Optimization Progress: 100%|██████████| 8800/8800 [38:01<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 8800/8800 [38:01<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 8800/8800 [38:01<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 8800/8800 [38:02<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:19:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8800/8800 [38:03<00:00,  2.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 8800/8800 [38:03<00:00,  2.91pipeline/s]Optimization Progress:  99%|█████████▉| 8806/8900 [38:04<00:35,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8806/8900 [38:04<00:35,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8807/8900 [38:04<00:35,  2.62pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8808/8900 [38:04<00:35,  2.62pipeline/s]Optimization Progress:  99%|█████████▉| 8810/8900 [38:11<01:11,  1.25pipeline/s]Optimization Progress: 100%|█████████▉| 8890/8900 [38:13<00:05,  1.77pipeline/s]
Generation 88 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 8900/8900 [38:13<00:00,  1.77pipeline/s]Optimization Progress: 100%|██████████| 8900/8900 [38:13<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:19:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:19:19] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8900/8900 [38:14<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 8900/8900 [38:15<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 8900/8900 [38:15<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 8900/8900 [38:15<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 8900/8900 [38:16<00:00,  2.40pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 8900/8900 [38:16<00:00,  2.40pipeline/s]Optimization Progress:  99%|█████████▉| 8903/9000 [38:17<00:58,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8903/9000 [38:17<00:58,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8904/9000 [38:17<00:58,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8905/9000 [38:17<00:57,  1.65pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 8906/9000 [38:17<00:57,  1.65pipeline/s]Optimization Progress:  99%|█████████▉| 8908/9000 [38:20<00:59,  1.55pipeline/s]Optimization Progress: 100%|█████████▉| 8988/9000 [38:22<00:05,  2.18pipeline/s]
Generation 89 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9000/9000 [38:22<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:19:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9000/9000 [38:22<00:00,  2.18pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9000/9000 [38:23<00:00,  2.18pipeline/s]Optimization Progress: 100%|██████████| 9000/9000 [38:23<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9000/9000 [38:23<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9000/9000 [38:23<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9000/9000 [38:23<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9000/9000 [38:24<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:19:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9000/9000 [38:24<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 9000/9000 [38:24<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9000/9000 [38:25<00:00,  2.97pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9000/9000 [38:25<00:00,  2.97pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9001/9100 [38:25<00:33,  2.97pipeline/s]Optimization Progress:  99%|█████████▉| 9002/9100 [38:25<00:54,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9002/9100 [38:25<00:54,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9003/9100 [38:25<00:53,  1.81pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9004/9100 [38:25<00:53,  1.81pipeline/s]Optimization Progress:  99%|█████████▉| 9006/9100 [38:29<01:07,  1.40pipeline/s]Optimization Progress: 100%|█████████▉| 9086/9100 [38:31<00:07,  1.98pipeline/s]
Generation 90 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9100/9100 [38:31<00:00,  1.98pipeline/s]Optimization Progress: 100%|██████████| 9100/9100 [38:31<00:00,  2.78pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 9100/9100 [38:32<00:00,  2.78pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:19:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9100/9100 [38:33<00:00,  2.78pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9100/9100 [38:33<00:00,  2.78pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9100/9100 [38:34<00:00,  2.78pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9100/9100 [38:34<00:00,  2.78pipeline/s]Optimization Progress:  99%|█████████▉| 9105/9200 [38:34<00:39,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9105/9200 [38:34<00:39,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9106/9200 [38:34<00:39,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9107/9200 [38:34<00:38,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9108/9200 [38:34<00:38,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9109/9200 [38:34<00:37,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9110/9200 [38:34<00:37,  2.40pipeline/s]Optimization Progress:  99%|█████████▉| 9112/9200 [38:38<00:40,  2.16pipeline/s]Optimization Progress: 100%|█████████▉| 9192/9200 [38:40<00:02,  3.00pipeline/s]
Generation 91 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9200/9200 [38:41<00:00,  3.00pipeline/s]Optimization Progress: 100%|██████████| 9200/9200 [38:41<00:00,  3.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9200/9200 [38:43<00:00,  3.84pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9200/9200 [38:43<00:00,  3.84pipeline/s]Optimization Progress:  99%|█████████▉| 9205/9300 [38:44<00:32,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9205/9300 [38:44<00:32,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9206/9300 [38:44<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9207/9300 [38:44<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9208/9300 [38:44<00:31,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9209/9300 [38:44<00:30,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9210/9300 [38:44<00:30,  2.96pipeline/s]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 9211/9300 [38:44<00:30,  2.96pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9212/9300 [38:44<00:29,  2.96pipeline/s]Optimization Progress:  99%|█████████▉| 9214/9300 [38:48<00:33,  2.60pipeline/s]Optimization Progress: 100%|█████████▉| 9294/9300 [38:50<00:01,  3.64pipeline/s]
Generation 92 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 9300/9300 [38:50<00:00,  3.64pipeline/s]Optimization Progress: 100%|██████████| 9300/9300 [38:50<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9300/9300 [38:51<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 [07:19:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9300/9300 [38:52<00:00,  4.21pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9300/9300 [38:53<00:00,  4.21pipeline/s]Optimization Progress:  99%|█████████▉| 9303/9400 [38:53<00:40,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9303/9400 [38:53<00:40,  2.40pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9304/9400 [38:53<00:39,  2.40pipeline/s]Optimization Progress:  99%|█████████▉| 9306/9400 [39:00<01:37,  1.03s/pipeline]Optimization Progress: 100%|█████████▉| 9386/9400 [39:05<00:10,  1.35pipeline/s]
Generation 93 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.35pipeline/s]Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [39:05<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9400/9400 [39:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9400/9400 [39:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [39:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:11] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [39:06<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9400/9400 [39:07<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9400/9400 [39:07<00:00,  1.91pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9400/9400 [39:07<00:00,  1.91pipeline/s]Optimization Progress:  99%|█████████▉| 9404/9500 [39:08<00:54,  1.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9404/9500 [39:08<00:54,  1.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9405/9500 [39:08<00:53,  1.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9406/9500 [39:08<00:53,  1.76pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9407/9500 [39:08<00:52,  1.76pipeline/s]Optimization Progress:  99%|█████████▉| 9409/9500 [39:11<00:55,  1.65pipeline/s]Optimization Progress: 100%|█████████▉| 9489/9500 [39:15<00:04,  2.29pipeline/s]
Generation 94 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-186991538.17554224	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9500/9500 [39:15<00:00,  2.29pipeline/s]Optimization Progress: 100%|██████████| 9500/9500 [39:15<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9500/9500 [39:16<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 9500/9500 [39:16<00:00,  3.15pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9500/9500 [39:17<00:00,  3.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9500/9600 [39:18<00:31,  3.15pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9501/9600 [39:18<00:31,  3.15pipeline/s]Optimization Progress:  99%|█████████▉| 9502/9600 [39:18<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9502/9600 [39:18<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9503/9600 [39:18<01:05,  1.49pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9504/9600 [39:18<01:04,  1.49pipeline/s]Optimization Progress:  99%|█████████▉| 9506/9600 [39:22<01:12,  1.30pipeline/s]Optimization Progress: 100%|█████████▉| 9586/9600 [39:24<00:07,  1.83pipeline/s]
Generation 95 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9600/9600 [39:24<00:00,  1.83pipeline/s]Optimization Progress: 100%|██████████| 9600/9600 [39:24<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 9600/9600 [39:25<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9600/9600 [39:26<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 9600/9600 [39:26<00:00,  2.58pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 9600/9600 [39:27<00:00,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 9604/9700 [39:27<00:49,  1.92pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9604/9700 [39:27<00:49,  1.92pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9605/9700 [39:27<00:49,  1.92pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9606/9700 [39:27<00:48,  1.92pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9607/9700 [39:27<00:48,  1.92pipeline/s]Optimization Progress:  99%|█████████▉| 9609/9700 [39:32<00:57,  1.59pipeline/s]Optimization Progress: 100%|█████████▉| 9689/9700 [39:35<00:04,  2.21pipeline/s]
Generation 96 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  2.21pipeline/s]Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9700/9700 [39:36<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [39:37<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [39:37<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9700/9700 [39:37<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9700/9700 [39:38<00:00,  3.03pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9700/9700 [39:38<00:00,  3.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9700/9800 [39:39<00:32,  3.03pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9701/9800 [39:39<00:32,  3.03pipeline/s]Optimization Progress:  99%|█████████▉| 9703/9800 [39:44<01:44,  1.08s/pipeline]Optimization Progress: 100%|█████████▉| 9783/9800 [39:47<00:13,  1.30pipeline/s]
Generation 97 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 9800/9800 [39:48<00:00,  1.30pipeline/s]Optimization Progress: 100%|██████████| 9800/9800 [39:48<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:48<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9800/9800 [39:49<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9800/9800 [39:50<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9800/9800 [39:50<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:20:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9800/9800 [39:50<00:00,  1.86pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 9800/9800 [39:50<00:00,  1.86pipeline/s]Optimization Progress:  99%|█████████▉| 9806/9900 [39:50<00:48,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9806/9900 [39:50<00:48,  1.95pipeline/s]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9807/9900 [39:50<00:47,  1.95pipeline/s]Optimization Progress:  99%|█████████▉| 9810/9900 [39:54<00:55,  1.63pipeline/s]Optimization Progress: 100%|█████████▉| 9889/9900 [39:55<00:04,  2.30pipeline/s]
Generation 98 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [07:21:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 9900/9900 [39:55<00:00,  2.30pipeline/s]Optimization Progress: 100%|██████████| 9900/9900 [39:55<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [39:55<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9900/9900 [39:55<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 9900/9900 [39:57<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9900/9900 [39:57<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 9900/9900 [39:57<00:00,  3.25pipeline/s]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 9900/9900 [39:57<00:00,  3.25pipeline/s]Optimization Progress:  99%|█████████▉| 9908/10000 [39:58<00:30,  3.00pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9908/10000 [39:58<00:30,  3.00pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9909/10000 [39:58<00:30,  3.00pipeline/s]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 9910/10000 [39:58<00:29,  3.00pipeline/s]Optimization Progress:  99%|█████████▉| 9912/10000 [40:03<00:52,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 9992/10000 [40:04<00:03,  2.38pipeline/s]
Generation 99 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10000/10000 [40:05<00:00,  2.38pipeline/s]Optimization Progress: 100%|██████████| 10000/10000 [40:05<00:00,  3.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 10000/10000 [40:05<00:00,  3.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10000/10000 [40:05<00:00,  3.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 10000/10000 [40:05<00:00,  3.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 10000/10000 [40:06<00:00,  3.03pipeline/s]Optimization Progress:  99%|█████████▉| 10006/10100 [40:07<00:32,  2.94pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10006/10100 [40:07<00:32,  2.94pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10007/10100 [40:07<00:31,  2.94pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10008/10100 [40:07<00:31,  2.94pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10009/10100 [40:07<00:30,  2.94pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10010/10100 [40:07<00:30,  2.94pipeline/s]Optimization Progress:  99%|█████████▉| 10012/10100 [40:12<00:44,  1.98pipeline/s]Optimization Progress: 100%|█████████▉| 10092/10100 [40:14<00:02,  2.78pipeline/s]
Generation 100 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10100/10100 [40:14<00:00,  2.78pipeline/s]Optimization Progress: 100%|██████████| 10100/10100 [40:14<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10100/10100 [40:15<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:21:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:21:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10100/10100 [40:16<00:00,  3.61pipeline/s]Optimization Progress:  99%|█████████▉| 10106/10200 [40:17<00:29,  3.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10106/10200 [40:17<00:29,  3.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10107/10200 [40:17<00:28,  3.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10108/10200 [40:17<00:28,  3.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10109/10200 [40:17<00:28,  3.21pipeline/s]Optimization Progress:  99%|█████████▉| 10111/10200 [40:20<00:37,  2.40pipeline/s]Optimization Progress: 100%|█████████▉| 10191/10200 [40:23<00:02,  3.33pipeline/s]
Generation 101 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10200/10200 [40:23<00:00,  3.33pipeline/s]Optimization Progress: 100%|██████████| 10200/10200 [40:23<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:21:28] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10200/10200 [40:23<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:21:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10200/10200 [40:24<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 10200/10200 [40:25<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10200/10200 [40:25<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 10200/10200 [40:25<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10200/10200 [40:26<00:00,  4.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10200/10200 [40:26<00:00,  4.58pipeline/s]Optimization Progress:  99%|█████████▉| 10207/10300 [40:26<00:26,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10207/10300 [40:26<00:26,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10208/10300 [40:26<00:26,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10209/10300 [40:26<00:25,  3.53pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10210/10300 [40:26<00:25,  3.53pipeline/s]Optimization Progress:  99%|█████████▉| 10212/10300 [40:38<01:21,  1.09pipeline/s]Optimization Progress: 100%|█████████▉| 10292/10300 [40:40<00:05,  1.53pipeline/s]
Generation 102 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 10300/10300 [40:41<00:00,  1.53pipeline/s]Optimization Progress: 100%|██████████| 10300/10300 [40:41<00:00,  2.11pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 10300/10300 [40:41<00:00,  2.11pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 10300/10300 [40:42<00:00,  2.11pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10300/10300 [40:42<00:00,  2.11pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 10300/10300 [40:44<00:00,  2.11pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10301/10400 [40:44<00:46,  2.11pipeline/s]Optimization Progress:  99%|█████████▉| 10302/10400 [40:44<01:21,  1.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10302/10400 [40:44<01:21,  1.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10303/10400 [40:44<01:20,  1.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10304/10400 [40:44<01:19,  1.21pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10305/10400 [40:44<01:18,  1.21pipeline/s]Optimization Progress:  99%|█████████▉| 10307/10400 [40:48<01:16,  1.21pipeline/s]Optimization Progress: 100%|█████████▉| 10387/10400 [40:49<00:07,  1.72pipeline/s]
Generation 103 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10400/10400 [40:50<00:00,  1.72pipeline/s]Optimization Progress: 100%|██████████| 10400/10400 [40:50<00:00,  2.30pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10400/10400 [40:51<00:00,  2.30pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 10400/10400 [40:52<00:00,  2.30pipeline/s]Optimization Progress:  99%|█████████▉| 10403/10500 [40:52<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10403/10500 [40:52<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10404/10500 [40:52<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10405/10500 [40:52<00:48,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10406/10500 [40:52<00:48,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10407/10500 [40:53<00:47,  1.95pipeline/s]Optimization Progress:  99%|█████████▉| 10409/10500 [40:58<00:56,  1.60pipeline/s]Optimization Progress: 100%|█████████▉| 10489/10500 [41:00<00:04,  2.24pipeline/s]
Generation 104 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 10500/10500 [41:00<00:00,  2.24pipeline/s]Optimization Progress: 100%|██████████| 10500/10500 [41:00<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10500/10500 [41:01<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10500/10500 [41:02<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10500/10500 [41:02<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10500/10500 [41:02<00:00,  3.17pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10500/10500 [41:03<00:00,  3.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10501/10600 [41:03<00:31,  3.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10502/10600 [41:03<00:30,  3.17pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10503/10600 [41:03<00:30,  3.17pipeline/s]Optimization Progress:  99%|█████████▉| 10505/10600 [41:06<00:56,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 10585/10600 [41:10<00:06,  2.35pipeline/s]
Generation 105 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-155593633.98178977	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-150778423.85355192	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(CombineDFs(input_matrix, input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-144362815.7115571	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10600/10600 [41:10<00:00,  2.35pipeline/s]Optimization Progress: 100%|██████████| 10600/10600 [41:10<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 10600/10600 [41:10<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 10600/10600 [41:11<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 10600/10600 [41:11<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 10600/10600 [41:12<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10600/10600 [41:13<00:00,  3.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 10600/10600 [41:13<00:00,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10603/10700 [41:14<00:29,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10604/10700 [41:14<00:28,  3.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10605/10700 [41:14<00:28,  3.32pipeline/s]Optimization Progress:  99%|█████████▉| 10606/10700 [41:14<00:39,  2.41pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10606/10700 [41:14<00:39,  2.41pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10607/10700 [41:14<00:38,  2.41pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10608/10700 [41:14<00:38,  2.41pipeline/s]Optimization Progress:  99%|█████████▉| 10610/10700 [41:18<00:55,  1.63pipeline/s]Optimization Progress: 100%|█████████▉| 10690/10700 [41:20<00:04,  2.29pipeline/s]
Generation 106 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  2.29pipeline/s]Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:26] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10700/10700 [41:21<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10700/10700 [41:22<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10700/10700 [41:22<00:00,  3.20pipeline/s]Optimization Progress:  99%|█████████▉| 10709/10800 [41:23<00:26,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10709/10800 [41:23<00:26,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10710/10800 [41:23<00:26,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10711/10800 [41:23<00:26,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10712/10800 [41:23<00:25,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10713/10800 [41:23<00:25,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10714/10800 [41:23<00:25,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10715/10800 [41:23<00:24,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10716/10800 [41:23<00:24,  3.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10717/10800 [41:23<00:24,  3.42pipeline/s]Optimization Progress:  99%|█████████▉| 10719/10800 [41:26<00:24,  3.27pipeline/s]Optimization Progress: 100%|█████████▉| 10799/10800 [41:27<00:00,  4.64pipeline/s]
Generation 107 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10800/10800 [41:27<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10800/10800 [41:28<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10800/10800 [41:28<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 10800/10800 [41:29<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 10800/10800 [41:29<00:00,  4.64pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 10800/10800 [41:30<00:00,  4.64pipeline/s]Optimization Progress:  99%|█████████▉| 10807/10900 [41:30<00:25,  3.69pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10807/10900 [41:30<00:25,  3.69pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10808/10900 [41:30<00:24,  3.69pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10809/10900 [41:30<00:24,  3.69pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10810/10900 [41:30<00:24,  3.69pipeline/s]Optimization Progress:  99%|█████████▉| 10812/10900 [41:34<00:41,  2.12pipeline/s]Optimization Progress: 100%|█████████▉| 10892/10900 [41:35<00:02,  3.00pipeline/s]
Generation 108 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 10900/10900 [41:36<00:00,  3.00pipeline/s]Optimization Progress: 100%|██████████| 10900/10900 [41:36<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 10900/10900 [41:36<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10900/10900 [41:36<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10900/10900 [41:36<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10900/10900 [41:37<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 10900/10900 [41:37<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10900/10900 [41:37<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 10900/10900 [41:37<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 10900/10900 [41:38<00:00,  3.95pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:22:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 10900/10900 [41:38<00:00,  3.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10903/11000 [41:39<00:24,  3.95pipeline/s]Optimization Progress:  99%|█████████▉| 10904/11000 [41:39<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10904/11000 [41:39<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10905/11000 [41:39<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 10906/11000 [41:39<00:40,  2.35pipeline/s]Optimization Progress:  99%|█████████▉| 10907/11000 [41:50<00:39,  2.35pipeline/s]Optimization Progress:  99%|█████████▉| 10908/11000 [41:53<02:01,  1.32s/pipeline]Optimization Progress: 100%|█████████▉| 10988/11000 [41:55<00:11,  1.08pipeline/s]
Generation 109 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11000/11000 [41:56<00:00,  1.08pipeline/s]Optimization Progress: 100%|██████████| 11000/11000 [41:56<00:00,  1.47pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11000/11000 [41:56<00:00,  1.47pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11000/11000 [41:56<00:00,  1.47pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11000/11000 [41:57<00:00,  1.47pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11000/11000 [41:57<00:00,  1.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11000/11100 [41:58<01:07,  1.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11001/11100 [41:58<01:07,  1.47pipeline/s]Optimization Progress:  99%|█████████▉| 11002/11100 [41:58<01:21,  1.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11002/11100 [41:58<01:21,  1.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11003/11100 [41:58<01:20,  1.20pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11004/11100 [41:58<01:20,  1.20pipeline/s]Optimization Progress:  99%|█████████▉| 11006/11100 [42:03<01:29,  1.06pipeline/s]Optimization Progress: 100%|█████████▉| 11086/11100 [42:06<00:09,  1.49pipeline/s]
Generation 110 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 11100/11100 [42:06<00:00,  1.49pipeline/s]Optimization Progress: 100%|██████████| 11100/11100 [42:06<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11100/11100 [42:06<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 11100/11100 [42:06<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:23:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11100/11100 [42:06<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11100/11100 [42:07<00:00,  2.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11100/11100 [42:08<00:00,  2.10pipeline/s]Optimization Progress:  99%|█████████▉| 11106/11200 [42:10<00:49,  1.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11106/11200 [42:10<00:49,  1.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11107/11200 [42:10<00:48,  1.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11108/11200 [42:10<00:48,  1.90pipeline/s]Optimization Progress:  99%|█████████▉| 11110/11200 [42:14<01:00,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 11190/11200 [42:17<00:04,  2.06pipeline/s]
Generation 111 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 11200/11200 [42:18<00:00,  2.06pipeline/s]Optimization Progress: 100%|██████████| 11200/11200 [42:18<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11200/11200 [42:18<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11200/11200 [42:18<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 11200/11200 [42:19<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 11200/11200 [42:19<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11200/11200 [42:19<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11200/11200 [42:20<00:00,  2.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11200/11200 [42:20<00:00,  2.78pipeline/s]Optimization Progress:  99%|█████████▉| 11202/11300 [42:21<01:05,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11202/11300 [42:21<01:05,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11203/11300 [42:21<01:05,  1.49pipeline/s]Optimization Progress:  99%|█████████▉| 11205/11300 [42:26<01:38,  1.04s/pipeline]Optimization Progress: 100%|█████████▉| 11285/11300 [42:29<00:11,  1.36pipeline/s]
Generation 112 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11300/11300 [42:29<00:00,  1.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 11300/11300 [42:29<00:00,  1.36pipeline/s]Optimization Progress: 100%|██████████| 11300/11300 [42:29<00:00,  1.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11300/11300 [42:30<00:00,  1.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 11300/11300 [42:30<00:00,  1.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 11300/11300 [42:30<00:00,  1.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11300/11300 [42:32<00:00,  1.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 11300/11300 [42:32<00:00,  1.92pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11303/11400 [42:32<00:50,  1.92pipeline/s]Optimization Progress:  99%|█████████▉| 11304/11400 [42:32<01:01,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11304/11400 [42:32<01:01,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11305/11400 [42:32<01:00,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11306/11400 [42:32<00:59,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11307/11400 [42:32<00:59,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11308/11400 [42:32<00:58,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11309/11400 [42:32<00:57,  1.57pipeline/s]Optimization Progress:  99%|█████████▉| 11311/11400 [42:38<01:00,  1.46pipeline/s]Optimization Progress: 100%|█████████▉| 11391/11400 [42:39<00:04,  2.07pipeline/s]
Generation 113 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 11400/11400 [42:39<00:00,  2.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11400/11400 [42:39<00:00,  2.07pipeline/s]Optimization Progress: 100%|██████████| 11400/11400 [42:39<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11400/11400 [42:40<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11400/11400 [42:41<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 11400/11400 [42:42<00:00,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11403/11500 [42:42<00:33,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 11404/11500 [42:42<00:45,  2.11pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11404/11500 [42:42<00:45,  2.11pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11405/11500 [42:42<00:45,  2.11pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11406/11500 [42:43<00:44,  2.11pipeline/s]Optimization Progress:  99%|█████████▉| 11408/11500 [42:49<01:16,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 11488/11500 [42:54<00:07,  1.67pipeline/s]
Generation 114 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11500/11500 [42:54<00:00,  1.67pipeline/s]Optimization Progress: 100%|██████████| 11500/11500 [42:54<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11500/11500 [42:56<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11500/11500 [42:56<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 [07:24:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11500/11500 [42:56<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11500/11500 [42:57<00:00,  2.34pipeline/s]Optimization Progress:  99%|█████████▉| 11506/11600 [42:57<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11506/11600 [42:57<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11507/11600 [42:57<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11508/11600 [42:57<00:40,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11509/11600 [42:57<00:40,  2.26pipeline/s]Optimization Progress:  99%|█████████▉| 11511/11600 [43:02<00:56,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 11591/11600 [43:03<00:04,  2.23pipeline/s]
Generation 115 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11600/11600 [43:03<00:00,  2.23pipeline/s]Optimization Progress: 100%|██████████| 11600/11600 [43:03<00:00,  3.15pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11600/11600 [43:04<00:00,  3.15pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11600/11600 [43:04<00:00,  3.15pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 11600/11600 [43:04<00:00,  3.15pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 11600/11600 [43:06<00:00,  3.15pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 11600/11600 [43:06<00:00,  3.15pipeline/s]Optimization Progress:  99%|█████████▉| 11606/11700 [43:06<00:35,  2.64pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11606/11700 [43:06<00:35,  2.64pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11607/11700 [43:06<00:35,  2.64pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11608/11700 [43:06<00:34,  2.64pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11609/11700 [43:06<00:34,  2.64pipeline/s]Optimization Progress:  99%|█████████▉| 11611/11700 [43:11<00:48,  1.84pipeline/s]Optimization Progress: 100%|█████████▉| 11691/11700 [43:12<00:03,  2.61pipeline/s]
Generation 116 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-199639742.4893096	LinearSVR(Normalizer(Nystroem(CombineDFs(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11700/11700 [43:12<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 11700/11700 [43:12<00:00,  2.61pipeline/s]Optimization Progress: 100%|██████████| 11700/11700 [43:12<00:00,  3.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11700/11700 [43:14<00:00,  3.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11700/11700 [43:14<00:00,  3.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11700/11700 [43:14<00:00,  3.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 11700/11700 [43:15<00:00,  3.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11704/11800 [43:15<00:26,  3.66pipeline/s]Optimization Progress:  99%|█████████▉| 11705/11800 [43:15<00:35,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11705/11800 [43:15<00:35,  2.71pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11706/11800 [43:15<00:34,  2.71pipeline/s]Optimization Progress:  99%|█████████▉| 11709/11800 [43:20<00:56,  1.61pipeline/s]Optimization Progress: 100%|█████████▉| 11788/11800 [43:22<00:05,  2.26pipeline/s]
Generation 117 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11800/11800 [43:22<00:00,  2.26pipeline/s]Optimization Progress: 100%|██████████| 11800/11800 [43:22<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11800/11800 [43:24<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 11800/11800 [43:24<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 11800/11800 [43:24<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 11800/11800 [43:24<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 11800/11800 [43:24<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 11800/11800 [43:26<00:00,  3.07pipeline/s]Optimization Progress:  99%|█████████▉| 11809/11900 [43:26<00:32,  2.83pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11809/11900 [43:26<00:32,  2.83pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11810/11900 [43:26<00:31,  2.83pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11811/11900 [43:26<00:31,  2.83pipeline/s]Optimization Progress:  99%|█████████▉| 11813/11900 [43:34<01:16,  1.13pipeline/s]Optimization Progress: 100%|█████████▉| 11893/11900 [43:36<00:04,  1.60pipeline/s]
Generation 118 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 11900/11900 [43:36<00:00,  1.60pipeline/s]Optimization Progress: 100%|██████████| 11900/11900 [43:36<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 11900/11900 [43:37<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11900/11900 [43:37<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 11900/11900 [43:37<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11900/11900 [43:37<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 11900/11900 [43:38<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 11900/11900 [43:38<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 11900/11900 [43:38<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 11900/11900 [43:38<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 11900/11900 [43:39<00:00,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 11909/12000 [43:39<00:38,  2.39pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11909/12000 [43:39<00:38,  2.39pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 11910/12000 [43:39<00:37,  2.39pipeline/s]Optimization Progress:  99%|█████████▉| 11912/12000 [43:49<01:47,  1.22s/pipeline]Optimization Progress: 100%|█████████▉| 11992/12000 [43:49<00:06,  1.16pipeline/s]
Generation 119 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 12000/12000 [43:50<00:00,  1.16pipeline/s]Optimization Progress: 100%|██████████| 12000/12000 [43:50<00:00,  1.59pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 12000/12000 [43:50<00:00,  1.59pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:56] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12000/12000 [43:51<00:00,  1.59pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12000/12000 [43:52<00:00,  1.59pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 12000/12000 [43:53<00:00,  1.59pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:24:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12000/12000 [43:53<00:00,  1.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12002/12100 [43:54<01:01,  1.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12003/12100 [43:54<01:00,  1.59pipeline/s]Optimization Progress:  99%|█████████▉| 12004/12100 [43:54<01:09,  1.38pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12004/12100 [43:54<01:09,  1.38pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12005/12100 [43:54<01:08,  1.38pipeline/s]Optimization Progress:  99%|█████████▉| 12007/12100 [44:01<01:54,  1.23s/pipeline]Optimization Progress: 100%|█████████▉| 12087/12100 [44:03<00:11,  1.15pipeline/s]
Generation 120 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.15pipeline/s]Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:04<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:05<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 12100/12100 [44:05<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12100/12100 [44:06<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12100/12100 [44:07<00:00,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12100/12200 [44:07<01:02,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12101/12200 [44:07<01:01,  1.61pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12102/12200 [44:07<01:00,  1.61pipeline/s]Optimization Progress:  99%|█████████▉| 12103/12200 [44:07<01:11,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12103/12200 [44:07<01:11,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12104/12200 [44:07<01:11,  1.35pipeline/s]Optimization Progress:  99%|█████████▉| 12106/12200 [44:11<01:32,  1.02pipeline/s]Optimization Progress: 100%|█████████▉| 12186/12200 [44:14<00:09,  1.44pipeline/s]
Generation 121 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12200/12200 [44:14<00:00,  1.44pipeline/s]Optimization Progress: 100%|██████████| 12200/12200 [44:14<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12200/12200 [44:15<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 12200/12200 [44:15<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12200/12200 [44:16<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12200/12200 [44:16<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 12200/12200 [44:16<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12200/12200 [44:16<00:00,  2.03pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 12200/12200 [44:16<00:00,  2.03pipeline/s]Optimization Progress:  99%|█████████▉| 12208/12300 [44:17<00:41,  2.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12208/12300 [44:17<00:41,  2.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12209/12300 [44:17<00:40,  2.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12210/12300 [44:17<00:40,  2.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12211/12300 [44:17<00:39,  2.24pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12212/12300 [44:17<00:39,  2.24pipeline/s]Optimization Progress:  99%|█████████▉| 12214/12300 [44:24<00:58,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 12294/12300 [44:27<00:02,  2.05pipeline/s]
Generation 122 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-141514962.2442389	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12300/12300 [44:28<00:00,  2.05pipeline/s]Optimization Progress: 100%|██████████| 12300/12300 [44:28<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 12300/12300 [44:28<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12300/12300 [44:29<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12300/12300 [44:30<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12300/12300 [44:30<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12300/12300 [44:30<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12300/12300 [44:30<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12300/12300 [44:31<00:00,  2.80pipeline/s]Optimization Progress:  99%|█████████▉| 12305/12400 [44:31<00:42,  2.25pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12305/12400 [44:31<00:42,  2.25pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12306/12400 [44:31<00:41,  2.25pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12307/12400 [44:31<00:41,  2.25pipeline/s]Optimization Progress:  99%|█████████▉| 12309/12400 [44:35<00:53,  1.69pipeline/s]Optimization Progress: 100%|█████████▉| 12389/12400 [44:38<00:04,  2.36pipeline/s]
Generation 123 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12400/12400 [44:38<00:00,  2.36pipeline/s]Optimization Progress: 100%|██████████| 12400/12400 [44:38<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 12400/12400 [44:39<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12400/12400 [44:39<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12400/12400 [44:40<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:45] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12400/12400 [44:40<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12400/12400 [44:40<00:00,  3.10pipeline/s]Optimization Progress:  99%|█████████▉| 12404/12500 [44:41<00:39,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12404/12500 [44:41<00:39,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12405/12500 [44:41<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12406/12500 [44:41<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12407/12500 [44:41<00:37,  2.45pipeline/s]Optimization Progress:  99%|█████████▉| 12409/12500 [44:45<00:48,  1.88pipeline/s]Optimization Progress: 100%|█████████▉| 12489/12500 [44:47<00:04,  2.64pipeline/s]
Generation 124 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 12500/12500 [44:47<00:00,  2.64pipeline/s]Optimization Progress: 100%|██████████| 12500/12500 [44:47<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12500/12500 [44:48<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12500/12500 [44:48<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:25:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12500/12500 [44:49<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12500/12500 [44:50<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 12500/12500 [44:50<00:00,  3.67pipeline/s]Optimization Progress:  99%|█████████▉| 12505/12600 [44:50<00:36,  2.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12505/12600 [44:50<00:36,  2.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12506/12600 [44:50<00:35,  2.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12507/12600 [44:50<00:35,  2.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12508/12600 [44:50<00:35,  2.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12509/12600 [44:50<00:34,  2.63pipeline/s]Optimization Progress:  99%|█████████▉| 12511/12600 [44:54<00:41,  2.14pipeline/s]Optimization Progress: 100%|█████████▉| 12591/12600 [44:55<00:02,  3.04pipeline/s]
Generation 125 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 12600/12600 [44:55<00:00,  3.04pipeline/s]Optimization Progress: 100%|██████████| 12600/12600 [44:55<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 12600/12600 [44:55<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 12600/12600 [44:56<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12600/12600 [44:56<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12600/12600 [44:56<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12600/12600 [44:57<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 12600/12600 [44:58<00:00,  4.22pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12600/12600 [44:58<00:00,  4.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12603/12700 [44:59<00:22,  4.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12604/12700 [44:59<00:22,  4.22pipeline/s]Optimization Progress:  99%|█████████▉| 12605/12700 [44:59<00:35,  2.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12605/12700 [44:59<00:35,  2.68pipeline/s]Optimization Progress:  99%|█████████▉| 12609/12700 [45:06<01:14,  1.22pipeline/s]Optimization Progress: 100%|█████████▉| 12687/12700 [45:10<00:07,  1.70pipeline/s]
Generation 126 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12700/12700 [45:10<00:00,  1.70pipeline/s]Optimization Progress: 100%|██████████| 12700/12700 [45:10<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12700/12700 [45:11<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12700/12700 [45:11<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:16] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12700/12700 [45:11<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12700/12700 [45:12<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12700/12700 [45:13<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 12700/12700 [45:13<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12700/12700 [45:13<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 12700/12700 [45:13<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 12700/12700 [45:13<00:00,  2.39pipeline/s]Optimization Progress:  99%|█████████▉| 12706/12800 [45:14<00:44,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12706/12800 [45:14<00:44,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12707/12800 [45:14<00:44,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12708/12800 [45:14<00:43,  2.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12709/12800 [45:14<00:43,  2.09pipeline/s]Optimization Progress:  99%|█████████▉| 12711/12800 [45:17<00:46,  1.92pipeline/s]Optimization Progress: 100%|█████████▉| 12791/12800 [45:23<00:03,  2.58pipeline/s]
Generation 127 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12800/12800 [45:23<00:00,  2.58pipeline/s]Optimization Progress: 100%|██████████| 12800/12800 [45:23<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 12800/12800 [45:23<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12800/12800 [45:24<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 12800/12800 [45:25<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 12800/12800 [45:25<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 12800/12800 [45:26<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 12800/12800 [45:26<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12800/12800 [45:26<00:00,  3.46pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12800/12800 [45:27<00:00,  3.46pipeline/s]Optimization Progress:  99%|█████████▉| 12804/12900 [45:27<00:43,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12804/12900 [45:27<00:43,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12805/12900 [45:27<00:42,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12806/12900 [45:27<00:42,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12807/12900 [45:27<00:41,  2.22pipeline/s]Optimization Progress:  99%|█████████▉| 12809/12900 [45:33<01:02,  1.46pipeline/s]Optimization Progress: 100%|█████████▉| 12889/12900 [45:35<00:05,  2.06pipeline/s]
Generation 128 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-132926197.44383204	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 12900/12900 [45:35<00:00,  2.06pipeline/s]Optimization Progress: 100%|██████████| 12900/12900 [45:35<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 12900/12900 [45:35<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 12900/12900 [45:36<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 12900/12900 [45:36<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 12900/12900 [45:36<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12900/12900 [45:36<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12900/12900 [45:37<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:26:42] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 12900/12900 [45:37<00:00,  2.79pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 12900/12900 [45:38<00:00,  2.79pipeline/s]Optimization Progress:  99%|█████████▉| 12902/13000 [45:39<01:14,  1.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12902/13000 [45:39<01:14,  1.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12903/13000 [45:39<01:13,  1.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12904/13000 [45:39<01:12,  1.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12905/13000 [45:39<01:12,  1.32pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 12906/13000 [45:39<01:11,  1.32pipeline/s]Optimization Progress:  99%|█████████▉| 12908/13000 [45:43<01:08,  1.34pipeline/s]Optimization Progress: 100%|█████████▉| 12988/13000 [45:46<00:06,  1.87pipeline/s]
Generation 129 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200209826.8771335	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13000/13000 [45:47<00:00,  1.87pipeline/s]Optimization Progress: 100%|██████████| 13000/13000 [45:47<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 13000/13000 [45:48<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13000/13000 [45:48<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 13000/13000 [45:48<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13000/13000 [45:48<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13000/13000 [45:48<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:26:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13000/13000 [45:49<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 13000/13000 [45:49<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13000/13000 [45:49<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 13000/13000 [45:49<00:00,  2.60pipeline/s]Optimization Progress:  99%|█████████▉| 13002/13100 [45:50<01:05,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13002/13100 [45:50<01:05,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13003/13100 [45:50<01:04,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13004/13100 [45:50<01:04,  1.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13005/13100 [45:50<01:03,  1.49pipeline/s]Optimization Progress:  99%|█████████▉| 13007/13100 [45:55<01:12,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 13087/13100 [45:57<00:07,  1.81pipeline/s]
Generation 130 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13100/13100 [45:57<00:00,  1.81pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 13100/13100 [45:57<00:00,  1.81pipeline/s]Optimization Progress: 100%|██████████| 13100/13100 [45:57<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 13100/13100 [45:57<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13100/13100 [45:58<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13100/13100 [45:58<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 13100/13100 [45:59<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13100/13100 [45:59<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13100/13100 [46:00<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13100/13100 [46:00<00:00,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13102/13200 [46:01<00:38,  2.56pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13103/13200 [46:01<00:37,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 13104/13200 [46:01<00:51,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13104/13200 [46:01<00:51,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13105/13200 [46:01<00:51,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13106/13200 [46:01<00:50,  1.86pipeline/s]Optimization Progress:  99%|█████████▉| 13108/13200 [46:05<01:01,  1.50pipeline/s]Optimization Progress: 100%|█████████▉| 13188/13200 [46:08<00:05,  2.09pipeline/s]
Generation 131 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:27:13] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13200/13200 [46:08<00:00,  2.09pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 13200/13200 [46:08<00:00,  2.09pipeline/s]Optimization Progress: 100%|██████████| 13200/13200 [46:08<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:27:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:09<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress: 100%|██████████| 13200/13200 [46:10<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 13200/13200 [46:10<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:27:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13200/13200 [46:10<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 13200/13200 [46:11<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13200/13200 [46:11<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:11<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13200/13200 [46:11<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13200/13200 [46:11<00:00,  2.92pipeline/s]Optimization Progress:  99%|█████████▉| 13204/13300 [46:11<00:43,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13204/13300 [46:11<00:43,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13205/13300 [46:11<00:42,  2.22pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13206/13300 [46:11<00:42,  2.22pipeline/s]Optimization Progress:  99%|█████████▉| 13208/13300 [46:15<00:54,  1.70pipeline/s]Optimization Progress: 100%|█████████▉| 13288/13300 [46:16<00:04,  2.40pipeline/s]
Generation 132 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13300/13300 [46:16<00:00,  2.40pipeline/s]Optimization Progress: 100%|██████████| 13300/13300 [46:16<00:00,  3.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:27:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13300/13300 [46:16<00:00,  3.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:27:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13300/13300 [46:16<00:00,  3.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13300/13300 [46:16<00:00,  3.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13300/13300 [46:18<00:00,  3.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13300/13300 [46:18<00:00,  3.38pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13304/13400 [46:19<00:28,  3.38pipeline/s]Optimization Progress:  99%|█████████▉| 13305/13400 [46:19<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13305/13400 [46:19<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13306/13400 [46:19<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13307/13400 [46:19<00:38,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13308/13400 [46:19<00:37,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13309/13400 [46:19<00:37,  2.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13310/13400 [46:19<00:36,  2.45pipeline/s]Optimization Progress:  99%|█████████▉| 13312/13400 [46:25<00:46,  1.91pipeline/s]Optimization Progress: 100%|█████████▉| 13392/13400 [46:28<00:03,  2.65pipeline/s]
Generation 133 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198758956.87266544	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13400/13400 [46:28<00:00,  2.65pipeline/s]Optimization Progress: 100%|██████████| 13400/13400 [46:28<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13400/13400 [46:29<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13400/13400 [46:29<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13400/13400 [46:29<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13400/13400 [46:29<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13400/13400 [46:30<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress: 100%|██████████| 13400/13400 [46:31<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13400/13400 [46:31<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13400/13400 [46:31<00:00,  3.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 13400/13400 [46:31<00:00,  3.42pipeline/s]Optimization Progress:  99%|█████████▉| 13406/13500 [46:31<00:32,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13406/13500 [46:31<00:32,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13407/13500 [46:31<00:32,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13408/13500 [46:31<00:32,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13409/13500 [46:31<00:31,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13410/13500 [46:31<00:31,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13411/13500 [46:31<00:31,  2.85pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13412/13500 [46:31<00:30,  2.85pipeline/s]Optimization Progress:  99%|█████████▉| 13414/13500 [46:36<00:36,  2.33pipeline/s]Optimization Progress: 100%|█████████▉| 13494/13500 [46:37<00:01,  3.29pipeline/s]
Generation 134 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13500/13500 [46:37<00:00,  3.29pipeline/s]Optimization Progress: 100%|██████████| 13500/13500 [46:37<00:00,  4.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13500/13500 [46:37<00:00,  4.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 13500/13500 [46:38<00:00,  4.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13500/13500 [46:38<00:00,  4.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13500/13500 [46:39<00:00,  4.42pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13500/13500 [46:39<00:00,  4.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13503/13600 [46:40<00:21,  4.42pipeline/s]Optimization Progress:  99%|█████████▉| 13504/13600 [46:40<00:35,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13504/13600 [46:40<00:35,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13505/13600 [46:40<00:34,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13506/13600 [46:40<00:34,  2.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13507/13600 [46:40<00:34,  2.72pipeline/s]Optimization Progress:  99%|█████████▉| 13509/13600 [46:45<00:52,  1.74pipeline/s]Optimization Progress: 100%|█████████▉| 13589/13600 [46:50<00:04,  2.40pipeline/s]
Generation 135 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13600/13600 [46:50<00:00,  2.40pipeline/s]Optimization Progress: 100%|██████████| 13600/13600 [46:50<00:00,  3.37pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:27:55] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13600/13600 [46:50<00:00,  3.37pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13600/13600 [46:50<00:00,  3.37pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13600/13600 [46:51<00:00,  3.37pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13600/13600 [46:51<00:00,  3.37pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress: 100%|██████████| 13600/13600 [46:53<00:00,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13600/13700 [46:54<00:29,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13601/13700 [46:54<00:29,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13602/13700 [46:54<00:29,  3.37pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 13603/13700 [46:54<00:28,  3.37pipeline/s]Optimization Progress:  99%|█████████▉| 13604/13700 [46:54<00:47,  2.01pipeline/s]Optimization Progress:  99%|█████████▉| 13607/13700 [46:58<01:09,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 13685/13700 [46:59<00:07,  1.89pipeline/s]
Generation 136 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13700/13700 [46:59<00:00,  1.89pipeline/s]Optimization Progress: 100%|██████████| 13700/13700 [46:59<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13700/13700 [47:00<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 13700/13700 [47:00<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13700/13700 [47:01<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13700/13700 [47:01<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 13700/13700 [47:01<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13700/13700 [47:02<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 13700/13700 [47:02<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13700/13700 [47:02<00:00,  2.65pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13700/13700 [47:03<00:00,  2.65pipeline/s]Optimization Progress:  99%|█████████▉| 13705/13800 [47:03<00:46,  2.05pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13705/13800 [47:03<00:46,  2.05pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13706/13800 [47:03<00:45,  2.05pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13707/13800 [47:03<00:45,  2.05pipeline/s]Optimization Progress:  99%|█████████▉| 13709/13800 [47:13<01:41,  1.11s/pipeline]Optimization Progress: 100%|█████████▉| 13789/13800 [47:15<00:08,  1.28pipeline/s]
Generation 137 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 13800/13800 [47:16<00:00,  1.28pipeline/s]Optimization Progress: 100%|██████████| 13800/13800 [47:16<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 13800/13800 [47:16<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 [07:28:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13800/13800 [47:17<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 13800/13800 [47:18<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13800/13800 [47:18<00:00,  1.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 13800/13800 [47:18<00:00,  1.73pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13801/13900 [47:18<00:57,  1.73pipeline/s]Optimization Progress:  99%|█████████▉| 13802/13900 [47:18<01:08,  1.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13802/13900 [47:18<01:08,  1.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13803/13900 [47:18<01:07,  1.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13804/13900 [47:18<01:06,  1.44pipeline/s]Optimization Progress:  99%|█████████▉| 13806/13900 [47:21<01:10,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 13886/13900 [47:24<00:07,  1.87pipeline/s]
Generation 138 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13900/13900 [47:25<00:00,  1.87pipeline/s]Optimization Progress: 100%|██████████| 13900/13900 [47:25<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13900/13900 [47:25<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13900/13900 [47:25<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:28:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 13900/13900 [47:26<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 13900/13900 [47:27<00:00,  2.56pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:28:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 13900/13900 [47:27<00:00,  2.56pipeline/s]Optimization Progress:  99%|█████████▉| 13902/14000 [47:28<01:12,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13902/14000 [47:28<01:12,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13903/14000 [47:28<01:11,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13904/14000 [47:28<01:11,  1.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 13905/14000 [47:28<01:10,  1.35pipeline/s]Optimization Progress:  99%|█████████▉| 13907/14000 [47:33<01:17,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 13987/14000 [47:47<00:08,  1.57pipeline/s]
Generation 139 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-200077527.4123445	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:28:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14000/14000 [47:47<00:00,  1.57pipeline/s]Optimization Progress: 100%|██████████| 14000/14000 [47:47<00:00,  2.18pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14000/14000 [47:47<00:00,  2.18pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:28:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14000/14000 [47:47<00:00,  2.18pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14000/14000 [47:49<00:00,  2.18pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14000/14000 [47:49<00:00,  2.18pipeline/s]Optimization Progress:  99%|█████████▉| 14007/14100 [47:50<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14007/14100 [47:50<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14008/14100 [47:50<00:40,  2.26pipeline/s]Optimization Progress:  99%|█████████▉| 14010/14100 [47:54<01:04,  1.39pipeline/s]Optimization Progress: 100%|█████████▉| 14090/14100 [47:56<00:05,  1.95pipeline/s]
Generation 140 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 14100/14100 [47:58<00:00,  1.95pipeline/s]Optimization Progress: 100%|██████████| 14100/14100 [47:58<00:00,  2.39pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14100/14100 [47:58<00:00,  2.39pipeline/s]Optimization Progress:  99%|█████████▉| 14104/14200 [48:00<00:38,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14104/14200 [48:00<00:38,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14105/14200 [48:00<00:38,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14106/14200 [48:00<00:37,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14107/14200 [48:00<00:37,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14108/14200 [48:00<00:37,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14109/14200 [48:00<00:36,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14110/14200 [48:00<00:36,  2.47pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14111/14200 [48:00<00:35,  2.47pipeline/s]Optimization Progress:  99%|█████████▉| 14113/14200 [48:13<01:02,  1.40pipeline/s]Optimization Progress: 100%|█████████▉| 14193/14200 [48:14<00:03,  1.99pipeline/s]
Generation 141 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.
Optimization Progress: 100%|██████████| 14200/14200 [48:15<00:00,  1.99pipeline/s]Optimization Progress: 100%|██████████| 14200/14200 [48:15<00:00,  2.57pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:29:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14200/14200 [48:15<00:00,  2.57pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:29:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14200/14200 [48:15<00:00,  2.57pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:29:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14200/14200 [48:17<00:00,  2.57pipeline/s]Optimization Progress:  99%|█████████▉| 14203/14300 [48:17<00:50,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14203/14300 [48:17<00:50,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14204/14300 [48:17<00:50,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14205/14300 [48:17<00:49,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14206/14300 [48:17<00:49,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14207/14300 [48:17<00:48,  1.91pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14208/14300 [48:17<00:48,  1.91pipeline/s]Optimization Progress:  99%|█████████▉| 14210/14300 [48:29<01:17,  1.17pipeline/s]Optimization Progress: 100%|█████████▉| 14290/14300 [48:32<00:06,  1.63pipeline/s]
Generation 142 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198757720.81300828	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=1, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14300/14300 [48:33<00:00,  1.63pipeline/s]Optimization Progress: 100%|██████████| 14300/14300 [48:33<00:00,  2.28pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 14300/14300 [48:33<00:00,  2.28pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 14300/14300 [48:33<00:00,  2.28pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 14300/14300 [48:34<00:00,  2.28pipeline/s]Optimization Progress:  99%|█████████▉| 14307/14400 [48:35<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14307/14400 [48:35<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14308/14400 [48:35<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14309/14400 [48:35<00:38,  2.34pipeline/s]Optimization Progress:  99%|█████████▉| 14311/14400 [48:40<00:57,  1.54pipeline/s]Optimization Progress: 100%|█████████▉| 14391/14400 [48:43<00:04,  2.15pipeline/s]
Generation 143 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:29:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14400/14400 [48:44<00:00,  2.15pipeline/s]Optimization Progress: 100%|██████████| 14400/14400 [48:44<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 14400/14400 [48:46<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14400/14400 [48:46<00:00,  2.80pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14400/14400 [48:46<00:00,  2.80pipeline/s]Optimization Progress:  99%|█████████▉| 14405/14500 [48:46<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14405/14500 [48:46<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14406/14500 [48:46<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14407/14500 [48:46<00:38,  2.44pipeline/s]Optimization Progress:  99%|█████████▉| 14409/14500 [48:51<00:56,  1.61pipeline/s]Optimization Progress: 100%|█████████▉| 14489/14500 [48:52<00:04,  2.27pipeline/s]
Generation 144 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:29:59] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14500/14500 [48:54<00:00,  2.27pipeline/s]Optimization Progress: 100%|██████████| 14500/14500 [48:54<00:00,  2.84pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14500/14500 [48:54<00:00,  2.84pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14500/14500 [48:55<00:00,  2.84pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:30:00] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14500/14500 [48:55<00:00,  2.84pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14500/14500 [48:55<00:00,  2.84pipeline/s]Optimization Progress:  99%|█████████▉| 14505/14600 [48:56<00:33,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14505/14600 [48:56<00:33,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14506/14600 [48:56<00:32,  2.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14507/14600 [48:56<00:32,  2.86pipeline/s]Optimization Progress:  99%|█████████▉| 14509/14600 [48:59<00:48,  1.87pipeline/s]Optimization Progress: 100%|█████████▉| 14589/14600 [49:01<00:04,  2.63pipeline/s]
Generation 145 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:30:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14600/14600 [49:01<00:00,  2.63pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14600/14600 [49:01<00:00,  2.63pipeline/s]Optimization Progress: 100%|██████████| 14600/14600 [49:01<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14600/14600 [49:02<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 14600/14600 [49:02<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14600/14600 [49:03<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:30:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14600/14600 [49:03<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 14600/14600 [49:03<00:00,  3.69pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14600/14600 [49:04<00:00,  3.69pipeline/s]Optimization Progress:  99%|█████████▉| 14606/14700 [49:04<00:32,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14606/14700 [49:04<00:32,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14607/14700 [49:04<00:32,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14608/14700 [49:04<00:31,  2.89pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14609/14700 [49:04<00:31,  2.89pipeline/s]Optimization Progress:  99%|█████████▉| 14611/14700 [49:09<00:49,  1.80pipeline/s]Optimization Progress: 100%|█████████▉| 14691/14700 [49:12<00:03,  2.52pipeline/s]
Generation 146 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 14700/14700 [49:12<00:00,  2.52pipeline/s]Optimization Progress: 100%|██████████| 14700/14700 [49:12<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14700/14700 [49:13<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14700/14700 [49:13<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14700/14700 [49:13<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14700/14700 [49:14<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 14700/14700 [49:15<00:00,  3.48pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14700/14700 [49:15<00:00,  3.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14702/14800 [49:15<00:28,  3.48pipeline/s]Optimization Progress:  99%|█████████▉| 14703/14800 [49:15<00:53,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14703/14800 [49:15<00:53,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14704/14800 [49:15<00:52,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14705/14800 [49:15<00:52,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14706/14800 [49:15<00:51,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14707/14800 [49:16<00:51,  1.82pipeline/s]Optimization Progress:  99%|█████████▉| 14709/14800 [49:29<01:34,  1.04s/pipeline]Optimization Progress: 100%|█████████▉| 14789/14800 [49:34<00:08,  1.34pipeline/s]
Generation 147 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 14800/14800 [49:34<00:00,  1.34pipeline/s]Optimization Progress: 100%|██████████| 14800/14800 [49:34<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 14800/14800 [49:35<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14800/14800 [49:35<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14800/14800 [49:35<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14800/14800 [49:36<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 14800/14800 [49:36<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14800/14800 [49:36<00:00,  1.88pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14800/14800 [49:37<00:00,  1.88pipeline/s]Optimization Progress:  99%|█████████▉| 14809/14900 [49:38<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14809/14900 [49:38<00:44,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14810/14900 [49:38<00:43,  2.06pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 14811/14900 [49:38<00:43,  2.06pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14812/14900 [49:38<00:42,  2.06pipeline/s]Optimization Progress:  99%|█████████▉| 14814/14900 [49:41<00:46,  1.85pipeline/s]Optimization Progress: 100%|█████████▉| 14894/14900 [49:42<00:02,  2.60pipeline/s]
Generation 148 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 14900/14900 [49:44<00:00,  2.60pipeline/s]Optimization Progress: 100%|██████████| 14900/14900 [49:44<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Input contains NaN, infinity or a value too large for dtype('float32')..
Optimization Progress: 100%|██████████| 14900/14900 [49:44<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:30:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 14900/14900 [49:44<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 14900/14900 [49:45<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14900/14900 [49:45<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 14900/14900 [49:45<00:00,  3.02pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 14900/14900 [49:45<00:00,  3.02pipeline/s]Optimization Progress:  99%|█████████▉| 14903/15000 [49:46<00:45,  2.12pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14903/15000 [49:46<00:45,  2.12pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14904/15000 [49:46<00:45,  2.12pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 14905/15000 [49:46<00:44,  2.12pipeline/s]Optimization Progress:  99%|█████████▉| 14907/15000 [49:51<01:02,  1.48pipeline/s]Optimization Progress: 100%|█████████▉| 14987/15000 [49:53<00:06,  2.08pipeline/s]
Generation 149 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15000/15000 [49:54<00:00,  2.08pipeline/s]Optimization Progress: 100%|██████████| 15000/15000 [49:54<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 15000/15000 [49:54<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 15000/15000 [49:54<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15000/15000 [49:54<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 15000/15000 [49:55<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 15000/15000 [49:55<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 15000/15000 [49:56<00:00,  2.86pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:01] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15000/15000 [49:56<00:00,  2.86pipeline/s]Optimization Progress:  99%|█████████▉| 15003/15100 [49:56<00:47,  2.02pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15003/15100 [49:56<00:47,  2.02pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15004/15100 [49:56<00:47,  2.02pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15005/15100 [49:56<00:46,  2.02pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 15006/15100 [49:56<00:46,  2.02pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15007/15100 [49:56<00:45,  2.02pipeline/s]Optimization Progress:  99%|█████████▉| 15009/15100 [50:02<00:58,  1.56pipeline/s]Optimization Progress: 100%|█████████▉| 15089/15100 [50:05<00:05,  2.17pipeline/s]
Generation 150 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 15100/15100 [50:05<00:00,  2.17pipeline/s]Optimization Progress: 100%|██████████| 15100/15100 [50:05<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15100/15100 [50:05<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 15100/15100 [50:06<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 15100/15100 [50:06<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15100/15100 [50:06<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15100/15100 [50:07<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15100/15100 [50:07<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15100/15100 [50:07<00:00,  3.06pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress: 100%|██████████| 15100/15100 [50:07<00:00,  3.06pipeline/s]Optimization Progress:  99%|█████████▉| 15107/15200 [50:08<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15107/15200 [50:08<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15108/15200 [50:08<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15109/15200 [50:08<00:33,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15110/15200 [50:08<00:32,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15111/15200 [50:08<00:32,  2.74pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15112/15200 [50:08<00:32,  2.74pipeline/s]Optimization Progress:  99%|█████████▉| 15114/15200 [50:13<00:38,  2.22pipeline/s]Optimization Progress: 100%|█████████▉| 15194/15200 [50:14<00:01,  3.13pipeline/s]
Generation 151 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15200/15200 [50:14<00:00,  3.13pipeline/s]Optimization Progress: 100%|██████████| 15200/15200 [50:14<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15200/15200 [50:14<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15200/15200 [50:15<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 15200/15200 [50:15<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15200/15200 [50:15<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15200/15200 [50:16<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15200/15200 [50:17<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15200/15200 [50:17<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 15200/15200 [50:17<00:00,  4.13pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15200/15200 [50:17<00:00,  4.13pipeline/s]Optimization Progress:  99%|█████████▉| 15208/15300 [50:17<00:27,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15208/15300 [50:18<00:27,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15209/15300 [50:18<00:26,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15210/15300 [50:18<00:26,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15211/15300 [50:18<00:26,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15212/15300 [50:18<00:26,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15213/15300 [50:18<00:25,  3.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15214/15300 [50:18<00:25,  3.37pipeline/s]Optimization Progress:  99%|█████████▉| 15216/15300 [50:26<00:43,  1.94pipeline/s]Optimization Progress: 100%|█████████▉| 15296/15300 [50:28<00:01,  2.70pipeline/s]
Generation 152 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15300/15300 [50:28<00:00,  2.70pipeline/s]Optimization Progress: 100%|██████████| 15300/15300 [50:28<00:00,  3.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 15300/15300 [50:28<00:00,  3.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 15300/15300 [50:29<00:00,  3.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15300/15300 [50:31<00:00,  3.73pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15300/15300 [50:32<00:00,  3.73pipeline/s]Optimization Progress:  99%|█████████▉| 15307/15400 [50:32<00:32,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15307/15400 [50:32<00:32,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15308/15400 [50:32<00:31,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15309/15400 [50:32<00:31,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15310/15400 [50:32<00:31,  2.88pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15311/15400 [50:32<00:30,  2.88pipeline/s]Optimization Progress:  99%|█████████▉| 15313/15400 [50:35<00:36,  2.41pipeline/s]Optimization Progress: 100%|█████████▉| 15393/15400 [50:37<00:02,  3.36pipeline/s]
Generation 153 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-128889990.03752032	LinearSVR(Normalizer(VarianceThreshold(PolynomialFeatures(Nystroem(XGBRegressor(ZeroCount(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=4, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=8), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), VarianceThreshold__threshold=0.1), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15400/15400 [50:37<00:00,  3.36pipeline/s]Optimization Progress: 100%|██████████| 15400/15400 [50:37<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:31:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:31:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15400/15400 [50:38<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 15400/15400 [50:39<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15400/15400 [50:39<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15400/15400 [50:39<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15400/15400 [50:39<00:00,  4.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15400/15400 [50:40<00:00,  4.67pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15403/15500 [50:40<00:20,  4.67pipeline/s]Optimization Progress:  99%|█████████▉| 15404/15500 [50:40<00:35,  2.68pipeline/s]Optimization Progress:  99%|█████████▉| 15407/15500 [50:46<01:19,  1.16pipeline/s]Optimization Progress: 100%|█████████▉| 15485/15500 [50:50<00:09,  1.63pipeline/s]
Generation 154 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 15500/15500 [50:50<00:00,  1.63pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15500/15500 [50:50<00:00,  1.63pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15500/15500 [50:50<00:00,  1.63pipeline/s]Optimization Progress: 100%|██████████| 15500/15500 [50:50<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15500/15500 [50:50<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.
Optimization Progress: 100%|██████████| 15500/15500 [50:51<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15500/15500 [50:52<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15500/15500 [50:52<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 15500/15500 [50:52<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:31:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15500/15500 [50:53<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:31:58] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15500/15500 [50:53<00:00,  2.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15500/15500 [50:53<00:00,  2.31pipeline/s]Optimization Progress:  99%|█████████▉| 15507/15600 [50:53<00:41,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15507/15600 [50:53<00:41,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15508/15600 [50:53<00:41,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15509/15600 [50:53<00:40,  2.23pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15510/15600 [50:53<00:40,  2.23pipeline/s]Optimization Progress:  99%|█████████▉| 15512/15600 [51:01<01:06,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 15592/15600 [51:04<00:04,  1.86pipeline/s]
Generation 155 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15600/15600 [51:04<00:00,  1.86pipeline/s]Optimization Progress: 100%|██████████| 15600/15600 [51:04<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 15600/15600 [51:04<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15600/15600 [51:05<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 15600/15600 [51:06<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 15600/15600 [51:06<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 15600/15600 [51:07<00:00,  2.61pipeline/s]Optimization Progress:  99%|█████████▉| 15604/15700 [51:08<00:55,  1.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15604/15700 [51:08<00:55,  1.72pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15605/15700 [51:08<00:55,  1.72pipeline/s]Optimization Progress:  99%|█████████▉| 15607/15700 [51:12<01:17,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 15687/15700 [51:16<00:07,  1.68pipeline/s]
Generation 156 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15700/15700 [51:16<00:00,  1.68pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 15700/15700 [51:16<00:00,  1.68pipeline/s]Optimization Progress: 100%|██████████| 15700/15700 [51:16<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15700/15700 [51:16<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 15700/15700 [51:16<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15700/15700 [51:17<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:22] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15700/15700 [51:17<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15700/15700 [51:17<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15700/15700 [51:17<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15700/15700 [51:17<00:00,  2.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 15700/15700 [51:18<00:00,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15702/15800 [51:18<00:41,  2.35pipeline/s]Optimization Progress:  99%|█████████▉| 15703/15800 [51:18<00:51,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15703/15800 [51:18<00:51,  1.87pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15704/15800 [51:18<00:51,  1.87pipeline/s]Optimization Progress:  99%|█████████▉| 15706/15800 [51:25<01:36,  1.02s/pipeline]Optimization Progress: 100%|█████████▉| 15786/15800 [51:28<00:10,  1.37pipeline/s]
Generation 157 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15800/15800 [51:29<00:00,  1.37pipeline/s]Optimization Progress: 100%|██████████| 15800/15800 [51:29<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 15800/15800 [51:29<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.
Optimization Progress: 100%|██████████| 15800/15800 [51:29<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15800/15800 [51:29<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15800/15800 [51:30<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 15800/15800 [51:30<00:00,  1.91pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:32:36] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15800/15800 [51:31<00:00,  1.91pipeline/s]Optimization Progress:  99%|█████████▉| 15803/15900 [51:32<01:08,  1.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15803/15900 [51:32<01:08,  1.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15804/15900 [51:32<01:07,  1.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15805/15900 [51:32<01:06,  1.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15806/15900 [51:32<01:06,  1.42pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15807/15900 [51:32<01:05,  1.42pipeline/s]Optimization Progress:  99%|█████████▉| 15808/15900 [51:50<01:04,  1.42pipeline/s]Optimization Progress:  99%|█████████▉| 15809/15900 [51:52<02:15,  1.49s/pipeline]Optimization Progress: 100%|█████████▉| 15889/15900 [51:55<00:11,  1.05s/pipeline]
Generation 158 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198755776.28822803	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), input_matrix), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15900/15900 [51:56<00:00,  1.05s/pipeline]Optimization Progress: 100%|██████████| 15900/15900 [51:56<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 15900/15900 [51:57<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 15900/15900 [51:58<00:00,  1.32pipeline/s]Optimization Progress:  99%|█████████▉| 15902/16000 [51:58<01:29,  1.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15902/16000 [51:58<01:29,  1.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15903/16000 [51:58<01:28,  1.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15904/16000 [51:58<01:27,  1.09pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 15905/16000 [51:58<01:26,  1.09pipeline/s]Optimization Progress:  99%|█████████▉| 15907/16000 [52:11<02:11,  1.41s/pipeline]Optimization Progress: 100%|█████████▉| 15987/16000 [52:14<00:12,  1.00pipeline/s]
Generation 159 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198191656.71031255	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16000/16000 [52:15<00:00,  1.00pipeline/s]Optimization Progress: 100%|██████████| 16000/16000 [52:15<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16000/16000 [52:15<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16000/16000 [52:15<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:21] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16000/16000 [52:16<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16000/16000 [52:17<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 16000/16000 [52:17<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16000/16000 [52:18<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:33:23] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16000/16000 [52:18<00:00,  1.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16000/16000 [52:18<00:00,  1.40pipeline/s]Optimization Progress:  99%|█████████▉| 16004/16100 [52:18<01:14,  1.29pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16004/16100 [52:18<01:14,  1.29pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16005/16100 [52:18<01:13,  1.29pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16006/16100 [52:18<01:12,  1.29pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16007/16100 [52:18<01:12,  1.29pipeline/s]Optimization Progress:  99%|█████████▉| 16009/16100 [52:23<01:15,  1.21pipeline/s]Optimization Progress: 100%|█████████▉| 16089/16100 [52:26<00:06,  1.69pipeline/s]
Generation 160 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199974908.6223349	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198191656.71031255	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16100/16100 [52:27<00:00,  1.69pipeline/s]Optimization Progress: 100%|██████████| 16100/16100 [52:27<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16100/16100 [52:27<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 16100/16100 [52:27<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16100/16100 [52:28<00:00,  2.32pipeline/s]Optimization Progress:  99%|█████████▉| 16103/16200 [52:30<00:58,  1.65pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16103/16200 [52:30<00:58,  1.65pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16104/16200 [52:30<00:58,  1.65pipeline/s]Optimization Progress:  99%|█████████▉| 16106/16200 [52:35<01:31,  1.03pipeline/s]Optimization Progress: 100%|█████████▉| 16186/16200 [52:38<00:09,  1.44pipeline/s]
Generation 161 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199816201.18656304	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198191656.71031255	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16200/16200 [52:40<00:00,  1.44pipeline/s]Optimization Progress: 100%|██████████| 16200/16200 [52:40<00:00,  1.96pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16200/16200 [52:40<00:00,  1.96pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16200/16200 [52:40<00:00,  1.96pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16200/16200 [52:40<00:00,  1.96pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16200/16200 [52:41<00:00,  1.96pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 16200/16200 [52:41<00:00,  1.96pipeline/s]Optimization Progress:  99%|█████████▉| 16202/16300 [52:42<01:03,  1.55pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16202/16300 [52:42<01:03,  1.55pipeline/s]Optimization Progress:  99%|█████████▉| 16204/16300 [52:48<02:19,  1.45s/pipeline]Optimization Progress: 100%|█████████▉| 16284/16300 [52:51<00:16,  1.03s/pipeline]
Generation 162 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199816201.18656304	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198191656.71031255	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16300/16300 [52:52<00:00,  1.03s/pipeline]Optimization Progress: 100%|██████████| 16300/16300 [52:52<00:00,  1.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16300/16300 [52:53<00:00,  1.35pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16300/16300 [52:53<00:00,  1.35pipeline/s]Optimization Progress:  99%|█████████▉| 16304/16400 [52:53<01:00,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16304/16400 [52:53<01:00,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16305/16400 [52:53<00:59,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16306/16400 [52:53<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16307/16400 [52:53<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16308/16400 [52:53<00:57,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16309/16400 [52:53<00:57,  1.60pipeline/s]Optimization Progress:  99%|█████████▉| 16311/16400 [53:02<01:12,  1.23pipeline/s]Optimization Progress: 100%|█████████▉| 16391/16400 [53:04<00:05,  1.74pipeline/s]
Generation 163 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198191656.71031255	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-146972895.9992294	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:34:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16400/16400 [53:04<00:00,  1.74pipeline/s]Optimization Progress: 100%|██████████| 16400/16400 [53:04<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 16400/16400 [53:04<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.
Optimization Progress: 100%|██████████| 16400/16400 [53:04<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16400/16400 [53:06<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16400/16400 [53:06<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16400/16400 [53:06<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16400/16400 [53:06<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:34:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16400/16400 [53:07<00:00,  2.40pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:34:12] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16400/16400 [53:07<00:00,  2.40pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16401/16500 [53:07<00:41,  2.40pipeline/s]Optimization Progress:  99%|█████████▉| 16402/16500 [53:07<01:07,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16402/16500 [53:07<01:07,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16403/16500 [53:07<01:06,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16404/16500 [53:07<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16405/16500 [53:07<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16406/16500 [53:07<01:04,  1.46pipeline/s]Optimization Progress:  99%|█████████▉| 16408/16500 [53:10<01:01,  1.50pipeline/s]Optimization Progress: 100%|█████████▉| 16488/16500 [53:16<00:05,  2.05pipeline/s]
Generation 164 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-146972895.9992294	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 16500/16500 [53:16<00:00,  2.05pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16500/16500 [53:16<00:00,  2.05pipeline/s]Optimization Progress: 100%|██████████| 16500/16500 [53:16<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16500/16500 [53:17<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16500/16500 [53:17<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16500/16500 [53:17<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 16500/16500 [53:18<00:00,  2.90pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 16500/16500 [53:20<00:00,  2.90pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16503/16600 [53:20<00:33,  2.90pipeline/s]Optimization Progress:  99%|█████████▉| 16504/16600 [53:20<00:48,  1.97pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16504/16600 [53:20<00:48,  1.97pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16505/16600 [53:20<00:48,  1.97pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16506/16600 [53:20<00:47,  1.97pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16507/16600 [53:20<00:47,  1.97pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16508/16600 [53:20<00:46,  1.97pipeline/s]Optimization Progress:  99%|█████████▉| 16510/16600 [53:26<00:57,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 16590/16600 [53:27<00:04,  2.21pipeline/s]
Generation 165 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-147347501.51894352	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-6	-143337752.11719382	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-131301831.25129354	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:34:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16600/16600 [53:27<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 16600/16600 [53:27<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16600/16600 [53:27<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 16600/16600 [53:28<00:00,  2.21pipeline/s]Optimization Progress: 100%|██████████| 16600/16600 [53:28<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 16600/16600 [53:29<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 16600/16600 [53:29<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 16600/16600 [53:30<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16600/16600 [53:30<00:00,  3.10pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16600/16600 [53:30<00:00,  3.10pipeline/s]Optimization Progress:  99%|█████████▉| 16605/16700 [53:31<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16605/16700 [53:31<00:40,  2.35pipeline/s]Optimization Progress:  99%|█████████▉| 16608/16700 [53:34<00:55,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 16687/16700 [53:38<00:05,  2.31pipeline/s]
Generation 166 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.
Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  2.31pipeline/s]Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16700/16700 [53:38<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16700/16700 [53:39<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:34:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16700/16700 [53:39<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16700/16700 [53:40<00:00,  3.20pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16700/16700 [53:40<00:00,  3.20pipeline/s]Optimization Progress:  99%|█████████▉| 16705/16800 [53:41<00:36,  2.62pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16705/16800 [53:41<00:36,  2.62pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16706/16800 [53:41<00:35,  2.62pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16707/16800 [53:41<00:35,  2.62pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16708/16800 [53:41<00:35,  2.62pipeline/s]Optimization Progress:  99%|█████████▉| 16710/16800 [53:46<00:51,  1.75pipeline/s]Optimization Progress: 100%|█████████▉| 16790/16800 [53:49<00:04,  2.44pipeline/s]
Generation 167 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16800/16800 [53:49<00:00,  2.44pipeline/s]Optimization Progress: 100%|██████████| 16800/16800 [53:49<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 16800/16800 [53:50<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 16800/16800 [53:50<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 16800/16800 [53:52<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16800/16800 [53:52<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16800/16800 [53:52<00:00,  3.27pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 16800/16800 [53:52<00:00,  3.27pipeline/s]Optimization Progress:  99%|█████████▉| 16804/16900 [53:52<00:42,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16804/16900 [53:52<00:42,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16805/16900 [53:52<00:42,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16806/16900 [53:52<00:41,  2.26pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16807/16900 [53:52<00:41,  2.26pipeline/s]Optimization Progress:  99%|█████████▉| 16809/16900 [54:08<01:54,  1.26s/pipeline]Optimization Progress: 100%|█████████▉| 16889/16900 [54:09<00:09,  1.13pipeline/s]
Generation 168 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16900/16900 [54:09<00:00,  1.13pipeline/s]Optimization Progress: 100%|██████████| 16900/16900 [54:09<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16900/16900 [54:10<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16900/16900 [54:10<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16900/16900 [54:10<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:35:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16900/16900 [54:10<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 16900/16900 [54:11<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 16900/16900 [54:12<00:00,  1.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 16900/16900 [54:12<00:00,  1.61pipeline/s]Optimization Progress:  99%|█████████▉| 16905/17000 [54:13<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16905/17000 [54:13<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16906/17000 [54:13<01:04,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16907/17000 [54:13<01:03,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16908/17000 [54:13<01:03,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 16909/17000 [54:13<01:02,  1.46pipeline/s]Optimization Progress:  99%|█████████▉| 16911/17000 [54:18<01:03,  1.41pipeline/s]Optimization Progress: 100%|█████████▉| 16991/17000 [54:20<00:04,  1.97pipeline/s]
Generation 169 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17000/17000 [54:20<00:00,  1.97pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17000/17000 [54:21<00:00,  1.97pipeline/s]Optimization Progress: 100%|██████████| 17000/17000 [54:21<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17000/17000 [54:21<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17000/17000 [54:21<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17000/17000 [54:21<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17000/17000 [54:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17000/17000 [54:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17000/17000 [54:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17000/17000 [54:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 17000/17000 [54:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 17000/17000 [54:23<00:00,  2.74pipeline/s]Optimization Progress:  99%|█████████▉| 17003/17100 [54:23<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17003/17100 [54:23<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17004/17100 [54:23<00:49,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17005/17100 [54:23<00:48,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17006/17100 [54:23<00:48,  1.95pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17007/17100 [54:23<00:47,  1.95pipeline/s]Optimization Progress:  99%|█████████▉| 17009/17100 [54:29<00:58,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 17089/17100 [54:32<00:05,  2.19pipeline/s]
Generation 170 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17100/17100 [54:32<00:00,  2.19pipeline/s]Optimization Progress: 100%|██████████| 17100/17100 [54:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 17100/17100 [54:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17100/17100 [54:32<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 17100/17100 [54:33<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 17100/17100 [54:33<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17100/17100 [54:33<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17100/17100 [54:34<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17100/17100 [54:34<00:00,  3.07pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17100/17100 [54:34<00:00,  3.07pipeline/s]Optimization Progress:  99%|█████████▉| 17105/17200 [54:35<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17105/17200 [54:35<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17106/17200 [54:35<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17107/17200 [54:35<00:38,  2.44pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17108/17200 [54:35<00:37,  2.44pipeline/s]Optimization Progress:  99%|█████████▉| 17110/17200 [54:39<00:47,  1.88pipeline/s]Optimization Progress: 100%|█████████▉| 17190/17200 [54:42<00:03,  2.61pipeline/s]
Generation 171 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17200/17200 [54:42<00:00,  2.61pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 17200/17200 [54:42<00:00,  2.61pipeline/s]Optimization Progress: 100%|██████████| 17200/17200 [54:42<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17200/17200 [54:43<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:48] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17200/17200 [54:43<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17200/17200 [54:43<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 17200/17200 [54:44<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.
Optimization Progress: 100%|██████████| 17200/17200 [54:44<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17200/17200 [54:44<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:49] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17200/17200 [54:44<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17200/17200 [54:44<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 17200/17200 [54:45<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17200/17200 [54:46<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:35:51] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17200/17200 [54:46<00:00,  3.67pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17200/17200 [54:46<00:00,  3.67pipeline/s]Optimization Progress:  99%|█████████▉| 17205/17300 [54:46<00:41,  2.28pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17205/17300 [54:46<00:41,  2.28pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17206/17300 [54:46<00:41,  2.28pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17207/17300 [54:46<00:40,  2.28pipeline/s]Optimization Progress:  99%|█████████▉| 17209/17300 [54:54<01:19,  1.15pipeline/s]Optimization Progress: 100%|█████████▉| 17289/17300 [54:56<00:06,  1.62pipeline/s]
Generation 172 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [54:56<00:00,  1.62pipeline/s]Optimization Progress: 100%|██████████| 17300/17300 [54:56<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17300/17300 [54:56<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:02] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [54:57<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [54:57<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:03] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [54:57<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17300/17300 [54:58<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17300/17300 [54:58<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17300/17300 [54:59<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:04] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [54:59<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17300/17300 [54:59<00:00,  2.21pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:05] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17300/17300 [55:00<00:00,  2.21pipeline/s]Optimization Progress:  99%|█████████▉| 17304/17400 [55:00<00:54,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17304/17400 [55:00<00:54,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17305/17400 [55:00<00:53,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17306/17400 [55:00<00:53,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17307/17400 [55:00<00:52,  1.76pipeline/s]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  99%|█████████▉| 17308/17400 [55:00<00:52,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17309/17400 [55:00<00:51,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17310/17400 [55:00<00:51,  1.76pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17311/17400 [55:00<00:50,  1.76pipeline/s]Optimization Progress: 100%|█████████▉| 17313/17400 [55:09<01:02,  1.39pipeline/s]Optimization Progress: 100%|█████████▉| 17393/17400 [55:10<00:03,  1.97pipeline/s]
Generation 173 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17400/17400 [55:11<00:00,  1.97pipeline/s]Optimization Progress: 100%|██████████| 17400/17400 [55:11<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17400/17400 [55:12<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17400/17400 [55:12<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17400/17400 [55:12<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17400/17400 [55:13<00:00,  2.60pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17400/17400 [55:13<00:00,  2.60pipeline/s]Optimization Progress:  99%|█████████▉| 17407/17500 [55:13<00:33,  2.75pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17407/17500 [55:13<00:33,  2.75pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17408/17500 [55:13<00:33,  2.75pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17409/17500 [55:13<00:33,  2.75pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17410/17500 [55:13<00:32,  2.75pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17411/17500 [55:13<00:32,  2.75pipeline/s]Optimization Progress: 100%|█████████▉| 17413/17500 [55:23<01:04,  1.34pipeline/s]Optimization Progress: 100%|█████████▉| 17493/17500 [55:24<00:03,  1.91pipeline/s]
Generation 174 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17500/17500 [55:24<00:00,  1.91pipeline/s]Optimization Progress: 100%|██████████| 17500/17500 [55:24<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17500/17500 [55:24<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 17500/17500 [55:25<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 17500/17500 [55:25<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17500/17500 [55:25<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 17500/17500 [55:27<00:00,  2.66pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17500/17500 [55:27<00:00,  2.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17501/17600 [55:27<00:37,  2.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17502/17600 [55:27<00:36,  2.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17503/17600 [55:27<00:36,  2.66pipeline/s]Optimization Progress:  99%|█████████▉| 17504/17600 [55:27<00:48,  1.97pipeline/s]Optimization Progress:  99%|█████████▉| 17507/17600 [55:32<01:16,  1.22pipeline/s]Optimization Progress: 100%|█████████▉| 17585/17600 [55:34<00:08,  1.71pipeline/s]
Generation 175 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17600/17600 [55:35<00:00,  1.71pipeline/s]Optimization Progress: 100%|██████████| 17600/17600 [55:35<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:41] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17600/17600 [55:36<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 17600/17600 [55:37<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17600/17600 [55:37<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 17600/17600 [55:37<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17600/17600 [55:37<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 x and y arrays must have at least 2 entries.
Optimization Progress: 100%|██████████| 17600/17600 [55:37<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17600/17600 [55:38<00:00,  2.43pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress: 100%|██████████| 17600/17600 [55:38<00:00,  2.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17602/17700 [55:39<00:40,  2.43pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17603/17700 [55:39<00:39,  2.43pipeline/s]Optimization Progress:  99%|█████████▉| 17604/17700 [55:39<00:57,  1.68pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17604/17700 [55:39<00:57,  1.68pipeline/s]Optimization Progress:  99%|█████████▉| 17607/17700 [55:42<01:11,  1.30pipeline/s]Optimization Progress: 100%|█████████▉| 17686/17700 [55:45<00:07,  1.82pipeline/s]
Generation 176 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  1.82pipeline/s]Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17700/17700 [55:46<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress: 100%|██████████| 17700/17700 [55:47<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17700/17700 [55:48<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:53] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17700/17700 [55:48<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17700/17700 [55:48<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17700/17700 [55:49<00:00,  2.52pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:36:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17700/17700 [55:49<00:00,  2.52pipeline/s]Optimization Progress:  99%|█████████▉| 17709/17800 [55:49<00:35,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17709/17800 [55:49<00:35,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17710/17800 [55:49<00:34,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17711/17800 [55:49<00:34,  2.59pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17712/17800 [55:49<00:34,  2.59pipeline/s]Optimization Progress: 100%|█████████▉| 17714/17800 [55:57<01:06,  1.29pipeline/s]Optimization Progress: 100%|█████████▉| 17794/17800 [55:59<00:03,  1.82pipeline/s]
Generation 177 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 17800/17800 [55:59<00:00,  1.82pipeline/s]Optimization Progress: 100%|██████████| 17800/17800 [55:59<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17800/17800 [56:00<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17800/17800 [56:00<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress: 100%|██████████| 17800/17800 [56:01<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17800/17800 [56:01<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17800/17800 [56:01<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17800/17800 [56:01<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 17800/17800 [56:01<00:00,  2.34pipeline/s]Optimization Progress:  99%|█████████▉| 17806/17900 [56:02<00:40,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17806/17900 [56:02<00:40,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17807/17900 [56:02<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17808/17900 [56:02<00:39,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17809/17900 [56:02<00:38,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17810/17900 [56:02<00:38,  2.34pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 17811/17900 [56:02<00:37,  2.34pipeline/s]Optimization Progress: 100%|█████████▉| 17813/17900 [56:06<00:40,  2.14pipeline/s]Optimization Progress: 100%|█████████▉| 17893/17900 [56:10<00:02,  2.92pipeline/s]
Generation 178 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17900/17900 [56:10<00:00,  2.92pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 17900/17900 [56:10<00:00,  2.92pipeline/s]Optimization Progress: 100%|██████████| 17900/17900 [56:10<00:00,  4.00pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 17900/17900 [56:11<00:00,  4.00pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 17900/17900 [56:12<00:00,  4.00pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 17900/17900 [56:12<00:00,  4.00pipeline/s]Optimization Progress:  99%|█████████▉| 17902/18000 [56:14<01:07,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17902/18000 [56:14<01:07,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17903/18000 [56:14<01:06,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17904/18000 [56:14<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17905/18000 [56:14<01:05,  1.46pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 17906/18000 [56:14<01:04,  1.46pipeline/s]Optimization Progress:  99%|█████████▉| 17908/18000 [56:22<01:21,  1.13pipeline/s]Optimization Progress: 100%|█████████▉| 17988/18000 [56:23<00:07,  1.60pipeline/s]
Generation 179 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress: 100%|██████████| 18000/18000 [56:24<00:00,  1.60pipeline/s]Optimization Progress: 100%|██████████| 18000/18000 [56:24<00:00,  2.14pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18000/18000 [56:25<00:00,  2.14pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 18000/18000 [56:26<00:00,  2.14pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 18000/18000 [56:26<00:00,  2.14pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18000/18000 [56:26<00:00,  2.14pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18000/18000 [56:27<00:00,  2.14pipeline/s]Optimization Progress:  99%|█████████▉| 18002/18100 [56:27<01:11,  1.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18002/18100 [56:27<01:11,  1.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18003/18100 [56:27<01:10,  1.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18004/18100 [56:27<01:10,  1.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18005/18100 [56:27<01:09,  1.37pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18006/18100 [56:27<01:08,  1.37pipeline/s]Optimization Progress:  99%|█████████▉| 18008/18100 [56:33<01:15,  1.22pipeline/s]Optimization Progress: 100%|█████████▉| 18088/18100 [56:36<00:07,  1.71pipeline/s]
Generation 180 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 18100/18100 [56:36<00:00,  1.71pipeline/s]Optimization Progress: 100%|██████████| 18100/18100 [56:36<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18100/18100 [56:37<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18100/18100 [56:37<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18100/18100 [56:37<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18100/18100 [56:38<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:37:43] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18100/18100 [56:38<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18100/18100 [56:38<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:44] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18100/18100 [56:39<00:00,  2.36pipeline/s]Optimization Progress:  99%|█████████▉| 18105/18200 [56:39<00:45,  2.07pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18105/18200 [56:39<00:45,  2.07pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18106/18200 [56:39<00:45,  2.07pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18107/18200 [56:39<00:44,  2.07pipeline/s]Optimization Progress: 100%|█████████▉| 18109/18200 [56:45<01:12,  1.26pipeline/s]Optimization Progress: 100%|█████████▉| 18189/18200 [56:47<00:06,  1.78pipeline/s]
Generation 181 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18200/18200 [56:47<00:00,  1.78pipeline/s]Optimization Progress: 100%|██████████| 18200/18200 [56:47<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:37:52] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18200/18200 [56:47<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18200/18200 [56:48<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18200/18200 [56:48<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18200/18200 [56:48<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:37:54] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18200/18200 [56:49<00:00,  2.51pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 18200/18200 [56:49<00:00,  2.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18203/18300 [56:51<00:38,  2.51pipeline/s]Optimization Progress:  99%|█████████▉| 18204/18300 [56:51<00:52,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18204/18300 [56:51<00:52,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18205/18300 [56:51<00:52,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18206/18300 [56:51<00:51,  1.82pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18207/18300 [56:51<00:51,  1.82pipeline/s]Optimization Progress: 100%|█████████▉| 18209/18300 [56:57<01:08,  1.33pipeline/s]Optimization Progress: 100%|█████████▉| 18289/18300 [57:01<00:05,  1.84pipeline/s]
Generation 182 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18300/18300 [57:03<00:00,  1.84pipeline/s]Optimization Progress: 100%|██████████| 18300/18300 [57:03<00:00,  2.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18300/18300 [57:03<00:00,  2.38pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 18300/18300 [57:05<00:00,  2.38pipeline/s]Optimization Progress:  99%|█████████▉| 18302/18400 [57:05<01:01,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18302/18400 [57:05<01:01,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18303/18400 [57:05<01:00,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18304/18400 [57:05<00:59,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18305/18400 [57:05<00:59,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18306/18400 [57:05<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18307/18400 [57:05<00:58,  1.60pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18308/18400 [57:05<00:57,  1.60pipeline/s]Optimization Progress: 100%|█████████▉| 18310/18400 [57:09<00:52,  1.71pipeline/s]Optimization Progress: 100%|█████████▉| 18390/18400 [57:11<00:04,  2.39pipeline/s]
Generation 183 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 18400/18400 [57:11<00:00,  2.39pipeline/s]Optimization Progress: 100%|██████████| 18400/18400 [57:11<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 18400/18400 [57:12<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 18400/18400 [57:12<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 18400/18400 [57:12<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18400/18400 [57:12<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18400/18400 [57:13<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18400/18400 [57:13<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.
Optimization Progress: 100%|██████████| 18400/18400 [57:13<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:20] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18400/18400 [57:14<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress: 100%|██████████| 18400/18400 [57:15<00:00,  3.33pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18400/18400 [57:15<00:00,  3.33pipeline/s]Optimization Progress:  99%|█████████▉| 18405/18500 [57:15<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18405/18500 [57:15<00:40,  2.35pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18406/18500 [57:15<00:40,  2.35pipeline/s]Optimization Progress: 100%|█████████▉| 18408/18500 [57:25<01:56,  1.27s/pipeline]Optimization Progress: 100%|█████████▉| 18488/18500 [57:28<00:10,  1.11pipeline/s]
Generation 184 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:33] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18500/18500 [57:28<00:00,  1.11pipeline/s]Optimization Progress: 100%|██████████| 18500/18500 [57:28<00:00,  1.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18500/18500 [57:30<00:00,  1.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18500/18500 [57:30<00:00,  1.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:35] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18500/18500 [57:30<00:00,  1.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18500/18500 [57:31<00:00,  1.58pipeline/s]Optimization Progress:  99%|█████████▉| 18505/18600 [57:32<01:05,  1.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18505/18600 [57:32<01:05,  1.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18506/18600 [57:32<01:05,  1.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18507/18600 [57:32<01:04,  1.45pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18508/18600 [57:32<01:03,  1.45pipeline/s]Optimization Progress: 100%|█████████▉| 18510/18600 [57:41<01:30,  1.01s/pipeline]Optimization Progress: 100%|█████████▉| 18590/18600 [57:43<00:07,  1.40pipeline/s]
Generation 185 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18600/18600 [57:43<00:00,  1.40pipeline/s]Optimization Progress: 100%|██████████| 18600/18600 [57:43<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 18600/18600 [57:44<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18600/18600 [57:45<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:50] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18600/18600 [57:45<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18600/18600 [57:45<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 18600/18600 [57:46<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18600/18600 [57:47<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18600/18600 [57:47<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18600/18600 [57:47<00:00,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18600/18700 [57:47<00:50,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18601/18700 [57:47<00:49,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18602/18700 [57:47<00:49,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18603/18700 [57:47<00:48,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18604/18700 [57:47<00:48,  1.99pipeline/s]Optimization Progress:  99%|█████████▉| 18605/18700 [57:47<00:57,  1.66pipeline/s]Optimization Progress: 100%|█████████▉| 18609/18700 [57:56<01:39,  1.10s/pipeline]Optimization Progress: 100%|█████████▉| 18686/18700 [58:00<00:10,  1.28pipeline/s]
Generation 186 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198188812.74887055	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18700/18700 [58:01<00:00,  1.28pipeline/s]Optimization Progress: 100%|██████████| 18700/18700 [58:01<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 18700/18700 [58:01<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18700/18700 [58:02<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 18700/18700 [58:02<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18700/18700 [58:03<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:09] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18700/18700 [58:03<00:00,  1.78pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 18700/18700 [58:04<00:00,  1.78pipeline/s]Optimization Progress: 100%|█████████▉| 18707/18800 [58:04<00:50,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18707/18800 [58:04<00:50,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18708/18800 [58:04<00:49,  1.86pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18709/18800 [58:05<00:48,  1.86pipeline/s]Optimization Progress: 100%|█████████▉| 18711/18800 [58:09<01:03,  1.41pipeline/s]Optimization Progress: 100%|█████████▉| 18791/18800 [58:11<00:04,  1.99pipeline/s]
Generation 187 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18800/18800 [58:11<00:00,  1.99pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 18800/18800 [58:12<00:00,  1.99pipeline/s]Optimization Progress: 100%|██████████| 18800/18800 [58:12<00:00,  2.54pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:39:17] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18800/18800 [58:12<00:00,  2.54pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.
Optimization Progress: 100%|██████████| 18800/18800 [58:12<00:00,  2.54pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress: 100%|██████████| 18800/18800 [58:15<00:00,  2.54pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 18800/18800 [58:15<00:00,  2.54pipeline/s]Optimization Progress:  99%|█████████▉| 18803/18900 [58:15<00:59,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18803/18900 [58:15<00:59,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18804/18900 [58:15<00:58,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18805/18900 [58:15<00:58,  1.63pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18806/18900 [58:15<00:57,  1.63pipeline/s]Optimization Progress: 100%|█████████▉| 18808/18900 [58:20<01:06,  1.38pipeline/s]Optimization Progress: 100%|█████████▉| 18888/18900 [58:22<00:06,  1.94pipeline/s]
Generation 188 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18900/18900 [58:22<00:00,  1.94pipeline/s]Optimization Progress: 100%|██████████| 18900/18900 [58:22<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18900/18900 [58:23<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress: 100%|██████████| 18900/18900 [58:24<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18900/18900 [58:24<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 18900/18900 [58:24<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18900/18900 [58:25<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 18900/18900 [58:25<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:30] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18900/18900 [58:25<00:00,  2.74pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 18900/18900 [58:25<00:00,  2.74pipeline/s]Optimization Progress:  99%|█████████▉| 18904/19000 [58:26<00:48,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 18904/19000 [58:26<00:48,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18905/19000 [58:26<00:47,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18906/19000 [58:26<00:47,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18907/19000 [58:26<00:46,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18908/19000 [58:26<00:46,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18909/19000 [58:26<00:45,  1.99pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 18910/19000 [58:26<00:45,  1.99pipeline/s]Optimization Progress: 100%|█████████▉| 18912/19000 [58:35<01:02,  1.41pipeline/s]Optimization Progress: 100%|█████████▉| 18992/19000 [58:36<00:03,  2.00pipeline/s]
Generation 189 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19000/19000 [58:37<00:00,  2.00pipeline/s]Optimization Progress: 100%|██████████| 19000/19000 [58:37<00:00,  2.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19000/19000 [58:38<00:00,  2.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 19000/19000 [58:40<00:00,  2.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19000/19000 [58:40<00:00,  2.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19000/19000 [58:41<00:00,  2.58pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19000/19000 [58:41<00:00,  2.58pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19002/19100 [58:41<00:38,  2.58pipeline/s]Optimization Progress:  99%|█████████▉| 19003/19100 [58:41<01:04,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19003/19100 [58:41<01:04,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19004/19100 [58:41<01:03,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19005/19100 [58:41<01:02,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19006/19100 [58:41<01:02,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19007/19100 [58:41<01:01,  1.51pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19008/19100 [58:41<01:00,  1.51pipeline/s]Optimization Progress: 100%|█████████▉| 19010/19100 [58:46<01:01,  1.45pipeline/s]Optimization Progress: 100%|█████████▉| 19090/19100 [58:50<00:04,  2.02pipeline/s]
Generation 190 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:39:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19100/19100 [58:51<00:00,  2.02pipeline/s]Optimization Progress: 100%|██████████| 19100/19100 [58:51<00:00,  2.53pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:39:57] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19100/19100 [58:51<00:00,  2.53pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19100/19100 [58:52<00:00,  2.53pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19100/19100 [58:52<00:00,  2.53pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19100/19100 [58:52<00:00,  2.53pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.
Optimization Progress: 100%|██████████| 19100/19100 [58:53<00:00,  2.53pipeline/s]Optimization Progress: 100%|█████████▉| 19105/19200 [58:53<00:38,  2.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19105/19200 [58:53<00:38,  2.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19106/19200 [58:54<00:37,  2.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19107/19200 [58:54<00:37,  2.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19108/19200 [58:54<00:37,  2.48pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19109/19200 [58:54<00:36,  2.48pipeline/s]Optimization Progress: 100%|█████████▉| 19111/19200 [58:59<00:49,  1.79pipeline/s]Optimization Progress: 100%|█████████▉| 19191/19200 [59:00<00:03,  2.52pipeline/s]
Generation 191 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  2.52pipeline/s]Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 [07:40:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=2 [07:40:06] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19200/19200 [59:01<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:02<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19200/19200 [59:02<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:07] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:02<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress: 100%|██████████| 19200/19200 [59:02<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 19200/19200 [59:02<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19200/19200 [59:03<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 19200/19200 [59:03<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:08] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19200/19200 [59:03<00:00,  3.49pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19200/19200 [59:04<00:00,  3.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19201/19300 [59:04<00:28,  3.49pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19202/19300 [59:04<00:28,  3.49pipeline/s]Optimization Progress:  99%|█████████▉| 19203/19300 [59:04<00:52,  1.84pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19203/19300 [59:04<00:52,  1.84pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19204/19300 [59:04<00:52,  1.84pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19205/19300 [59:04<00:51,  1.84pipeline/s]Optimization Progress: 100%|█████████▉| 19207/19300 [59:07<00:55,  1.67pipeline/s]Optimization Progress: 100%|█████████▉| 19287/19300 [59:09<00:05,  2.34pipeline/s]
Generation 192 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.
Optimization Progress: 100%|██████████| 19300/19300 [59:09<00:00,  2.34pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.
Optimization Progress: 100%|██████████| 19300/19300 [59:09<00:00,  2.34pipeline/s]Optimization Progress: 100%|██████████| 19300/19300 [59:09<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:15] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 19300/19300 [59:10<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19300/19300 [59:11<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 19300/19300 [59:11<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19300/19300 [59:13<00:00,  3.31pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:18] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19300/19300 [59:13<00:00,  3.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  99%|█████████▉| 19302/19400 [59:14<00:29,  3.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19303/19400 [59:14<00:29,  3.31pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19304/19400 [59:14<00:29,  3.31pipeline/s]Optimization Progress: 100%|█████████▉| 19305/19400 [59:14<00:44,  2.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19305/19400 [59:14<00:44,  2.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19306/19400 [59:14<00:44,  2.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19307/19400 [59:14<00:43,  2.14pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19308/19400 [59:14<00:43,  2.14pipeline/s]Optimization Progress: 100%|█████████▉| 19310/19400 [59:22<01:14,  1.20pipeline/s]Optimization Progress: 100%|█████████▉| 19390/19400 [59:25<00:05,  1.68pipeline/s]
Generation 193 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 19400/19400 [59:25<00:00,  1.68pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress: 100%|██████████| 19400/19400 [59:26<00:00,  1.68pipeline/s]Optimization Progress: 100%|██████████| 19400/19400 [59:26<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19400/19400 [59:26<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress: 100%|██████████| 19400/19400 [59:26<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:32] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19400/19400 [59:26<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 19400/19400 [59:27<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 19400/19400 [59:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 19400/19400 [59:28<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 19400/19400 [59:29<00:00,  2.32pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:40:34] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f328d3e0dc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f328d4f1669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f328d4fef8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f328d4e5cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f328d3d2f35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f289b18d9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f289b18d067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f289b1a527e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f289b1a5cb4]

.
Optimization Progress: 100%|██████████| 19400/19400 [59:29<00:00,  2.32pipeline/s]Optimization Progress: 100%|█████████▉| 19403/19500 [59:29<01:01,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19403/19500 [59:29<01:01,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19404/19500 [59:29<01:01,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19405/19500 [59:29<01:00,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19406/19500 [59:29<00:59,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19407/19500 [59:29<00:59,  1.57pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19408/19500 [59:29<00:58,  1.57pipeline/s]Optimization Progress: 100%|█████████▉| 19410/19500 [59:38<01:14,  1.21pipeline/s]Optimization Progress: 100%|█████████▉| 19490/19500 [59:42<00:05,  1.68pipeline/s]
Generation 194 - Current Pareto front scores:
-1	-353357964.06561625	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.01, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)
-2	-205478371.126787	LinearSVR(Normalizer(input_matrix, Normalizer__norm=l1), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-3	-199728728.8641791	LinearSVR(Normalizer(XGBRegressor(CombineDFs(input_matrix, CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(input_matrix, input_matrix), CombineDFs(input_matrix, input_matrix)), CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)))), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.0001)
-4	-198187729.66089755	LinearSVR(Normalizer(Nystroem(CombineDFs(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=11, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.05)), CombineDFs(input_matrix, input_matrix)), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Normalizer__norm=l2), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05)
-5	-139825237.19104025	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(XGBRegressor(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(CombineDFs(CombineDFs(input_matrix, CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix)), CombineDFs(input_matrix, input_matrix)), input_matrix), input_matrix)), input_matrix), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.01)
-7	-133804706.5983825	LinearSVR(Normalizer(PolynomialFeatures(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(CombineDFs(input_matrix, input_matrix), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-8	-126745665.24412945	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)
-9	-117091164.28629617	LinearSVR(Normalizer(PolynomialFeatures(StandardScaler(Nystroem(Nystroem(XGBRegressor(VarianceThreshold(GradientBoostingRegressor(CombineDFs(input_matrix, input_matrix), GradientBoostingRegressor__alpha=0.85, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=6, GradientBoostingRegressor__max_features=0.7500000000000001, GradientBoostingRegressor__min_samples_leaf=15, GradientBoostingRegressor__min_samples_split=2, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.3), VarianceThreshold__threshold=0.01), XGBRegressor__learning_rate=0.001, XGBRegressor__max_depth=2, XGBRegressor__min_child_weight=20, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9000000000000001), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6), Nystroem__gamma=0.6000000000000001, Nystroem__kernel=polynomial, Nystroem__n_components=6)), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), Normalizer__norm=max), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1)                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19500/19500 [59:42<00:00,  1.68pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19500/19500 [59:42<00:00,  1.68pipeline/s]Optimization Progress: 100%|██████████| 19500/19500 [59:42<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19500/19500 [59:43<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 19500/19500 [59:43<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19500/19500 [59:43<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 19500/19500 [59:43<00:00,  2.36pipeline/s]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 19500/19500 [59:45<00:00,  2.36pipeline/s]Optimization Progress: 100%|█████████▉| 19503/19600 [59:45<00:58,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19503/19600 [59:45<00:58,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19504/19600 [59:45<00:57,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19505/19600 [59:45<00:57,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19506/19600 [59:45<00:56,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19507/19600 [59:45<00:56,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19508/19600 [59:45<00:55,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19509/19600 [59:45<00:54,  1.66pipeline/s]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress: 100%|█████████▉| 19510/19600 [59:46<00:54,  1.66pipeline/s]Optimization Progress: 100%|█████████▉| 19512/19600 [59:56<01:07,  1.30pipeline/s]                                                                                  
Optimization Progress: 100%|█████████▉| 19591/19600 [59:56<00:06,  1.30pipeline/s]                                                                                  60.04 minutes have elapsed. TPOT will close down.
TPOT closed during evaluation in one generation.
WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.
Optimization Progress: 100%|█████████▉| 19591/19600 [59:56<00:06,  1.30pipeline/s]                                                                                  
Optimization Progress: 100%|█████████▉| 19591/19600 [59:56<00:06,  1.30pipeline/s]                                                                                  
TPOT closed prematurely. Will use the current best pipeline.
Optimization Progress: 100%|█████████▉| 19591/19600 [59:56<00:06,  1.30pipeline/s]                                                                                  Best pipeline:
0. FeatureUnion(transformer_list=[('functiontransformer-1',
                                FunctionTransformer(func=<function copy at 0x7f2895082b00>)),
                               ('functiontransformer-2',
                                FunctionTransformer(func=<function copy at 0x7f2895082b00>))])
1. StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85,
                                                      loss='quantile',
                                                      max_depth=6,
                                                      max_features=0.7500000000000001,
                                                      min_samples_leaf=15,
                                                      subsample=0.3))
2. VarianceThreshold(threshold=0.01)
3. StackingEstimator(estimator=XGBRegressor(base_score=0.5, booster='gbtree',
                                         colsample_bylevel=1,
                                         colsample_bynode=1, colsample_bytree=1,
                                         gamma=0, gpu_id=-1,
                                         importance_type='gain',
                                         interaction_constraints='',
                                         learning_rate=0.001, max_delta_step=0,
                                         max_depth=2, min_child_weight=20,
                                         missing=nan, monotone_constraints='()',
                                         n_estimators=100, n_jobs=1, nthread=1,
                                         num_parallel_tree=1, random_state=0,
                                         reg_alpha=0, reg_lambda=1,
                                         scale_pos_weight=1,
                                         subsample=0.9000000000000001,
                                         tree_method='exact',
                                         validate_parameters=1,
                                         verbosity=None))
4. Nystroem(gamma=0.6000000000000001, kernel='polynomial', n_components=6)
5. Nystroem(gamma=0.6000000000000001, kernel='polynomial', n_components=6)
6. StandardScaler()
7. PolynomialFeatures(include_bias=False)
8. Normalizer(norm='max')
9. LinearSVR(C=10.0, epsilon=0.001, tol=0.1)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
