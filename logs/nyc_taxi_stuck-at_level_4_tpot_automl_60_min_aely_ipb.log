30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]Optimization Progress:   3%|▎         | 3/100 [00:06<03:35,  2.22s/pipeline]Optimization Progress:  83%|████████▎ | 83/100 [00:11<00:26,  1.58s/pipeline]                                                                             _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:12<00:00,  1.58s/pipeline]Optimization Progress: 100%|██████████| 100/100 [00:12<00:00,  1.11s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 [14:48:38] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 100/100 [00:12<00:00,  1.11s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 100/100 [00:12<00:00,  1.11s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [14:48:39] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 100/100 [00:13<00:00,  1.11s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:13<00:00,  1.11s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 100/100 [00:17<00:00,  1.11s/pipeline]Optimization Progress:  52%|█████▎    | 105/200 [00:17<01:43,  1.09s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  52%|█████▎    | 105/200 [00:17<01:43,  1.09s/pipeline]Optimization Progress:  54%|█████▎    | 107/200 [01:16<14:53,  9.60s/pipeline]Optimization Progress:  94%|█████████▎| 187/200 [01:21<01:27,  6.74s/pipeline]
Generation 1 - Current Pareto front scores:
-1	-570323224.3075778	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=5, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress: 100%|██████████| 200/200 [01:22<00:00,  6.74s/pipeline]Optimization Progress: 100%|██████████| 200/200 [01:22<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 200/200 [01:22<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress: 100%|██████████| 200/200 [01:24<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:26<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [01:26<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 200/200 [01:26<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 200/200 [01:27<00:00,  4.73s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress: 100%|██████████| 200/200 [01:28<00:00,  4.73s/pipeline]Optimization Progress:  68%|██████▊   | 204/300 [01:28<06:06,  3.82s/pipeline]Optimization Progress:  68%|██████▊   | 205/300 [01:35<07:22,  4.66s/pipeline]Optimization Progress:  95%|█████████▌| 285/300 [01:41<00:49,  3.28s/pipeline]
Generation 2 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [01:41<00:00,  3.28s/pipeline]Optimization Progress: 100%|██████████| 300/300 [01:41<00:00,  2.31s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 300/300 [01:44<00:00,  2.31s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [14:50:14] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 300/300 [01:47<00:00,  2.31s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 300/300 [01:48<00:00,  2.31s/pipeline]Optimization Progress:  76%|███████▋  | 306/400 [01:48<03:05,  1.98s/pipeline]Optimization Progress:  77%|███████▋  | 308/400 [01:55<03:34,  2.33s/pipeline]Optimization Progress:  97%|█████████▋| 387/400 [01:57<00:21,  1.64s/pipeline]
Generation 3 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [01:58<00:00,  1.64s/pipeline]Optimization Progress: 100%|██████████| 400/400 [01:58<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [01:58<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:00<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [14:50:27] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 400/400 [02:00<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:03<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 400/400 [02:04<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:05<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 400/400 [02:06<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:07<00:00,  1.17s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 400/400 [02:08<00:00,  1.17s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:08<02:55,  1.81s/pipeline]Optimization Progress:  81%|████████  | 403/500 [02:20<02:55,  1.81s/pipeline]Optimization Progress:  81%|████████  | 404/500 [05:08<1:28:26, 55.28s/pipeline]Optimization Progress:  97%|█████████▋| 484/500 [05:18<10:19, 38.73s/pipeline]  
Generation 4 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-541197013.054898	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [05:19<00:00, 38.73s/pipeline]Optimization Progress: 100%|██████████| 500/500 [05:19<00:00, 27.13s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 [14:53:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 500/500 [05:20<00:00, 27.13s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 500/500 [05:22<00:00, 27.13s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 500/500 [05:27<00:00, 27.13s/pipeline]Optimization Progress:  84%|████████▎ | 501/600 [05:27<35:37, 21.59s/pipeline]Optimization Progress:  84%|████████▎ | 502/600 [07:47<1:33:07, 57.02s/pipeline]Optimization Progress:  97%|█████████▋| 582/600 [07:51<11:58, 39.93s/pipeline]  
Generation 5 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-541197013.054898	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-533348061.5955559	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 600/600 [07:52<00:00, 39.93s/pipeline]Optimization Progress: 100%|██████████| 600/600 [07:52<00:00, 27.97s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [07:55<00:00, 27.97s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 600/600 [07:55<00:00, 27.97s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..
Optimization Progress: 100%|██████████| 600/600 [07:55<00:00, 27.97s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 600/600 [07:58<00:00, 27.97s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress: 100%|██████████| 600/600 [07:58<00:00, 27.97s/pipeline]Optimization Progress:  86%|████████▌ | 602/700 [08:00<33:54, 20.77s/pipeline]                                                                              Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  86%|████████▌ | 602/700 [08:00<33:54, 20.77s/pipeline]Optimization Progress:  86%|████████▋ | 604/700 [09:03<38:22, 23.98s/pipeline]Optimization Progress:  98%|█████████▊| 684/700 [09:08<04:28, 16.80s/pipeline]
Generation 6 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [09:09<00:00, 16.80s/pipeline]Optimization Progress: 100%|██████████| 700/700 [09:09<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.
Optimization Progress: 100%|██████████| 700/700 [09:11<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress: 100%|██████████| 700/700 [09:13<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [09:15<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 700/700 [09:15<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress: 100%|██████████| 700/700 [09:16<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 700/700 [09:16<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 700/700 [09:19<00:00, 11.79s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 700/700 [09:19<00:00, 11.79s/pipeline]Optimization Progress:  88%|████████▊ | 701/800 [09:20<18:48, 11.40s/pipeline]                                                                              Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 701/800 [09:20<18:48, 11.40s/pipeline]Optimization Progress:  88%|████████▊ | 703/800 [09:40<17:50, 11.03s/pipeline]Optimization Progress:  98%|█████████▊| 783/800 [09:45<02:11,  7.74s/pipeline]
Generation 7 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.
Optimization Progress: 100%|██████████| 800/800 [09:47<00:00,  7.74s/pipeline]Optimization Progress: 100%|██████████| 800/800 [09:47<00:00,  5.45s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 800/800 [09:49<00:00,  5.45s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 800/800 [09:51<00:00,  5.45s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress: 100%|██████████| 800/800 [09:52<00:00,  5.45s/pipeline]Optimization Progress:  89%|████████▉ | 801/900 [09:58<11:35,  7.03s/pipeline]Optimization Progress:  89%|████████▉ | 802/900 [10:04<11:14,  6.88s/pipeline]Optimization Progress:  98%|█████████▊| 882/900 [10:30<01:28,  4.92s/pipeline]
Generation 8 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [10:30<00:00,  4.92s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [10:31<00:00,  4.92s/pipeline]Optimization Progress: 100%|██████████| 900/900 [10:31<00:00,  3.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 900/900 [10:40<00:00,  3.46s/pipeline]                                                                              _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 900/900 [10:40<00:00,  3.46s/pipeline]Optimization Progress:  90%|█████████ | 901/1000 [10:50<13:17,  8.06s/pipeline]Optimization Progress:  98%|█████████▊| 981/1000 [10:57<01:47,  5.67s/pipeline]
Generation 9 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1000/1000 [11:01<00:00,  5.67s/pipeline]Optimization Progress: 100%|██████████| 1000/1000 [11:01<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 1000/1000 [11:02<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [11:02<00:00,  4.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1000/1000 [11:06<00:00,  4.04s/pipeline]Optimization Progress:  91%|█████████ | 1001/1100 [11:08<07:56,  4.82s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  91%|█████████ | 1001/1100 [11:08<07:56,  4.82s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  91%|█████████ | 1002/1100 [11:08<07:52,  4.82s/pipeline]Optimization Progress:  91%|█████████▏| 1004/1100 [11:15<06:35,  4.12s/pipeline]Optimization Progress:  99%|█████████▊| 1084/1100 [11:19<00:46,  2.90s/pipeline]
Generation 10 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [14:59:46] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 1100/1100 [11:19<00:00,  2.90s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [11:19<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1100/1100 [11:19<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [11:21<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1100/1100 [11:22<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1100/1100 [11:23<00:00,  2.04s/pipeline]Optimization Progress: 100%|██████████| 1100/1100 [11:30<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [11:32<00:00,  2.04s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1100/1100 [11:32<00:00,  2.04s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  92%|█████████▏| 1100/1200 [11:32<03:23,  2.04s/pipeline]Optimization Progress:  92%|█████████▏| 1101/1200 [11:32<08:46,  5.32s/pipeline]Optimization Progress:  92%|█████████▏| 1102/1200 [14:02<1:19:18, 48.56s/pipeline]Optimization Progress:  98%|█████████▊| 1182/1200 [14:54<10:15, 34.19s/pipeline]  
Generation 11 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-518094601.15035945	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)
-4	-518029797.38566434	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.55, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.4), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [14:55<00:00, 34.19s/pipeline]Optimization Progress: 100%|██████████| 1200/1200 [14:55<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:03:25] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 1200/1200 [14:58<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1200/1200 [14:59<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [15:00<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [15:01<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1200/1200 [15:03<00:00, 23.94s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1200/1200 [15:07<00:00, 23.94s/pipeline]Optimization Progress:  92%|█████████▏| 1200/1300 [15:10<39:53, 23.94s/pipeline]Optimization Progress:  92%|█████████▏| 1201/1300 [15:17<38:59, 23.63s/pipeline]Optimization Progress:  99%|█████████▊| 1281/1300 [15:24<05:14, 16.57s/pipeline]
Generation 12 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-512719837.16337234	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1300/1300 [15:24<00:00, 16.57s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1300/1300 [15:25<00:00, 16.57s/pipeline]Optimization Progress: 100%|██████████| 1300/1300 [15:25<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [15:26<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1300/1300 [15:26<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1300/1300 [15:29<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1300/1300 [15:31<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1300/1300 [15:32<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1300/1300 [15:32<00:00, 11.62s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1300/1300 [15:36<00:00, 11.62s/pipeline]Optimization Progress:  93%|█████████▎| 1301/1400 [15:37<19:25, 11.77s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1301/1400 [15:37<19:25, 11.77s/pipeline]Optimization Progress:  93%|█████████▎| 1303/1400 [15:47<15:41,  9.70s/pipeline]Optimization Progress:  99%|█████████▉| 1383/1400 [18:10<02:04,  7.33s/pipeline]
Generation 13 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-532947082.6523318	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-512719837.16337234	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)
-4	-486887519.98199445	GradientBoostingRegressor(PCA(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=18, DecisionTreeRegressor__min_samples_split=9), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)Optimization Progress: 100%|██████████| 1400/1400 [18:23<00:00,  5.37s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  93%|█████████▎| 1401/1500 [18:24<08:51,  5.37s/pipeline]Optimization Progress:  94%|█████████▎| 1403/1500 [18:48<10:01,  6.20s/pipeline]Optimization Progress:  99%|█████████▉| 1483/1500 [18:52<01:14,  4.36s/pipeline]
Generation 14 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-524046532.18554497	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.35000000000000003, RandomForestRegressor__min_samples_leaf=1, RandomForestRegressor__min_samples_split=19, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.15000000000000002)
-3	-503141496.17548114	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)
-4	-486887519.98199445	GradientBoostingRegressor(PCA(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=18, DecisionTreeRegressor__min_samples_split=9), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1500/1500 [19:01<00:00,  4.36s/pipeline]Optimization Progress: 100%|██████████| 1500/1500 [19:01<00:00,  3.20s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1500/1500 [19:01<00:00,  3.20s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:07:29] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 1500/1500 [19:02<00:00,  3.20s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress: 100%|██████████| 1500/1500 [19:02<00:00,  3.20s/pipeline]Optimization Progress:  94%|█████████▍| 1505/1600 [19:06<04:03,  2.56s/pipeline]Optimization Progress:  94%|█████████▍| 1506/1600 [20:02<29:20, 18.73s/pipeline]Optimization Progress:  99%|█████████▉| 1586/1600 [20:06<03:03, 13.13s/pipeline]
Generation 15 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-515255940.5064475	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.1, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-501034512.046284	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.45), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
-4	-486887519.98199445	GradientBoostingRegressor(PCA(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=18, DecisionTreeRegressor__min_samples_split=9), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1600/1600 [20:07<00:00, 13.13s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1600/1600 [20:09<00:00, 13.13s/pipeline]Optimization Progress: 100%|██████████| 1600/1600 [20:09<00:00,  9.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 1600/1600 [20:17<00:00,  9.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1600/1600 [20:18<00:00,  9.24s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 1600/1600 [20:23<00:00,  9.24s/pipeline]Optimization Progress:  94%|█████████▍| 1603/1700 [20:23<12:45,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 1603/1700 [20:23<12:45,  7.89s/pipeline]Optimization Progress:  94%|█████████▍| 1605/1700 [21:08<19:26, 12.28s/pipeline]Optimization Progress:  99%|█████████▉| 1685/1700 [21:13<02:09,  8.61s/pipeline]
Generation 16 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-515255940.5064475	GradientBoostingRegressor(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.1, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=12, RandomForestRegressor__n_estimators=100), GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=9, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)
-3	-501034512.046284	ExtraTreesRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.45), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
-4	-486887519.98199445	GradientBoostingRegressor(PCA(RandomForestRegressor(DecisionTreeRegressor(input_matrix, DecisionTreeRegressor__max_depth=10, DecisionTreeRegressor__min_samples_leaf=18, DecisionTreeRegressor__min_samples_split=9), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1700/1700 [21:15<00:00,  8.61s/pipeline]Optimization Progress: 100%|██████████| 1700/1700 [21:15<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.
Optimization Progress: 100%|██████████| 1700/1700 [21:17<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 1700/1700 [21:18<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1700/1700 [21:21<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1700/1700 [21:22<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1700/1700 [21:23<00:00,  6.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1700/1700 [21:25<00:00,  6.06s/pipeline]Optimization Progress:  95%|█████████▍| 1702/1800 [21:30<10:36,  6.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1702/1800 [21:30<10:36,  6.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1703/1800 [21:30<10:29,  6.49s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▍| 1704/1800 [21:30<10:23,  6.49s/pipeline]Optimization Progress:  95%|█████████▍| 1706/1800 [21:38<08:01,  5.12s/pipeline]Optimization Progress:  99%|█████████▉| 1786/1800 [21:43<00:50,  3.60s/pipeline]
Generation 17 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1800/1800 [21:46<00:00,  3.60s/pipeline]Optimization Progress: 100%|██████████| 1800/1800 [21:46<00:00,  2.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress: 100%|██████████| 1800/1800 [21:47<00:00,  2.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [21:48<00:00,  2.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1800/1800 [21:55<00:00,  2.60s/pipeline]Optimization Progress:  95%|█████████▍| 1801/1900 [21:56<07:55,  4.80s/pipeline]Optimization Progress:  95%|█████████▍| 1802/1900 [22:10<12:09,  7.44s/pipeline]Optimization Progress:  99%|█████████▉| 1882/1900 [22:32<01:35,  5.29s/pipeline]
Generation 18 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [22:35<00:00,  5.29s/pipeline]Optimization Progress: 100%|██████████| 1900/1900 [22:35<00:00,  3.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 1900/1900 [22:36<00:00,  3.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1900/1900 [22:36<00:00,  3.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [22:39<00:00,  3.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 1900/1900 [22:42<00:00,  3.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 1900/1900 [22:44<00:00,  3.76s/pipeline]Optimization Progress:  95%|█████████▌| 1902/2000 [22:45<06:38,  4.07s/pipeline]Optimization Progress:  95%|█████████▌| 1903/2000 [22:53<08:41,  5.38s/pipeline]Optimization Progress:  99%|█████████▉| 1983/2000 [23:00<01:04,  3.79s/pipeline]
Generation 19 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2000/2000 [23:01<00:00,  3.79s/pipeline]Optimization Progress: 100%|██████████| 2000/2000 [23:01<00:00,  2.67s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2000/2000 [23:02<00:00,  2.67s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2000/2000 [23:02<00:00,  2.67s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2000/2000 [23:07<00:00,  2.67s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 2000/2000 [23:08<00:00,  2.67s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2000/2000 [23:11<00:00,  2.67s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2000/2100 [23:14<04:26,  2.67s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2001/2100 [23:14<04:23,  2.67s/pipeline]Optimization Progress:  95%|█████████▌| 2002/2100 [23:14<06:19,  3.87s/pipeline]Optimization Progress:  95%|█████████▌| 2003/2100 [26:11<1:30:07, 55.75s/pipeline]Optimization Progress:  99%|█████████▉| 2083/2100 [26:18<11:03, 39.05s/pipeline]  
Generation 20 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-4	-478375268.36949015	ExtraTreesRegressor(SGDRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2100/2100 [26:31<00:00, 39.05s/pipeline]Optimization Progress: 100%|██████████| 2100/2100 [26:31<00:00, 27.58s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  95%|█████████▌| 2100/2200 [26:32<45:57, 27.58s/pipeline]Optimization Progress:  96%|█████████▌| 2101/2200 [26:32<32:12, 19.52s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2101/2200 [26:32<32:12, 19.52s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2102/2200 [26:32<31:52, 19.52s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2103/2200 [26:32<31:33, 19.52s/pipeline]Optimization Progress:  96%|█████████▌| 2105/2200 [26:41<22:44, 14.37s/pipeline]Optimization Progress:  99%|█████████▉| 2185/2200 [26:47<02:31, 10.08s/pipeline]
Generation 21 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-4	-478375268.36949015	ExtraTreesRegressor(SGDRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [26:47<00:00, 10.08s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [26:47<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress: 100%|██████████| 2200/2200 [26:50<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [26:51<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [26:55<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [26:56<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [26:57<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2200/2200 [26:57<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [26:59<00:00,  7.06s/pipeline]Optimization Progress: 100%|██████████| 2200/2200 [27:00<00:00,  7.06s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2200/2200 [27:01<00:00,  7.06s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2200/2300 [27:02<11:45,  7.06s/pipeline]Optimization Progress:  96%|█████████▌| 2201/2300 [27:02<15:20,  9.29s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2201/2300 [27:02<15:20,  9.29s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2202/2300 [27:02<15:10,  9.29s/pipeline]Optimization Progress:  96%|█████████▌| 2204/2300 [27:37<16:06, 10.07s/pipeline]Optimization Progress:  99%|█████████▉| 2284/2300 [27:44<01:53,  7.07s/pipeline]
Generation 22 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-4	-478375268.36949015	ExtraTreesRegressor(SGDRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2300/2300 [27:44<00:00,  7.07s/pipeline]Optimization Progress: 100%|██████████| 2300/2300 [27:44<00:00,  4.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2300/2300 [27:48<00:00,  4.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [27:48<00:00,  4.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2300/2300 [27:57<00:00,  4.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2301/2400 [27:59<08:10,  4.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2302/2400 [27:59<08:05,  4.96s/pipeline]Optimization Progress:  96%|█████████▌| 2303/2400 [27:59<08:01,  4.96s/pipeline]Optimization Progress:  96%|█████████▌| 2304/2400 [28:55<32:01, 20.01s/pipeline]Optimization Progress:  99%|█████████▉| 2384/2400 [29:00<03:44, 14.03s/pipeline]
Generation 23 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-4	-478375268.36949015	ExtraTreesRegressor(SGDRegressor(FastICA(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100), FastICA__tol=0.6000000000000001), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.25, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=3, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [29:03<00:00, 14.03s/pipeline]Optimization Progress: 100%|██████████| 2400/2400 [29:03<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2400/2400 [29:03<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2400/2400 [29:06<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [29:08<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [29:09<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2400/2400 [29:09<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2400/2400 [29:13<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress: 100%|██████████| 2400/2400 [29:13<00:00,  9.87s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2400/2400 [29:13<00:00,  9.87s/pipeline]Optimization Progress:  96%|█████████▌| 2405/2500 [29:14<12:01,  7.60s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2405/2500 [29:14<12:01,  7.60s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2406/2500 [29:14<11:54,  7.60s/pipeline]Optimization Progress:  96%|█████████▋| 2408/2500 [30:46<22:19, 14.56s/pipeline]Optimization Progress: 100%|█████████▉| 2488/2500 [30:50<02:02, 10.21s/pipeline]
Generation 24 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.
Optimization Progress: 100%|██████████| 2500/2500 [30:51<00:00, 10.21s/pipeline]Optimization Progress: 100%|██████████| 2500/2500 [30:51<00:00,  7.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [30:54<00:00,  7.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [30:54<00:00,  7.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [30:58<00:00,  7.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2500/2500 [31:03<00:00,  7.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:19:31] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 2500/2500 [31:04<00:00,  7.17s/pipeline]Optimization Progress:  96%|█████████▌| 2501/2600 [31:04<14:44,  8.93s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2501/2600 [31:04<14:44,  8.93s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2502/2600 [31:04<14:35,  8.93s/pipeline]Optimization Progress:  96%|█████████▋| 2504/2600 [31:14<11:37,  7.27s/pipeline]Optimization Progress:  99%|█████████▉| 2584/2600 [31:19<01:21,  5.10s/pipeline]
Generation 25 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [31:19<00:00,  5.10s/pipeline]Optimization Progress: 100%|██████████| 2600/2600 [31:19<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [31:22<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [31:24<00:00,  3.58s/pipeline]Optimization Progress: 100%|██████████| 2600/2600 [31:30<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2600/2600 [31:30<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress: 100%|██████████| 2600/2600 [31:33<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 2600/2600 [31:33<00:00,  3.58s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2600/2600 [31:33<00:00,  3.58s/pipeline]Optimization Progress:  96%|█████████▋| 2601/2700 [31:34<11:28,  6.96s/pipeline]Optimization Progress:  96%|█████████▋| 2602/2700 [31:56<18:42, 11.45s/pipeline]Optimization Progress:  99%|█████████▉| 2682/2700 [32:02<02:24,  8.04s/pipeline]
Generation 26 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [32:03<00:00,  8.04s/pipeline]Optimization Progress: 100%|██████████| 2700/2700 [32:03<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [32:06<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [32:06<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2700/2700 [32:08<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [32:11<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [32:11<00:00,  5.66s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 2700/2700 [32:16<00:00,  5.66s/pipeline]Optimization Progress:  96%|█████████▋| 2701/2800 [32:18<13:42,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▋| 2701/2800 [32:18<13:42,  8.30s/pipeline]Optimization Progress:  97%|█████████▋| 2703/2800 [32:31<12:32,  7.75s/pipeline]Optimization Progress:  99%|█████████▉| 2783/2800 [33:39<01:36,  5.68s/pipeline]
Generation 27 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress: 100%|██████████| 2800/2800 [33:40<00:00,  5.68s/pipeline]Optimization Progress: 100%|██████████| 2800/2800 [33:40<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 [15:22:10] ../src/learner.cc:543: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0xa5dc4) [0x7f3cf259ddc4]
  [bt] (1) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b6669) [0x7f3cf26ae669]
  [bt] (2) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1c3f8e) [0x7f3cf26bbf8e]
  [bt] (3) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1aacbb) [0x7f3cf26a2cbb]
  [bt] (4) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x55) [0x7f3cf258ff35]
  [bt] (5) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f330034a9dd]
  [bt] (6) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f330034a067]
  [bt] (7) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f330036227e]
  [bt] (8) /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12cb4) [0x7f3300362cb4]

.
Optimization Progress: 100%|██████████| 2800/2800 [33:44<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.
Optimization Progress: 100%|██████████| 2800/2800 [33:44<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2800/2800 [33:47<00:00,  3.99s/pipeline]Optimization Progress: 100%|██████████| 2800/2800 [33:50<00:00,  3.99s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 2800/2800 [33:53<00:00,  3.99s/pipeline]Optimization Progress:  97%|█████████▋| 2802/2900 [33:54<08:07,  4.98s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2802/2900 [33:54<08:07,  4.98s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2803/2900 [33:54<08:02,  4.98s/pipeline]Optimization Progress:  97%|█████████▋| 2805/2900 [36:08<26:38, 16.83s/pipeline]Optimization Progress:  99%|█████████▉| 2885/2900 [36:14<02:57, 11.80s/pipeline]
Generation 28 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:16<00:00, 11.80s/pipeline]Optimization Progress: 100%|██████████| 2900/2900 [36:16<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress: 100%|██████████| 2900/2900 [36:17<00:00,  8.31s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 2900/2900 [36:19<00:00,  8.31s/pipeline]Optimization Progress:  97%|█████████▋| 2901/3000 [36:28<15:11,  9.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2901/3000 [36:28<15:11,  9.21s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 2902/3000 [36:28<15:02,  9.21s/pipeline]Optimization Progress:  97%|█████████▋| 2904/3000 [36:39<12:07,  7.58s/pipeline]Optimization Progress:  99%|█████████▉| 2984/3000 [36:45<01:25,  5.33s/pipeline]
Generation 29 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)Optimization Progress: 100%|██████████| 3000/3000 [36:57<00:00,  3.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3003/3100 [36:57<06:23,  3.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3004/3100 [36:57<06:19,  3.96s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3005/3100 [36:57<06:15,  3.96s/pipeline]Optimization Progress:  97%|█████████▋| 3007/3100 [39:01<12:29,  8.06s/pipeline]Optimization Progress: 100%|█████████▉| 3087/3100 [39:05<01:13,  5.66s/pipeline]
Generation 30 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [39:06<00:00,  5.66s/pipeline]Optimization Progress: 100%|██████████| 3100/3100 [39:06<00:00,  3.97s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress: 100%|██████████| 3100/3100 [39:06<00:00,  3.97s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress: 100%|██████████| 3100/3100 [39:06<00:00,  3.97s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3100/3100 [39:07<00:00,  3.97s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3100/3100 [39:12<00:00,  3.97s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3100/3100 [39:14<00:00,  3.97s/pipeline]Optimization Progress:  97%|█████████▋| 3102/3200 [39:15<06:54,  4.23s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3102/3200 [39:15<06:54,  4.23s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3103/3200 [39:16<06:50,  4.23s/pipeline]Optimization Progress:  97%|█████████▋| 3105/3200 [39:24<05:58,  3.77s/pipeline]Optimization Progress: 100%|█████████▉| 3185/3200 [39:28<00:39,  2.66s/pipeline]
Generation 31 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3200/3200 [39:33<00:00,  2.66s/pipeline]Optimization Progress: 100%|██████████| 3200/3200 [39:33<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [39:33<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.
Optimization Progress: 100%|██████████| 3200/3200 [39:33<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.
Optimization Progress: 100%|██████████| 3200/3200 [39:34<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [39:36<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [39:36<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3200/3200 [39:39<00:00,  1.96s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3200/3200 [39:39<00:00,  1.96s/pipeline]Optimization Progress:  97%|█████████▋| 3201/3300 [39:40<05:35,  3.39s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3201/3300 [39:40<05:35,  3.39s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3202/3300 [39:40<05:31,  3.39s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3203/3300 [39:40<05:28,  3.39s/pipeline]Optimization Progress:  97%|█████████▋| 3205/3300 [41:23<16:02, 10.13s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Optimization Progress: 100%|█████████▉| 3285/3300 [45:37<02:00,  8.04s/pipeline]
Generation 32 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3300/3300 [45:40<00:00,  8.04s/pipeline]Optimization Progress: 100%|██████████| 3300/3300 [45:40<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [45:42<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3300/3300 [45:42<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 3300/3300 [45:46<00:00,  5.69s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress: 100%|██████████| 3300/3300 [45:46<00:00,  5.69s/pipeline]/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress: 100%|██████████| 3300/3300 [45:47<00:00,  5.69s/pipeline]Optimization Progress:  97%|█████████▋| 3303/3400 [45:48<07:49,  4.84s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3303/3400 [45:48<07:49,  4.84s/pipeline]Optimization Progress:  97%|█████████▋| 3305/3400 [47:38<31:28, 19.88s/pipeline]Optimization Progress: 100%|█████████▉| 3385/3400 [47:45<03:29, 13.94s/pipeline]
Generation 33 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3400/3400 [47:46<00:00, 13.94s/pipeline]Optimization Progress: 100%|██████████| 3400/3400 [47:46<00:00,  9.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3400/3400 [47:46<00:00,  9.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [47:47<00:00,  9.77s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3400/3400 [47:51<00:00,  9.77s/pipeline]Optimization Progress:  97%|█████████▋| 3403/3500 [47:56<12:44,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3403/3500 [47:56<12:44,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3404/3500 [47:56<12:37,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3405/3500 [47:56<12:29,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3406/3500 [47:56<12:21,  7.89s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3407/3500 [47:56<12:13,  7.89s/pipeline]Optimization Progress:  97%|█████████▋| 3409/3500 [51:20<23:49, 15.71s/pipeline]Optimization Progress: 100%|█████████▉| 3489/3500 [51:25<02:01, 11.01s/pipeline]
Generation 34 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [51:28<00:00, 11.01s/pipeline]Optimization Progress: 100%|██████████| 3500/3500 [51:28<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3500/3500 [51:30<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3500/3500 [51:33<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress: 100%|██████████| 3500/3500 [51:37<00:00,  7.79s/pipeline]Optimization Progress:  97%|█████████▋| 3501/3600 [51:38<13:41,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3501/3600 [51:38<13:41,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3502/3600 [51:38<13:33,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3503/3600 [51:38<13:25,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3504/3600 [51:38<13:16,  8.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3505/3600 [51:38<13:08,  8.30s/pipeline]Optimization Progress:  97%|█████████▋| 3507/3600 [54:10<20:50, 13.45s/pipeline]Optimization Progress: 100%|█████████▉| 3587/3600 [54:16<02:02,  9.44s/pipeline]
Generation 35 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress: 100%|██████████| 3600/3600 [54:16<00:00,  9.44s/pipeline]Optimization Progress: 100%|██████████| 3600/3600 [54:16<00:00,  6.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress: 100%|██████████| 3600/3600 [54:24<00:00,  6.61s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3600/3600 [54:25<00:00,  6.61s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3603/3700 [54:28<10:40,  6.61s/pipeline]Optimization Progress:  97%|█████████▋| 3604/3700 [54:28<08:47,  5.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3604/3700 [54:28<08:47,  5.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3605/3700 [54:28<08:42,  5.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3606/3700 [54:28<08:36,  5.50s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3607/3700 [54:28<08:31,  5.50s/pipeline]Optimization Progress:  98%|█████████▊| 3609/3700 [57:45<23:48, 15.70s/pipeline]Optimization Progress: 100%|█████████▉| 3689/3700 [57:52<02:01, 11.02s/pipeline]
Generation 36 - Current Pareto front scores:
-1	-542413755.3228805	XGBRegressor(input_matrix, XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-2	-480943041.89940405	XGBRegressor(Nystroem(input_matrix, Nystroem__gamma=0.2, Nystroem__kernel=additive_chi2, Nystroem__n_components=7), XGBRegressor__learning_rate=1.0, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__nthread=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.15000000000000002)
-3	-423756185.42665446	GradientBoostingRegressor(PCA(RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9000000000000001, RandomForestRegressor__min_samples_leaf=4, RandomForestRegressor__min_samples_split=5, RandomForestRegressor__n_estimators=100), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=quantile, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.9000000000000001, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=3, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.25)                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [57:55<00:00, 11.02s/pipeline]Optimization Progress: 100%|██████████| 3700/3700 [57:55<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..
Optimization Progress: 100%|██████████| 3700/3700 [57:56<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [57:59<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [58:00<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress: 100%|██████████| 3700/3700 [58:00<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [58:02<00:00,  7.79s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress: 100%|██████████| 3700/3700 [58:02<00:00,  7.79s/pipeline]Optimization Progress:  97%|█████████▋| 3702/3800 [58:03<10:59,  6.73s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  97%|█████████▋| 3702/3800 [58:03<10:59,  6.73s/pipeline]Optimization Progress:  97%|█████████▋| 3704/3800 [1:03:04<1:19:50, 49.90s/pipeline]                                                                                    Skipped pipeline #3711 due to time out. Continuing to the next pipeline.
Optimization Progress:  98%|█████████▊| 3711/3800 [1:03:04<1:14:01, 49.90s/pipeline]                                                                                    
Optimization Progress: 100%|█████████▉| 3784/3800 [1:03:04<13:18, 49.90s/pipeline]                                                                                  63.20 minutes have elapsed. TPOT will close down.
TPOT closed during evaluation in one generation.
WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.
Optimization Progress: 100%|█████████▉| 3784/3800 [1:03:04<13:18, 49.90s/pipeline]                                                                                  
Optimization Progress: 100%|█████████▉| 3784/3800 [1:03:04<13:18, 49.90s/pipeline]                                                                                  
TPOT closed prematurely. Will use the current best pipeline.
Optimization Progress: 100%|█████████▉| 3784/3800 [1:03:04<13:18, 49.90s/pipeline]                                                                                  /projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
Best pipeline:
0. StackingEstimator(estimator=RandomForestRegressor(max_features=0.9000000000000001,
                                                  min_samples_leaf=4,
                                                  min_samples_split=5))
1. PCA(iterated_power=8, svd_solver='randomized')
2. GradientBoostingRegressor(alpha=0.95, learning_rate=0.5, loss='quantile',
                          max_depth=2, max_features=0.9000000000000001,
                          min_samples_leaf=5, min_samples_split=3,
                          subsample=0.25)
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
/projappl/project_2003107/anaconda3/envs/tpot_automl/lib/python3.7/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.
  warnings.warn("Warning: optional dependency `torch` is not available. - skipping import of NN models.")
